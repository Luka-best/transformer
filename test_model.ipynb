{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bc1dacfd2e411c91b50f7f71441838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup dataset + index\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import faiss\n",
    "retrieval_vector_size = 768\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"id\": [str(i) for i in range(10)],\n",
    "        \"text\": [\"My favourite number is 3455\", \"The secret word is FROG\"] * 5,\n",
    "        \"embeddings\": [\n",
    "            0.1 * np.ones(retrieval_vector_size),\n",
    "            0.9 * np.ones(retrieval_vector_size),\n",
    "        ] * 5,\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset.add_faiss_index(\"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "from src.transformers.models.atlas.retrieval_atlas import AtlasRetrieverIndex, AtlasConfig, AtlasTokenizer\n",
    "\n",
    "config = AtlasConfig.from_pretrained(\"./data/atlas-pretrained\")\n",
    "tokenizer = AtlasTokenizer.from_pretrained(\"./data/atlas-pretrained\", config=config)\n",
    "\n",
    "retriever_index = AtlasRetrieverIndex(config, tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason, src.transformers.models.atlas.modeling_atlas as import causes an error here\n",
    "# model doesn't load right?\n",
    "from transformers import AtlasModel\n",
    "\n",
    "atlas = AtlasModel.from_pretrained('data/atlas-pretrained', retriever_index=retriever_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c38f9632f240c8ab8618c3fa43dc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e9355866e344b99734842259548111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  822,    10,   363,    19,    82,  3960,   381,    58,  1525,    10,\n",
      "             3, 32099,     1],\n",
      "        [  822,    10,   363,    19,     8,  2829,  1448,    58,  1525,    10,\n",
      "             3, 32099,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AtlasModelOutput(generator_loss=tensor(3.9709, grad_fn=<NllLossBackward0>), retriever_loss=tensor(0.0053, grad_fn=<KlDivBackward0>), logits=tensor([[[-12.8470,  -3.9406,  -3.9002,  ..., -13.3872, -11.8982, -13.3796],\n",
       "         [-25.8175,  -2.2608,  -7.4646,  ..., -25.6973, -25.7258, -25.3556],\n",
       "         [-30.8447,  -5.4745,  -8.6406,  ..., -30.5384, -30.6604, -30.6171],\n",
       "         [-28.4645,  -3.0418,  -6.6645,  ..., -27.9618, -28.6331, -27.9040],\n",
       "         [-31.9013,  -0.7016,  -4.6872,  ..., -31.3952, -32.0902, -31.1038]],\n",
       "\n",
       "        [[-12.7249,  -5.5796,  -6.8293,  ..., -13.2572, -12.4315, -13.3793],\n",
       "         [-37.6499,  -7.6553, -11.8667,  ..., -37.5812, -37.6253, -37.5362],\n",
       "         [-37.7885,  -8.9861,  -7.1741,  ..., -37.3482, -38.3792, -37.5330],\n",
       "         [-49.3809, -15.4851, -13.5717,  ..., -48.8605, -49.7353, -49.3180],\n",
       "         [-33.3617,  -0.7058,  -6.9944,  ..., -33.0043, -33.3792, -32.9788]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), doc_scores=None, past_key_values=None, retrieved_doc_embeds=None, retrieved_doc_ids=None, context_input_ids=None, context_attention_mask=None, question_encoder_last_hidden_state=None, question_enc_hidden_states=None, question_enc_attentions=None, generator_enc_last_hidden_state=tensor([[[-2.8237e-01,  0.0000e+00, -1.7130e-01,  ...,  0.0000e+00,\n",
       "          -1.4732e-01, -1.9349e-01],\n",
       "         [-4.2343e-02, -7.8747e-02, -4.6386e-02,  ...,  1.4916e-01,\n",
       "           1.3117e-01,  6.9247e-02],\n",
       "         [ 4.7900e-02,  0.0000e+00,  3.4405e-02,  ..., -6.9517e-02,\n",
       "          -1.5711e-01,  9.7963e-02],\n",
       "         ...,\n",
       "         [-1.4744e-01, -2.6450e-01, -0.0000e+00,  ...,  2.4919e-01,\n",
       "           1.9634e-01, -2.6415e-01],\n",
       "         [ 2.0187e-02,  1.7279e-02,  2.5260e-02,  ...,  3.5178e-03,\n",
       "          -4.5748e-03, -6.0740e-03],\n",
       "         [ 1.1014e-01,  4.0267e-02,  4.8298e-02,  ...,  2.0813e-01,\n",
       "           1.0159e-04,  2.1348e-02]],\n",
       "\n",
       "        [[-3.4076e-01,  2.2802e-01, -0.0000e+00,  ...,  7.2432e-02,\n",
       "          -2.3122e-01,  4.7232e-02],\n",
       "         [ 1.6325e-02, -6.4139e-02, -2.0614e-01,  ...,  1.3462e-01,\n",
       "          -7.7498e-02, -3.8984e-02],\n",
       "         [-8.6948e-02,  1.5527e-01, -9.7702e-02,  ...,  2.3287e-02,\n",
       "          -3.6865e-01,  3.2943e-02],\n",
       "         ...,\n",
       "         [ 1.7112e-01,  2.4340e-01, -1.4167e-01,  ...,  8.0528e-02,\n",
       "           7.0508e-02,  1.4864e-01],\n",
       "         [-7.6992e-02, -2.1653e-02,  7.8717e-03,  ...,  2.6384e-01,\n",
       "           1.3635e-01, -1.4723e-01],\n",
       "         [ 2.3769e-02,  1.4020e-02,  2.6334e-02,  ...,  2.2040e-03,\n",
       "          -5.5119e-03, -0.0000e+00]]], grad_fn=<ViewBackward0>), generator_enc_hidden_states=None, generator_enc_attentions=None, generator_dec_hidden_states=None, generator_dec_attentions=None, generator_cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_index.reindex(atlas, batch_size=2)\n",
    "\n",
    "\n",
    "inputs_string = [\"What is my favourite number?\", \"What is the secret word?\"]\n",
    "target_string = [\"3455\", \"FROG\"]\n",
    "\n",
    "inputs_string = [f\"question: {question} answer: <extra_id_0>\" for question in inputs_string]\n",
    "target_string = [f\"<extra_id_0> {answer}\" for answer in target_string]\n",
    "\n",
    "tokens = tokenizer.generator(inputs_string, return_tensors=\"pt\", padding=True)\n",
    "labels = tokenizer.generator(target_string, return_tensors=\"pt\", padding=True)\n",
    "query_tokens = tokenizer.retriever(inputs_string, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "labels[labels == tokenizer.generator.pad_token_id] = -100\n",
    "\n",
    "atlas.train()\n",
    "atlas.config.query_side_retriever_training = True\n",
    "\n",
    "print(tokens)\n",
    "atlas.forward(\n",
    "    input_ids=tokens.input_ids,\n",
    "    attention_mask=tokens.attention_mask,\n",
    "    labels=labels.input_ids,\n",
    "    query_input_ids=query_tokens.input_ids,\n",
    "    query_attention_mask=query_tokens.attention_mask,\n",
    "    top_k=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1015, grad_fn=<KlDivBackward0>)\n",
      "tensor(0.1034, grad_fn=<KlDivBackward0>)\n",
      "tensor(0.0187, grad_fn=<KlDivBackward0>)\n",
      "tensor(0.0284, grad_fn=<KlDivBackward0>)\n",
      "tensor(0.0025, grad_fn=<KlDivBackward0>)\n",
      "tensor(0.0041, grad_fn=<KlDivBackward0>)\n",
      "tensor(0.1115, grad_fn=<KlDivBackward0>)\n",
      "tensor(0.0009, grad_fn=<KlDivBackward0>)\n",
      "tensor(0.0028, grad_fn=<KlDivBackward0>)\n",
      "tensor(0.0057, grad_fn=<KlDivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "atlas.train()\n",
    "import torch\n",
    "optimizer = torch.optim.Adam(atlas.parameters(), lr=1e-4)\n",
    "for i in range(10):\n",
    "    loss = atlas.forward(\n",
    "        input_ids=tokens.input_ids,\n",
    "        attention_mask=tokens.attention_mask,\n",
    "        labels=labels.input_ids,\n",
    "        query_input_ids=query_tokens.input_ids,\n",
    "        query_attention_mask=query_tokens.attention_mask,\n",
    "        top_k=2,\n",
    "    )[1]\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # zero out gradients\n",
    "    optimizer.zero_grad()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "472f3460692ba2c0861145e5e150d03c8a5c0e40e057944a047c431b9050b93d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
