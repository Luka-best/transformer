{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Put this notebook at the same level as the `SpeechT5` repo.\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- To run on CPU: In `speecht5/sequence_generator.py`, comment out where it does `.to(device=\"cuda\")`\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `speecht5_base_asr.pt` and `t5_transformer_lm.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "You also need an input audio file, any WAV at 16 kHz will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b45926",
   "metadata": {},
   "source": [
    "## Load audio and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c18f317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/141231/1272-141231-0020.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b474ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/128104/1272-128104-0000.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a855d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69120,), 16000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "wav_data, cur_sample_rate = sf.read(input_file)\n",
    "wav_data.shape, cur_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e60de",
   "metadata": {},
   "source": [
    "NOTE: The `Wav2Vec2FeatureExtractor` does not make sure the audio file is mono. If it has shape `(2, length)` or even `(1, length)` then the output from the feature extractor is incorrect!\n",
    "\n",
    "The `do_normalize` option is False for the SpeechT5 ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9699780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(do_normalize=False, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea3da78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69120])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(wav_data, sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a94286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing padding mask\n",
    "inputs[\"attention_mask\"][:, 40000:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "62e522e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 2))\n",
    "plt.plot(inputs[\"input_values\"][0])\n",
    "plt.plot(inputs[\"attention_mask\"][0])\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f594f3",
   "metadata": {},
   "source": [
    "Transform multiple inputs into a single padded batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb99431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128632,), 16000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file2 = \"/Users/matthijs/Documents/FILES/HuggingFace/S2S/textless/AUDIO_DIR/selfdestruct.wav\"\n",
    "wav_data2, cur_sample_rate2 = sf.read(input_file2)\n",
    "wav_data2.shape, cur_sample_rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1daee2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128632])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2 = feature_extractor([wav_data, wav_data2], sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs2[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb653d87",
   "metadata": {},
   "source": [
    "The original model used a `padding_mask` as input, where False means no padding. The `Wav2Vec2FeatureExtractor` can return an `attention_mask`, where 1 means no padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3ab7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7163b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs2   # use the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint. I used `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task s2t \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5Tokenizer,\n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForSpeechToText, \n",
    "    SpeechT5ForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SpeechT5Config()\n",
    "hf_model = SpeechT5Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SpeechT5ForSpeechToText.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForSpeechToText(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5EncoderWithSpeechPrenet(\n",
       "      (prenet): SpeechT5SpeechEncoderPrenet(\n",
       "        (feature_encoder): SpeechT5FeatureEncoder(\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): SpeechT5GroupNormConvLayer(\n",
       "              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (1): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (2): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (3): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (4): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (5): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (6): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (feature_projection): SpeechT5FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (pos_conv_embed): SpeechT5PositionalConvEmbedding(\n",
       "          (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (padding): SpeechT5SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (pos_sinusoidal_embed): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5DecoderWithTextPrenet(\n",
       "      (prenet): SpeechT5TextDecoderPrenet(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder_postnet): SpeechT5TextDecoderPostnet(\n",
       "    (lm_head): Linear(in_features=768, out_features=81, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09c1c4b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr were not used when initializing SpeechT5Model: ['speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight', 'speecht5.encoder.prenet.masked_spec_embed', 'speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight', 'speecht5.encoder.prenet.pos_sinusoidal_embed.weights', 'speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_v', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight', 'speecht5.encoder.prenet.feature_projection.projection.bias', 'text_decoder_postnet.lm_head.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.bias', 'speecht5.decoder.prenet.embed_positions.weights', 'speecht5.encoder.prenet.feature_projection.projection.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_g', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight', 'speecht5.decoder.prenet.embed_tokens.weight', 'speecht5.encoder.prenet.feature_projection.layer_norm.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias', 'speecht5.encoder.prenet.feature_projection.layer_norm.bias']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading should work OK for class `SpeechT5Model` too:\n",
    "hf_model_naked = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb16ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = SpeechT5Tokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ede052eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr', vocab_size=79, model_max_len=450, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7628cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the attention layer weights are correct\n",
    "# for i in range(len(hf_model.speecht5.encoder.layers)):\n",
    "#     print(i, \"k_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.weight == orig_model.encoder.layers[i].self_attn.k_proj.weight))\n",
    "#     print(i, \"k_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.bias == orig_model.encoder.layers[i].self_attn.k_proj.bias))\n",
    "#     print(i, \"v_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.weight == orig_model.encoder.layers[i].self_attn.v_proj.weight))\n",
    "#     print(i, \"v_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.bias == orig_model.encoder.layers[i].self_attn.v_proj.bias))\n",
    "#     print(i, \"q_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.weight == orig_model.encoder.layers[i].self_attn.q_proj.weight))\n",
    "#     print(i, \"q_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.bias == orig_model.encoder.layers[i].self_attn.q_proj.bias))\n",
    "#     print(i, \"out_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.weight == orig_model.encoder.layers[i].self_attn.out_proj.weight))\n",
    "#     print(i, \"out_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.bias == orig_model.encoder.layers[i].self_attn.out_proj.bias))\n",
    "#     print(\"---\")\n",
    "\n",
    "# print(\"pos_emb weight\", torch.all(hf_model.speecht5.encoder.pos_emb.pe_k.weight == orig_model.encoder.pos_emb.pe_k.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82233c52",
   "metadata": {},
   "source": [
    "Run a single forward pass. This should run the encoder, decoder, and the relevant pre- and postnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29bd2f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.prenet(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e1f8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using no attention_mask\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speech_encoder_prenet(input_values=inputs[\"input_values\"])\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "359778a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97a77055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7db34f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41afd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fcef1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d01d920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 215, 768]), torch.Size([1, 215])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in hf_outputs if hasattr(x, \"shape\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d5ff106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.4147, -3.6930, 10.6119,  ...,  1.3639,  2.2972,  0.8101],\n",
       "         [ 8.1118, -4.2742, 12.2622,  ...,  0.1729,  1.6734,  0.6476],\n",
       "         [-2.8165,  2.4560, -1.3008,  ...,  0.9983, -0.4333,  2.5262],\n",
       "         ...,\n",
       "         [ 7.7907, -6.4359,  5.6191,  ...,  0.0352,  3.1737,  0.4928],\n",
       "         [-0.1691, -4.2425,  6.3816,  ..., -1.6755,  1.3843,  0.2407],\n",
       "         [ 2.0105, -3.1966,  7.5800,  ..., -0.5673,  1.7153,  0.5722]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d24920f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_encoder_input = hf_outputs[0]\n",
    "hf_encoder_attention_mask = hf_outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {},
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {},
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24e0f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {},
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea4b4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"s2t\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {},
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a79fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9b2818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/SpeechT5/speecht5_base_asr.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda7a201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.speecht5.T5TransformerModel"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64d53b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.modules.encoder.TransformerEncoder"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bad44136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c1faddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.data.encoders.sentencepiece_bpe.SentencepieceBPE"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.data import encoders\n",
    "from argparse import Namespace\n",
    "tokenizer = encoders.build_bpe(\n",
    "    Namespace(\n",
    "        bpe='sentencepiece', \n",
    "        sentencepiece_model='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/MODEL_DIR/spm_char.model'\n",
    "    )\n",
    ")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61dd5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_model.decoder.layers[0].encoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dfdb2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(checkpoint[\"cfg\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e96ec29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint[\"cfg\"][\"task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bff0def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {},
   "source": [
    "## Verify speech encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {},
   "source": [
    "This first uses the `speech_encoder_prenet` to convert the raw audio data into embeddings of shape `(batch, sequence_length, 768)`. The sequence length is roughly `number of audio samples / 320`, so there is one vector every 20 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01638115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69120])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = inputs[\"input_values\"]\n",
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35b0a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.BoolTensor(source.shape).fill_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94586252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = torch.BoolTensor((1 - inputs[\"attention_mask\"]).bool())\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "374cf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't work on the original model\n",
    "#padding_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31d1d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input, encoder_padding_mask = orig_model.speech_encoder_prenet(\n",
    "    source, padding_mask=padding_mask, mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36334be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input = orig_model.speech_encoder_prenet.feature_extractor(source)\n",
    "# encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26c4a65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 215, 768]), torch.Size([1, 215]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "058de538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.4147, -3.6930, 10.6119,  ...,  1.3639,  2.2972,  0.8101],\n",
       "         [ 8.1118, -4.2742, 12.2622,  ...,  0.1729,  1.6734,  0.6476],\n",
       "         [-2.8165,  2.4560, -1.3008,  ...,  0.9983, -0.4333,  2.5262],\n",
       "         ...,\n",
       "         [ 8.1539, -5.4359,  5.8689,  ...,  1.0349,  4.1734,  1.4926],\n",
       "         [ 0.8112, -3.6817,  5.7369,  ..., -0.6757,  2.3840,  1.2404],\n",
       "         [ 2.7066, -3.5683,  6.5834,  ...,  0.4324,  2.7151,  1.5720]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c07884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {},
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b099ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]))\n",
    "torch.max(torch.abs(encoder_input - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5533cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa7bd86ac70>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]).numpy()[0] > 1e-5)\n",
    "plt.imshow(torch.abs(encoder_input - hf_outputs[0]).numpy()[0] > 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf93df",
   "metadata": {},
   "source": [
    "The line that is different is where the padding mask goes from 1 to 0; the original model handles this a little different than we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b92b758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"extract_features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d18286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.abs(encoder_input - hf_outputs[\"last_hidden_state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {},
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47c86f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with the original's speech prenet input:\n",
    "# with torch.no_grad():\n",
    "#     encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b8b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with our input, which is slightly different (see above)\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(hf_encoder_input, ~hf_encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41ff76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([215, 1, 768])\n",
      "encoder_padding_mask shape torch.Size([1, 215])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([215, 1, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a38979d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412, -0.1983, -0.5504,  ...,  0.3371, -0.7611,  0.2468],\n",
       "         [-0.3532, -0.1819, -0.6027,  ...,  0.2886, -0.8502,  0.1505],\n",
       "         [-0.4093, -0.1701, -0.6913,  ...,  0.2886, -0.7586, -0.0191],\n",
       "         ...,\n",
       "         [ 0.2191,  0.1308, -0.2119,  ...,  0.2059, -0.7364,  0.2486],\n",
       "         [ 0.0395,  0.0048, -0.3112,  ...,  0.1692, -0.7535,  0.2188],\n",
       "         [-0.0338, -0.1440, -0.3480,  ...,  0.2844, -0.7159,  0.1947]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9b9dda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -9.4632,  -9.6230,  -9.6073,  ...,  -9.5406,  -9.3881,  12.4264],\n",
       "         [ -9.8169,  -9.9027,  -9.9673,  ...,  -9.8993,  -9.7949,  13.0776],\n",
       "         [-10.9712, -11.0219, -11.0252,  ..., -11.0365, -10.9394,  14.6879],\n",
       "         ...,\n",
       "         [ -8.2366,  -8.3445,  -8.3155,  ...,  -8.4186,  -8.0964,  10.4814],\n",
       "         [ -8.7586,  -8.9771,  -8.8052,  ...,  -8.8178,  -8.5105,  11.1125],\n",
       "         [ -9.6384, -10.0383,  -9.7550,  ...,  -9.7551,  -9.3112,  11.0622]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cde6dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exact same inputs as the original model:\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speecht5(\n",
    "#          inputs_embeds=encoder_input,\n",
    "#          attention_mask=(~encoder_padding_mask),\n",
    "#      )\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28f529a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.wrapped_encoder(\n",
    "         hidden_states=hf_encoder_input,\n",
    "         attention_mask=hf_encoder_attention_mask,\n",
    "#          input_values=inputs.input_values,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb12e71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.3412, -0.1983, -0.5504,  ...,  0.3371, -0.7611,  0.2468],\n",
       "         [-0.3532, -0.1819, -0.6027,  ...,  0.2886, -0.8502,  0.1505],\n",
       "         [-0.4093, -0.1701, -0.6913,  ...,  0.2886, -0.7586, -0.0191],\n",
       "         ...,\n",
       "         [ 0.2191,  0.1308, -0.2119,  ...,  0.2059, -0.7364,  0.2486],\n",
       "         [ 0.0395,  0.0048, -0.3112,  ...,  0.1692, -0.7535,  0.2188],\n",
       "         [-0.0338, -0.1440, -0.3480,  ...,  0.2844, -0.7159,  0.1947]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06fb675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8fe9914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 215, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69eda6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412, -0.1983, -0.5504,  ...,  0.3371, -0.7611,  0.2468],\n",
       "         [-0.3532, -0.1819, -0.6027,  ...,  0.2886, -0.8502,  0.1505],\n",
       "         [-0.4093, -0.1701, -0.6913,  ...,  0.2886, -0.7586, -0.0191],\n",
       "         ...,\n",
       "         [ 0.2191,  0.1308, -0.2119,  ...,  0.2059, -0.7364,  0.2486],\n",
       "         [ 0.0395,  0.0048, -0.3112,  ...,  0.1692, -0.7535,  0.2188],\n",
       "         [-0.0338, -0.1440, -0.3480,  ...,  0.2844, -0.7159,  0.1947]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a747e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518e152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca490a88",
   "metadata": {},
   "source": [
    "## Verify CTC model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98055be9",
   "metadata": {},
   "source": [
    "This model only needs the encoder portion.\n",
    "\n",
    "This uses the same checkpoint as before: `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Run the following to convert, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task ctc \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69ca25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model_ctc = SpeechT5ForCTC(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a32978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_ctc = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc\"\n",
    "hf_model_ctc = SpeechT5ForCTC.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b28453ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc were not used when initializing SpeechT5Model: ['speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight', 'speecht5.encoder.prenet.masked_spec_embed', 'speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight', 'speecht5.encoder.prenet.pos_sinusoidal_embed.weights', 'speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_v', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight', 'speecht5.encoder.prenet.feature_projection.projection.bias', 'lm_head.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight', 'lm_head.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.bias', 'speecht5.encoder.prenet.feature_projection.projection.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_g', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight', 'speecht5.encoder.prenet.feature_projection.layer_norm.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias', 'speecht5.encoder.prenet.feature_projection.layer_norm.bias']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SpeechT5Model were not initialized from the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc and are newly initialized: ['speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.4.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.output_dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# This should also work:\n",
    "hf_model_ctc_naked = SpeechT5Model.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8d81d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutput"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the full model:\n",
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(**inputs)\n",
    "\n",
    "# Run without attention_mask:\n",
    "# with torch.no_grad():\n",
    "#     hf_outputs = hf_model_ctc(input_values=inputs[\"input_values\"])\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de48ff1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2)) - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "32fabb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(hf_outputs[0], dim=-1, dtype=torch.float32)\n",
    "probs = probs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "267f80ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4, 46, 80, 80, 16, 80, 80, 12, 12,  6,  6, 80, 80, 80, 80,\n",
       "         80,  4,  4,  4,  6,  6, 11, 11, 13, 13, 13, 80, 16, 16, 80, 80, 80, 80,\n",
       "         12, 12,  6,  6, 80, 80, 80, 80, 80, 80, 80, 80,  4,  4,  4, 80, 80,  7,\n",
       "          9, 14, 14,  4,  4, 80, 24, 80, 80, 80, 80,  5, 13, 13, 80, 80, 13, 13,\n",
       "         80, 22, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4, 80,  7,  7,  9, 14,\n",
       "         14, 80,  4,  4,  4, 27, 80, 80, 10, 80, 17, 17,  6,  6,  6,  8, 13, 13,\n",
       "         13, 22, 22, 80, 80,  4,  4,  6,  8,  8, 80,  4,  6,  6, 11,  5,  5,  4,\n",
       "         12, 12, 80,  6,  6, 13, 13, 80, 80,  8, 80,  9,  9, 80, 21, 21, 80, 80,\n",
       "          5, 13, 13, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "90ca3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tgt_dict.string(probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5bcf370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                jusstt   tthhrrruusstt   andd  perrrry       aandd   vicctttorrryy  too tthee ssttrronnggerr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(probs.shape[0]):\n",
    "    print(tokenizer.decode(tgt_dict.string(probs[i])).replace(\"<ctc_blank>\", \"\"))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2067e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e436f56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'SpeechT5CTCTokenizer'. \n",
      "The class this function is called from is 'SpeechT5Tokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc', vocab_size=79, model_max_len=450, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer_ctc = SpeechT5Tokenizer.from_pretrained(model_checkpoint_ctc)\n",
    "hf_tokenizer_ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "53a05e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf_tokenizer_ctc.decode(probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0edc5fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> j <ctc_blank> <ctc_blank> u <ctc_blank> <ctc_blank> sstt <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> tthhrrr <ctc_blank> uu <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> sstt <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank>  <ctc_blank> <ctc_blank> andd <ctc_blank> p <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> err <ctc_blank> <ctc_blank> rr <ctc_blank> y <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank>  <ctc_blank> aandd <ctc_blank> v <ctc_blank> <ctc_blank> i <ctc_blank> cctttorrryy <ctc_blank> <ctc_blank> too <ctc_blank> tthee ss <ctc_blank> ttrr <ctc_blank> <ctc_blank> o <ctc_blank> nn <ctc_blank> gg <ctc_blank> <ctc_blank> err <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank> <ctc_blank>']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer_ctc.batch_decode(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559056f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "75b0c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer_ctc = SpeechT5CTCTokenizer.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9739fb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.speecht5.tokenization_speecht5.SpeechT5CTCTokenizer"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "hf_tokenizer_ctc = AutoTokenizer.from_pretrained(model_checkpoint_ctc)\n",
    "type(hf_tokenizer_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "732acc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just thrust and perry and victory to the stronger'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer_ctc.convert_tokens_to_string(\n",
    "    hf_tokenizer_ctc2.convert_ids_to_tokens(probs[0]),\n",
    "    #spaces_between_special_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "79d8d0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just thrust and perry and victory to the stronger'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer_ctc.decode(probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a4b6a1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just thrust and perry and victory to the stronger']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer_ctc.batch_decode(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aac0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60229b2c",
   "metadata": {},
   "source": [
    "Calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d6f22609",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(\n",
    "         **inputs, \n",
    "         labels=torch.tensor(\n",
    "           [[ 46, 16, 12,  6,  4,  6, 11, 13, 16, 12,  6,  4,  7,  9, 14,  4,\n",
    "         24,  7, 13, 13, 22,  4,  7,  9, 14,  4, 27, 10, 17,  6,  8, 13, 22,  4,\n",
    "          6,  8,  4,  6, 11,  5,  4, 12,  6, 13,  8,  9, 21,  5, 13, ]]\n",
    "         ),\n",
    "         output_hidden_states=True,\n",
    "         return_dict=True,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5f57c285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1208.0262)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9dd3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {},
   "source": [
    "## Verify text decoder prenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ba48d",
   "metadata": {},
   "source": [
    "First this calls `text_decoder_prenet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3747a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inputs.input_values.size(0)\n",
    "beam_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "28dcb259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.tensor([2, 4, 46, 16, 12, 16] * beam_size * batch_size).reshape(beam_size * batch_size, -1)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e6bd8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one has padding (token_id = 1)\n",
    "# The results will be different with the HF implementation because\n",
    "# we don't set the attention_mask to 0 for padding tokens\n",
    "# tokens = torch.tensor([2, 4, 46, 16, 1, 12] * beam_size * batch_size).reshape(beam_size * batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f09f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60cc6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a sequence length of 1\n",
    "# with torch.no_grad():\n",
    "#     prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "725cfc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: misleading name; these are not the actual tokens but their embeddings!\n",
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "84764c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81026d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "77134734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.prenet(\n",
    "         input_ids=tokens,\n",
    "#          attention_mask=hf_encoder_attention_mask,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "#          past_key_values=[(torch.ones(5, 1, 1), torch.ones(5, 1, 1))],\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f007852",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds_hf, decoder_attention_mask = hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ea4dd4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeds_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "21f6193b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(prev_output_tokens - token_embeds_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a827caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d7a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {},
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "42a4fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = source.size(0)\n",
    "new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "encoder_outs = orig_model.encoder.reorder_encoder_out(encoder_output, new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a98652fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6229b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_outs,\n",
    "        incremental_state=incremental_state,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0a190958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fea0351e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        [[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        [[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "05117956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b593692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9d486f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.wrapped_decoder(\n",
    "         hidden_states=prev_output_tokens,\n",
    "         attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_outs[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ea512cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'past_key_values']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d82c369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 768])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eb6282bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        [[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        [[-0.4042, -0.3464, -0.1654,  ..., -0.2053, -0.4237,  0.0788],\n",
       "         [-0.4293, -0.6809, -1.6442,  ..., -0.3035,  0.1346,  0.0505],\n",
       "         [ 0.2834, -0.7885, -1.0885,  ...,  0.2483,  0.1978,  0.5294],\n",
       "         [ 0.7349, -1.1453, -1.6684,  ...,  0.3394,  0.5876, -0.4216],\n",
       "         [ 0.6584, -1.1614, -0.8039,  ..., -0.5429,  0.5048, -0.2464],\n",
       "         [ 0.5320, -0.6075, -0.9035,  ...,  0.6435,  0.6379, -0.0853]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]],\n",
       "\n",
       "        [[-0.4469, -0.2283, -0.0834,  ..., -0.2076, -0.4499,  0.0166],\n",
       "         [ 0.0655, -0.4424, -1.4536,  ..., -0.2090, -0.1812, -0.1850],\n",
       "         [ 0.6125, -0.1873, -0.7856,  ...,  0.1821, -0.3962,  0.9259],\n",
       "         [ 0.8581, -0.5432, -1.7023,  ..., -0.0384,  0.2454,  0.0275],\n",
       "         [ 0.8709, -0.2559, -0.4044,  ..., -0.4054, -0.4117, -0.0879],\n",
       "         [ 0.6939, -0.6869, -0.7209,  ...,  0.2299,  0.0294,  0.2049]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3e254cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6689e-06)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_output - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify text decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs = orig_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 81])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7d6de14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4168e+01, -2.2556e+01,  5.4097e-01, -2.2586e+01,  1.7349e+01,\n",
       "          4.6760e-02,  6.2041e-01, -9.4606e-01, -4.4315e-01, -1.8812e+00,\n",
       "         -3.9952e-01,  2.1893e-01,  3.9134e-01, -1.3906e+00, -2.2705e+00,\n",
       "         -1.5500e+00, -1.0440e+00, -2.6232e+00, -7.7046e-01, -4.0442e-02,\n",
       "          1.1401e+00,  2.4883e-01, -3.7950e+00, -2.2715e+01, -8.6929e-01,\n",
       "          6.3732e-01, -2.2659e+01, -2.8817e+00, -3.3690e+00, -2.2606e+01,\n",
       "         -2.2637e+01,  9.3877e-02, -2.2744e+01, -2.2613e+01, -2.2609e+01,\n",
       "         -2.2670e+01, -2.2651e+01, -5.3913e+00, -2.2602e+01, -2.2722e+01,\n",
       "         -2.2586e+01, -2.2674e+01, -2.2655e+01, -2.2676e+01, -2.2658e+01,\n",
       "         -2.4368e+00, -2.3410e+00, -2.2580e+01, -2.2609e+01, -2.2735e+01,\n",
       "         -2.2613e+01, -2.2698e+01, -2.2605e+01, -2.2590e+01, -2.2633e+01,\n",
       "         -2.2558e+01, -2.2639e+01, -6.2336e+00, -2.2628e+01, -2.2604e+01,\n",
       "         -2.2654e+01, -2.2722e+01, -2.2647e+01, -2.2622e+01, -2.2642e+01,\n",
       "         -2.2666e+01, -2.2624e+01, -2.2692e+01, -2.2651e+01, -2.2621e+01,\n",
       "         -2.2545e+01, -2.2643e+01, -2.2597e+01, -2.2617e+01, -2.2579e+01,\n",
       "         -2.2647e+01, -2.2589e+01, -2.2632e+01, -2.2609e+01, -2.1264e+01,\n",
       "         -2.2597e+01],\n",
       "        [-1.9043e+01, -1.6790e+01, -1.0073e+01, -1.6704e+01, -2.1654e+00,\n",
       "         -1.6095e-01,  2.0542e+00,  1.3301e+00,  8.8607e-01,  3.1429e+00,\n",
       "          1.7273e+00,  1.7962e+00,  2.6194e+00,  1.9622e+00,  1.1515e+00,\n",
       "          2.9886e-01, -3.7260e+00, -1.7935e+00,  1.7473e+01,  1.4869e+00,\n",
       "          2.9626e+00, -5.8204e-01, -1.0603e+00, -1.6751e+01, -2.1045e-01,\n",
       "          1.1617e+00, -1.6724e+01,  1.3194e-01, -1.3141e+00, -1.6618e+01,\n",
       "         -1.6784e+01, -7.2883e+00, -1.6691e+01, -1.6631e+01, -1.6725e+01,\n",
       "         -1.6714e+01, -1.6689e+01, -8.9997e+00, -1.6611e+01, -1.6721e+01,\n",
       "         -1.6792e+01, -1.6688e+01, -1.6791e+01, -1.6748e+01, -1.6654e+01,\n",
       "         -4.4007e+00,  1.5568e-02, -1.6610e+01, -1.6773e+01, -1.6620e+01,\n",
       "         -1.6670e+01, -1.6731e+01, -1.6700e+01, -1.6577e+01, -1.6680e+01,\n",
       "         -1.6566e+01, -1.6698e+01, -3.7797e+00, -1.6731e+01, -1.6726e+01,\n",
       "         -1.6751e+01, -1.6781e+01, -1.6745e+01, -1.6605e+01, -1.6571e+01,\n",
       "         -1.6787e+01, -1.6671e+01, -1.6678e+01, -1.6674e+01, -1.6768e+01,\n",
       "         -1.6603e+01, -1.6593e+01, -1.6689e+01, -1.6522e+01, -1.6743e+01,\n",
       "         -1.6653e+01, -1.6763e+01, -1.6809e+01, -1.6687e+01, -1.5258e+01,\n",
       "         -1.6675e+01],\n",
       "        [-2.3547e+01, -1.7763e+01, -3.8741e+00, -1.8379e+01,  4.6679e+00,\n",
       "          8.2100e+00, -1.0609e+00,  8.2360e+00,  6.7864e+00, -5.4316e-01,\n",
       "          1.1049e+01, -5.6321e+00, -1.6484e+00, -1.5931e+00, -5.5817e+00,\n",
       "         -4.7685e+00,  1.2519e+01, -4.8425e+00,  6.5959e-01, -5.7096e+00,\n",
       "         -7.5465e+00, -7.1944e+00,  1.1202e-01, -1.8384e+01, -2.8636e+00,\n",
       "         -8.6657e+00, -1.8400e+01, -2.7559e+00, -7.7308e+00, -1.8319e+01,\n",
       "         -1.8435e+01,  1.0387e+00, -1.8136e+01, -1.8234e+01, -1.8274e+01,\n",
       "         -1.8311e+01, -1.8356e+01, -1.0118e+01, -1.8370e+01, -1.8282e+01,\n",
       "         -1.8412e+01, -1.8292e+01, -1.8378e+01, -1.8441e+01, -1.8355e+01,\n",
       "         -8.4691e+00, -6.2354e+00, -1.8301e+01, -1.8260e+01, -1.8281e+01,\n",
       "         -1.8252e+01, -1.8347e+01, -1.8376e+01, -1.8136e+01, -1.8382e+01,\n",
       "         -1.8192e+01, -1.8309e+01, -7.9916e+00, -1.8421e+01, -1.8375e+01,\n",
       "         -1.8315e+01, -1.8357e+01, -1.8441e+01, -1.8317e+01, -1.8351e+01,\n",
       "         -1.8351e+01, -1.8408e+01, -1.8292e+01, -1.8241e+01, -1.8416e+01,\n",
       "         -1.8368e+01, -1.8337e+01, -1.8390e+01, -1.8279e+01, -1.8236e+01,\n",
       "         -1.8186e+01, -1.8362e+01, -1.8411e+01, -1.8322e+01, -1.7774e+01,\n",
       "         -1.8253e+01],\n",
       "        [-2.2486e+01, -1.6404e+01, -6.4348e+00, -1.6746e+01,  8.9949e-01,\n",
       "         -4.2801e+00,  1.9602e+00,  1.0625e+00, -6.0304e+00,  6.0616e+00,\n",
       "          3.1471e+00, -1.6299e+00,  1.1612e+01,  2.5203e+00,  7.0812e+00,\n",
       "          9.1620e+00, -4.3276e+00, -3.3373e+00,  5.9013e+00, -1.2608e+00,\n",
       "         -5.8013e+00, -2.0902e-01, -3.5005e+00, -1.6746e+01,  6.6611e+00,\n",
       "          3.9378e+00, -1.6607e+01,  1.7803e+00, -4.4496e+00, -1.6785e+01,\n",
       "         -1.6694e+01, -5.0086e+00, -1.6821e+01, -1.6751e+01, -1.6729e+01,\n",
       "         -1.6822e+01, -1.6882e+01, -4.3096e+00, -1.6694e+01, -1.6740e+01,\n",
       "         -1.6745e+01, -1.6832e+01, -1.6796e+01, -1.6810e+01, -1.6876e+01,\n",
       "         -4.3033e+00, -3.1710e+00, -1.6942e+01, -1.6760e+01, -1.6683e+01,\n",
       "         -1.6764e+01, -1.6752e+01, -1.6687e+01, -1.6698e+01, -1.6927e+01,\n",
       "         -1.6769e+01, -1.6844e+01, -3.7531e+00, -1.6823e+01, -1.6781e+01,\n",
       "         -1.6878e+01, -1.6840e+01, -1.6772e+01, -1.6743e+01, -1.6781e+01,\n",
       "         -1.6845e+01, -1.6749e+01, -1.6924e+01, -1.6653e+01, -1.6760e+01,\n",
       "         -1.6856e+01, -1.6797e+01, -1.6971e+01, -1.6886e+01, -1.6766e+01,\n",
       "         -1.6745e+01, -1.6744e+01, -1.6852e+01, -1.6832e+01, -1.5578e+01,\n",
       "         -1.6644e+01],\n",
       "        [-9.5296e+00, -1.2752e+01, -3.4893e+00, -1.2767e+01,  3.0611e+00,\n",
       "         -2.2461e-01,  2.1057e+01, -1.7268e+00, -2.7778e-01, -8.3309e-01,\n",
       "          1.2230e+00,  4.0795e-01,  4.5440e+00, -3.5393e+00,  1.2538e+00,\n",
       "         -4.1438e+00,  1.6193e+00,  3.5282e+00, -2.9716e+00,  1.2647e-01,\n",
       "         -4.9344e+00, -3.9908e+00, -5.4732e+00, -1.2774e+01,  7.8165e+00,\n",
       "         -3.4972e+00, -1.2680e+01, -5.0850e+00,  2.7922e+00, -1.2757e+01,\n",
       "         -1.2756e+01, -2.4174e+00, -1.2711e+01, -1.2820e+01, -1.2787e+01,\n",
       "         -1.2801e+01, -1.2719e+01, -7.6775e+00, -1.2788e+01, -1.2685e+01,\n",
       "         -1.2682e+01, -1.2738e+01, -1.2773e+01, -1.2815e+01, -1.2762e+01,\n",
       "          2.4289e+00, -5.5745e+00, -1.2719e+01, -1.2909e+01, -1.2759e+01,\n",
       "         -1.2833e+01, -1.2759e+01, -1.2749e+01, -1.2570e+01, -1.2817e+01,\n",
       "         -1.2820e+01, -1.2875e+01, -3.7295e+00, -1.2872e+01, -1.2890e+01,\n",
       "         -1.2752e+01, -1.2635e+01, -1.2750e+01, -1.2593e+01, -1.2744e+01,\n",
       "         -1.2788e+01, -1.2700e+01, -1.2930e+01, -1.2680e+01, -1.2771e+01,\n",
       "         -1.2743e+01, -1.2986e+01, -1.2817e+01, -1.2855e+01, -1.2810e+01,\n",
       "         -1.2831e+01, -1.2743e+01, -1.2848e+01, -1.2743e+01, -1.2448e+01,\n",
       "         -1.2683e+01],\n",
       "        [-2.4212e+01, -1.6132e+01, -1.8114e+00, -1.6146e+01,  5.8716e+00,\n",
       "          2.2145e+00,  2.4628e-01,  5.1427e+00, -7.3206e+00, -1.6881e+00,\n",
       "          1.3354e+00,  6.1891e-01,  4.9448e+00,  4.5401e+00, -2.6384e+00,\n",
       "          2.3586e+00, -6.9156e+00,  7.5051e-01,  2.8483e+00,  1.1872e+00,\n",
       "         -4.4748e+00,  3.9956e-01, -7.1030e+00, -1.6008e+01,  7.3428e+00,\n",
       "         -1.7044e+00, -1.5921e+01, -1.6534e+00,  1.4140e+00, -1.6134e+01,\n",
       "         -1.6120e+01, -2.2550e+00, -1.6123e+01, -1.6095e+01, -1.5970e+01,\n",
       "         -1.6187e+01, -1.6026e+01, -5.2942e+00, -1.5873e+01, -1.6145e+01,\n",
       "         -1.6020e+01, -1.6156e+01, -1.6066e+01, -1.6032e+01, -1.6148e+01,\n",
       "         -8.2226e-01, -6.0372e+00, -1.6184e+01, -1.6105e+01, -1.5980e+01,\n",
       "         -1.6257e+01, -1.6009e+01, -1.6110e+01, -1.5955e+01, -1.6270e+01,\n",
       "         -1.6133e+01, -1.6255e+01, -9.2853e+00, -1.6042e+01, -1.6079e+01,\n",
       "         -1.6159e+01, -1.6172e+01, -1.5961e+01, -1.6141e+01, -1.6138e+01,\n",
       "         -1.6220e+01, -1.6157e+01, -1.6292e+01, -1.6040e+01, -1.6088e+01,\n",
       "         -1.6151e+01, -1.6006e+01, -1.6257e+01, -1.6041e+01, -1.6106e+01,\n",
       "         -1.6049e+01, -1.6141e+01, -1.6144e+01, -1.6224e+01, -1.4362e+01,\n",
       "         -1.6027e+01]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ba7571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs_hf = hf_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5623a791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 81])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "64f0c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4168e+01, -2.2556e+01,  5.4097e-01, -2.2586e+01,  1.7349e+01,\n",
       "          4.6760e-02,  6.2041e-01, -9.4606e-01, -4.4315e-01, -1.8812e+00,\n",
       "         -3.9952e-01,  2.1893e-01,  3.9134e-01, -1.3906e+00, -2.2705e+00,\n",
       "         -1.5500e+00, -1.0440e+00, -2.6232e+00, -7.7046e-01, -4.0442e-02,\n",
       "          1.1401e+00,  2.4883e-01, -3.7950e+00, -2.2715e+01, -8.6929e-01,\n",
       "          6.3732e-01, -2.2659e+01, -2.8817e+00, -3.3690e+00, -2.2606e+01,\n",
       "         -2.2637e+01,  9.3877e-02, -2.2744e+01, -2.2613e+01, -2.2609e+01,\n",
       "         -2.2670e+01, -2.2651e+01, -5.3913e+00, -2.2602e+01, -2.2722e+01,\n",
       "         -2.2586e+01, -2.2674e+01, -2.2655e+01, -2.2676e+01, -2.2658e+01,\n",
       "         -2.4368e+00, -2.3410e+00, -2.2580e+01, -2.2609e+01, -2.2735e+01,\n",
       "         -2.2613e+01, -2.2698e+01, -2.2605e+01, -2.2590e+01, -2.2633e+01,\n",
       "         -2.2558e+01, -2.2639e+01, -6.2336e+00, -2.2628e+01, -2.2604e+01,\n",
       "         -2.2654e+01, -2.2722e+01, -2.2647e+01, -2.2622e+01, -2.2642e+01,\n",
       "         -2.2666e+01, -2.2624e+01, -2.2692e+01, -2.2651e+01, -2.2621e+01,\n",
       "         -2.2545e+01, -2.2643e+01, -2.2597e+01, -2.2617e+01, -2.2579e+01,\n",
       "         -2.2647e+01, -2.2589e+01, -2.2632e+01, -2.2609e+01, -2.1264e+01,\n",
       "         -2.2597e+01],\n",
       "        [-1.9043e+01, -1.6790e+01, -1.0073e+01, -1.6704e+01, -2.1654e+00,\n",
       "         -1.6095e-01,  2.0542e+00,  1.3301e+00,  8.8607e-01,  3.1429e+00,\n",
       "          1.7273e+00,  1.7962e+00,  2.6194e+00,  1.9622e+00,  1.1515e+00,\n",
       "          2.9886e-01, -3.7260e+00, -1.7935e+00,  1.7473e+01,  1.4869e+00,\n",
       "          2.9626e+00, -5.8204e-01, -1.0603e+00, -1.6751e+01, -2.1045e-01,\n",
       "          1.1617e+00, -1.6724e+01,  1.3194e-01, -1.3141e+00, -1.6618e+01,\n",
       "         -1.6784e+01, -7.2883e+00, -1.6691e+01, -1.6631e+01, -1.6725e+01,\n",
       "         -1.6714e+01, -1.6689e+01, -8.9997e+00, -1.6611e+01, -1.6721e+01,\n",
       "         -1.6792e+01, -1.6688e+01, -1.6791e+01, -1.6748e+01, -1.6654e+01,\n",
       "         -4.4007e+00,  1.5568e-02, -1.6610e+01, -1.6773e+01, -1.6620e+01,\n",
       "         -1.6670e+01, -1.6731e+01, -1.6700e+01, -1.6577e+01, -1.6680e+01,\n",
       "         -1.6566e+01, -1.6698e+01, -3.7797e+00, -1.6731e+01, -1.6726e+01,\n",
       "         -1.6751e+01, -1.6781e+01, -1.6745e+01, -1.6605e+01, -1.6571e+01,\n",
       "         -1.6787e+01, -1.6671e+01, -1.6678e+01, -1.6674e+01, -1.6768e+01,\n",
       "         -1.6603e+01, -1.6593e+01, -1.6689e+01, -1.6522e+01, -1.6743e+01,\n",
       "         -1.6653e+01, -1.6763e+01, -1.6809e+01, -1.6687e+01, -1.5258e+01,\n",
       "         -1.6675e+01],\n",
       "        [-2.3547e+01, -1.7763e+01, -3.8741e+00, -1.8379e+01,  4.6679e+00,\n",
       "          8.2100e+00, -1.0609e+00,  8.2360e+00,  6.7864e+00, -5.4316e-01,\n",
       "          1.1049e+01, -5.6321e+00, -1.6484e+00, -1.5931e+00, -5.5817e+00,\n",
       "         -4.7685e+00,  1.2519e+01, -4.8425e+00,  6.5959e-01, -5.7096e+00,\n",
       "         -7.5465e+00, -7.1944e+00,  1.1202e-01, -1.8384e+01, -2.8636e+00,\n",
       "         -8.6657e+00, -1.8400e+01, -2.7559e+00, -7.7308e+00, -1.8319e+01,\n",
       "         -1.8435e+01,  1.0387e+00, -1.8136e+01, -1.8234e+01, -1.8274e+01,\n",
       "         -1.8311e+01, -1.8356e+01, -1.0118e+01, -1.8370e+01, -1.8282e+01,\n",
       "         -1.8412e+01, -1.8292e+01, -1.8378e+01, -1.8441e+01, -1.8355e+01,\n",
       "         -8.4691e+00, -6.2354e+00, -1.8301e+01, -1.8260e+01, -1.8281e+01,\n",
       "         -1.8252e+01, -1.8347e+01, -1.8376e+01, -1.8136e+01, -1.8382e+01,\n",
       "         -1.8192e+01, -1.8309e+01, -7.9916e+00, -1.8421e+01, -1.8375e+01,\n",
       "         -1.8315e+01, -1.8357e+01, -1.8441e+01, -1.8317e+01, -1.8351e+01,\n",
       "         -1.8351e+01, -1.8408e+01, -1.8292e+01, -1.8241e+01, -1.8416e+01,\n",
       "         -1.8368e+01, -1.8337e+01, -1.8390e+01, -1.8279e+01, -1.8236e+01,\n",
       "         -1.8186e+01, -1.8362e+01, -1.8411e+01, -1.8322e+01, -1.7774e+01,\n",
       "         -1.8253e+01],\n",
       "        [-2.2486e+01, -1.6404e+01, -6.4348e+00, -1.6746e+01,  8.9949e-01,\n",
       "         -4.2801e+00,  1.9602e+00,  1.0625e+00, -6.0304e+00,  6.0616e+00,\n",
       "          3.1471e+00, -1.6299e+00,  1.1612e+01,  2.5203e+00,  7.0812e+00,\n",
       "          9.1620e+00, -4.3276e+00, -3.3373e+00,  5.9013e+00, -1.2608e+00,\n",
       "         -5.8013e+00, -2.0902e-01, -3.5005e+00, -1.6746e+01,  6.6611e+00,\n",
       "          3.9378e+00, -1.6607e+01,  1.7803e+00, -4.4496e+00, -1.6785e+01,\n",
       "         -1.6694e+01, -5.0086e+00, -1.6821e+01, -1.6751e+01, -1.6729e+01,\n",
       "         -1.6822e+01, -1.6882e+01, -4.3096e+00, -1.6694e+01, -1.6740e+01,\n",
       "         -1.6745e+01, -1.6832e+01, -1.6796e+01, -1.6810e+01, -1.6876e+01,\n",
       "         -4.3033e+00, -3.1710e+00, -1.6942e+01, -1.6760e+01, -1.6683e+01,\n",
       "         -1.6764e+01, -1.6752e+01, -1.6687e+01, -1.6698e+01, -1.6927e+01,\n",
       "         -1.6769e+01, -1.6844e+01, -3.7531e+00, -1.6823e+01, -1.6781e+01,\n",
       "         -1.6878e+01, -1.6840e+01, -1.6772e+01, -1.6743e+01, -1.6781e+01,\n",
       "         -1.6845e+01, -1.6749e+01, -1.6924e+01, -1.6653e+01, -1.6760e+01,\n",
       "         -1.6856e+01, -1.6797e+01, -1.6971e+01, -1.6886e+01, -1.6766e+01,\n",
       "         -1.6745e+01, -1.6744e+01, -1.6852e+01, -1.6832e+01, -1.5578e+01,\n",
       "         -1.6644e+01],\n",
       "        [-9.5296e+00, -1.2752e+01, -3.4893e+00, -1.2767e+01,  3.0611e+00,\n",
       "         -2.2461e-01,  2.1057e+01, -1.7268e+00, -2.7778e-01, -8.3309e-01,\n",
       "          1.2230e+00,  4.0795e-01,  4.5440e+00, -3.5393e+00,  1.2538e+00,\n",
       "         -4.1438e+00,  1.6193e+00,  3.5282e+00, -2.9716e+00,  1.2647e-01,\n",
       "         -4.9344e+00, -3.9908e+00, -5.4732e+00, -1.2774e+01,  7.8165e+00,\n",
       "         -3.4972e+00, -1.2680e+01, -5.0850e+00,  2.7922e+00, -1.2757e+01,\n",
       "         -1.2756e+01, -2.4174e+00, -1.2711e+01, -1.2820e+01, -1.2787e+01,\n",
       "         -1.2801e+01, -1.2719e+01, -7.6775e+00, -1.2788e+01, -1.2685e+01,\n",
       "         -1.2682e+01, -1.2738e+01, -1.2773e+01, -1.2815e+01, -1.2762e+01,\n",
       "          2.4289e+00, -5.5745e+00, -1.2719e+01, -1.2909e+01, -1.2759e+01,\n",
       "         -1.2833e+01, -1.2759e+01, -1.2749e+01, -1.2570e+01, -1.2817e+01,\n",
       "         -1.2820e+01, -1.2875e+01, -3.7295e+00, -1.2872e+01, -1.2890e+01,\n",
       "         -1.2752e+01, -1.2635e+01, -1.2750e+01, -1.2593e+01, -1.2744e+01,\n",
       "         -1.2788e+01, -1.2700e+01, -1.2930e+01, -1.2680e+01, -1.2771e+01,\n",
       "         -1.2743e+01, -1.2986e+01, -1.2817e+01, -1.2855e+01, -1.2810e+01,\n",
       "         -1.2831e+01, -1.2743e+01, -1.2848e+01, -1.2743e+01, -1.2448e+01,\n",
       "         -1.2683e+01],\n",
       "        [-2.4212e+01, -1.6132e+01, -1.8114e+00, -1.6146e+01,  5.8716e+00,\n",
       "          2.2145e+00,  2.4628e-01,  5.1427e+00, -7.3206e+00, -1.6881e+00,\n",
       "          1.3354e+00,  6.1891e-01,  4.9448e+00,  4.5401e+00, -2.6384e+00,\n",
       "          2.3586e+00, -6.9156e+00,  7.5051e-01,  2.8483e+00,  1.1872e+00,\n",
       "         -4.4748e+00,  3.9956e-01, -7.1030e+00, -1.6008e+01,  7.3428e+00,\n",
       "         -1.7044e+00, -1.5921e+01, -1.6534e+00,  1.4140e+00, -1.6134e+01,\n",
       "         -1.6120e+01, -2.2550e+00, -1.6123e+01, -1.6095e+01, -1.5970e+01,\n",
       "         -1.6187e+01, -1.6026e+01, -5.2942e+00, -1.5873e+01, -1.6145e+01,\n",
       "         -1.6020e+01, -1.6156e+01, -1.6066e+01, -1.6032e+01, -1.6148e+01,\n",
       "         -8.2226e-01, -6.0372e+00, -1.6184e+01, -1.6105e+01, -1.5980e+01,\n",
       "         -1.6257e+01, -1.6009e+01, -1.6110e+01, -1.5955e+01, -1.6270e+01,\n",
       "         -1.6133e+01, -1.6255e+01, -9.2853e+00, -1.6042e+01, -1.6079e+01,\n",
       "         -1.6159e+01, -1.6172e+01, -1.5961e+01, -1.6141e+01, -1.6138e+01,\n",
       "         -1.6220e+01, -1.6157e+01, -1.6292e+01, -1.6040e+01, -1.6088e+01,\n",
       "         -1.6151e+01, -1.6006e+01, -1.6257e+01, -1.6041e+01, -1.6106e+01,\n",
       "         -1.6049e+01, -1.6141e+01, -1.6144e+01, -1.6224e+01, -1.4362e+01,\n",
       "         -1.6027e+01]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6eeff463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(lprobs - lprobs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4e9dc",
   "metadata": {},
   "source": [
    "Run the full model to make sure this doesn't give any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         decoder_input_ids=torch.tensor([[3, 4, 5]]),\n",
    "         #decoder_input_ids=torch.tensor([[3, 4, 5], [2, 2, 2]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "677718c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logits', 'past_key_values', 'encoder_last_hidden_state']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7f2bcec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 81])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf0244b",
   "metadata": {},
   "source": [
    "Also calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1b7bd1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[2,  4, 18, 10, 12,  6,  5]]),\n",
    "         labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13]]),\n",
    "         #labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13], [4, 18, 10, 12,  6,  5, 13]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f34dd766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 81])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4edf2044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.1307e-06)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195961eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2ce14a",
   "metadata": {},
   "source": [
    "Generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "72ccdbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b2fc2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = hf_model.generate(inputs.input_values, max_length=100)\n",
    "# hf_outputs = hf_model.generate(inputs.input_values, num_beams=5, max_length=100) #, bos_token_id=2)\n",
    "# hf_outputs = hf_model.generate(torch.rand(1, 10000), num_beams=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "015c80b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 92])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ce7e1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4, 18, 10, 12,  6,  5, 13,  4, 45, 16, 10, 15,  6,  5, 13,  4, 10,\n",
       "         12,  4,  6, 11,  5,  4,  7, 24,  8, 12,  6, 15,  5,  4,  8, 19,  4,  6,\n",
       "         11,  5,  4, 18, 10, 14, 14, 15,  5,  4, 17, 15,  7, 12, 12,  5, 12,  4,\n",
       "          7,  9, 14,  4, 20,  5,  4,  7, 13,  5,  4, 21, 15,  7, 14,  4,  6,  8,\n",
       "          4, 20,  5, 15, 17,  8, 18,  5,  4, 11, 10, 12,  4, 21,  8, 12, 24,  5,\n",
       "         15,  2]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "11fce9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ m i s t e r ▁ q u i l t e r ▁ i s ▁ t h e ▁ a p o s t l e ▁ o f ▁ t h e ▁ m i d d l e ▁ c l a s s e s ▁ a n d ▁ w e ▁ a r e ▁ g l a d ▁ t o ▁ w e l c o m e ▁ h i s ▁ g o s p e l\n",
      "mister quilter is the apostle of the middle classes and we are glad to welcome his gospel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(hf_outputs.shape[0]):\n",
    "    print(tgt_dict.string(hf_outputs[i]))\n",
    "    print(tokenizer.decode(tgt_dict.string(hf_outputs[i])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "997fd48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<pad>', '</s>', '<unk>', '▁']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tgt_dict[x] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4d70bc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>mister quilter is the apostle of the middle classes and we are glad to welcome his gospel</s>']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.batch_decode(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a116f698",
   "metadata": {},
   "source": [
    "For comparison, Speech2Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00306c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "s2t_model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "s2t_processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f994828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthijs/anaconda3/envs/t5/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Reusing dataset librispeech_asr_dummy (/Users/matthijs/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 584, 80])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = s2t_processor(\n",
    "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n",
    ")\n",
    "input_features = inputs.input_features\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "dc8ef5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 584, 80])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test batch\n",
    "# input_features = torch.tile(input_features, dims=(2, 1, 1))\n",
    "# input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/transformers/src/transformers/models/speech_to_text/modeling_speech_to_text.py:561: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n",
      "/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/transformers/src/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = s2t_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = s2t_processor.batch_decode(generated_ids)[0]\n",
    "transcription\n",
    "\n",
    "#'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "31a21206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/s2t-small-librispeech-asr were not used when initializing Speech2TextModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Speech2TextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Speech2TextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Speech2TextModel were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** has_prefix_module True expects_prefix_module False\n",
      "*** remove_prefix_from_model False add_prefix_to_model True\n"
     ]
    }
   ],
   "source": [
    "from transformers import Speech2TextModel\n",
    "\n",
    "s2t_model = Speech2TextModel.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50884f2",
   "metadata": {},
   "source": [
    "Test other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cd38a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART, Speech2Text, Wav2Vec2 don't have pruning\n",
    "#hf_model.prune_heads({1: [0, 2], 2: [2,3 ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9778431c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(81, 768, padding_idx=1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "44be3e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=81, bias=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f954bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.resize_token_embeddings(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b17c7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e8369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0974c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0b945156",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_ctc.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "59c092a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_naked.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
