{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Put this notebook at the same level as the `SpeechT5` repo.\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- To run on CPU: In `speecht5/sequence_generator.py`, comment out where it does `.to(device=\"cuda\")`\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `speecht5_base_asr.pt` and `t5_transformer_lm.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "You also need an input audio file, any WAV at 16 kHz will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b45926",
   "metadata": {},
   "source": [
    "## Load audio and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18f317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/141231/1272-141231-0020.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b474ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/1272/128104/1272-128104-0000.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a855d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((93680,), 16000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "wav_data, cur_sample_rate = sf.read(input_file)\n",
    "wav_data.shape, cur_sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e60de",
   "metadata": {},
   "source": [
    "NOTE: The `Wav2Vec2FeatureExtractor` does not make sure the audio file is mono. If it has shape `(2, length)` or even `(1, length)` then the output from the feature extractor is incorrect!\n",
    "\n",
    "The `do_normalize` option is False for the SpeechT5 ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9699780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(do_normalize=False, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea3da78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(wav_data, sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a94286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing padding mask\n",
    "inputs[\"attention_mask\"][:, 40000:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e522e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 2))\n",
    "plt.plot(inputs[\"input_values\"][0])\n",
    "plt.plot(inputs[\"attention_mask\"][0])\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f594f3",
   "metadata": {},
   "source": [
    "Transform multiple inputs into a single padded batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb99431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128632,), 16000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file2 = \"/Users/matthijs/Documents/FILES/HuggingFace/S2S/textless/AUDIO_DIR/selfdestruct.wav\"\n",
    "wav_data2, cur_sample_rate2 = sf.read(input_file2)\n",
    "wav_data2.shape, cur_sample_rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1daee2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128632])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2 = feature_extractor([wav_data, wav_data2], sampling_rate=cur_sample_rate, padding=True, return_tensors=\"pt\")\n",
    "inputs2[\"input_values\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb653d87",
   "metadata": {},
   "source": [
    "The original model used a `padding_mask` as input, where False means no padding. The `Wav2Vec2FeatureExtractor` can return an `attention_mask`, where 1 means no padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3ab7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7163b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs2   # use the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint. I used `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task s2t \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForSpeechToText, \n",
    "    SpeechT5ForCTC, \n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SpeechT5Config()\n",
    "hf_model = SpeechT5Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SpeechT5ForSpeechToText.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForSpeechToText(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5EncoderWithSpeechPrenet(\n",
       "      (prenet): SpeechT5SpeechEncoderPrenet(\n",
       "        (feature_encoder): SpeechT5FeatureEncoder(\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): SpeechT5GroupNormConvLayer(\n",
       "              (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "              (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (1): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (2): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (3): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (4): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (5): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (6): SpeechT5NoLayerNormConvLayer(\n",
       "              (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (feature_projection): SpeechT5FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (pos_conv_embed): SpeechT5PositionalConvEmbedding(\n",
       "          (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (padding): SpeechT5SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (pos_sinusoidal_embed): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5DecoderWithTextPrenet(\n",
       "      (prenet): SpeechT5TextDecoderPrenet(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (embed_positions): SpeechT5SinusoidalPositionalEmbedding()\n",
       "      )\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder_postnet): SpeechT5TextDecoderPostnet(\n",
       "    (lm_head): Linear(in_features=768, out_features=81, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c1c4b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_asr were not used when initializing SpeechT5Model: ['speecht5.encoder.prenet.feature_projection.projection.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.bias', 'speecht5.decoder.prenet.embed_tokens.weight', 'speecht5.encoder.prenet.masked_spec_embed', 'speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias', 'speecht5.encoder.prenet.pos_sinusoidal_embed.weights', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_v', 'speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight', 'speecht5.encoder.prenet.feature_projection.layer_norm.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_g', 'speecht5.encoder.prenet.feature_projection.layer_norm.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight', 'text_decoder_postnet.lm_head.weight', 'speecht5.encoder.prenet.feature_projection.projection.bias', 'speecht5.decoder.prenet.embed_positions.weights']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading should work OK for class `SpeechT5Model` too:\n",
    "hf_model_naked = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7628cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the attention layer weights are correct\n",
    "# for i in range(len(hf_model.speecht5.encoder.layers)):\n",
    "#     print(i, \"k_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.weight == orig_model.encoder.layers[i].self_attn.k_proj.weight))\n",
    "#     print(i, \"k_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.k_proj.bias == orig_model.encoder.layers[i].self_attn.k_proj.bias))\n",
    "#     print(i, \"v_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.weight == orig_model.encoder.layers[i].self_attn.v_proj.weight))\n",
    "#     print(i, \"v_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.v_proj.bias == orig_model.encoder.layers[i].self_attn.v_proj.bias))\n",
    "#     print(i, \"q_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.weight == orig_model.encoder.layers[i].self_attn.q_proj.weight))\n",
    "#     print(i, \"q_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.q_proj.bias == orig_model.encoder.layers[i].self_attn.q_proj.bias))\n",
    "#     print(i, \"out_proj weight\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.weight == orig_model.encoder.layers[i].self_attn.out_proj.weight))\n",
    "#     print(i, \"out_proj bias\", torch.all(hf_model.speecht5.encoder.layers[i].attention.out_proj.bias == orig_model.encoder.layers[i].self_attn.out_proj.bias))\n",
    "#     print(\"---\")\n",
    "\n",
    "# print(\"pos_emb weight\", torch.all(hf_model.speecht5.encoder.pos_emb.pe_k.weight == orig_model.encoder.pos_emb.pe_k.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82233c52",
   "metadata": {},
   "source": [
    "Run a single forward pass. This should run the encoder, decoder, and the relevant pre- and postnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29bd2f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.prenet(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7e1f8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using no attention_mask\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speech_encoder_prenet(input_values=inputs[\"input_values\"])\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "359778a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "97a77055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7db34f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"extract_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "41afd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fcef1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_outputs[\"hidden_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d01d920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 292, 768]), torch.Size([1, 292])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in hf_outputs if hasattr(x, \"shape\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d5ff106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.4605,  5.1120,  5.0966,  ...,  0.9548,  2.4422,  0.0304],\n",
       "         [ 6.6734,  0.6380,  9.2025,  ...,  1.4536,  2.7775,  3.4076],\n",
       "         [ 2.4401, -1.7543, 11.1659,  ...,  1.7220,  2.5966,  2.3149],\n",
       "         ...,\n",
       "         [ 8.0375,  0.9100, 10.0326,  ..., -0.3356,  1.8808, -0.0438],\n",
       "         [ 3.8456, -2.4123, 12.9886,  ...,  0.7667,  2.0042, -0.6446],\n",
       "         [ 1.3660,  2.2960, 10.2969,  ...,  0.0711,  1.3946, -3.2424]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d24920f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_encoder_input = hf_outputs[0]\n",
    "hf_encoder_attention_mask = hf_outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {},
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {},
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24e0f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {},
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea4b4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"s2t\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {},
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a79fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9b2818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/SpeechT5/speecht5_base_asr.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bda7a201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.speecht5.T5TransformerModel"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64d53b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.modules.encoder.TransformerEncoder"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bad44136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c1faddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.data.encoders.sentencepiece_bpe.SentencepieceBPE"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.data import encoders\n",
    "from argparse import Namespace\n",
    "tokenizer = encoders.build_bpe(\n",
    "    Namespace(\n",
    "        bpe='sentencepiece', \n",
    "        sentencepiece_model='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/MODEL_DIR/spm_char.model'\n",
    "    )\n",
    ")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61dd5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_model.decoder.layers[0].encoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {},
   "source": [
    "## Verify speech encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {},
   "source": [
    "This first uses the `speech_encoder_prenet` to convert the raw audio data into embeddings of shape `(batch, sequence_length, 768)`. The sequence length is roughly `number of audio samples / 320`, so there is one vector every 20 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01638115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = inputs[\"input_values\"]\n",
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35b0a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = torch.BoolTensor(source.shape).fill_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94586252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = torch.BoolTensor((1 - inputs[\"attention_mask\"]).bool())\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "374cf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't work on the original model\n",
    "#padding_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31d1d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input, encoder_padding_mask = orig_model.speech_encoder_prenet(\n",
    "    source, padding_mask=padding_mask, mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36334be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input = orig_model.speech_encoder_prenet.feature_extractor(source)\n",
    "# encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26c4a65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 292, 768]), torch.Size([1, 292]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "058de538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.4605,  5.1120,  5.0966,  ...,  0.9548,  2.4422,  0.0304],\n",
       "         [ 6.6734,  0.6380,  9.2025,  ...,  1.4536,  2.7775,  3.4076],\n",
       "         [ 2.4401, -1.7543, 11.1659,  ...,  1.7220,  2.5966,  2.3149],\n",
       "         ...,\n",
       "         [ 8.0375,  0.9100, 10.0326,  ..., -0.3356,  1.8808, -0.0438],\n",
       "         [ 3.8456, -2.4123, 12.9886,  ...,  0.7667,  2.0042, -0.6446],\n",
       "         [ 1.3660,  2.2960, 10.2969,  ...,  0.0711,  1.3946, -3.2424]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c07884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {},
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b099ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]))\n",
    "torch.max(torch.abs(encoder_input - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5533cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbf15b85190>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(torch.abs(encoder_input - hf_outputs[\"hidden_states\"]).numpy()[0] > 1e-5)\n",
    "plt.imshow(torch.abs(encoder_input - hf_outputs[0]).numpy()[0] > 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf93df",
   "metadata": {},
   "source": [
    "The line that is different is where the padding mask goes from 1 to 0; the original model handles this a little different than we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b92b758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(torch.abs(encoder_input - hf_outputs[\"extract_features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d18286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.abs(encoder_input - hf_outputs[\"last_hidden_state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {},
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47c86f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with the original's speech prenet input:\n",
    "# with torch.no_grad():\n",
    "#     encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b8b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it with our input, which is slightly different (see above)\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(hf_encoder_input, ~hf_encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41ff76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([292, 1, 768])\n",
      "encoder_padding_mask shape torch.Size([1, 292])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([292, 1, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a38979d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3372, -0.0506, -0.3856,  ...,  0.1234, -0.5690,  0.0807],\n",
       "         [-0.2310, -0.0536, -0.4906,  ...,  0.0510, -0.4996,  0.0065],\n",
       "         [-0.2983, -0.0891, -0.5549,  ...,  0.0920, -0.4049, -0.1221],\n",
       "         ...,\n",
       "         [-0.4527, -0.3337, -0.0625,  ...,  0.0698, -0.2376, -0.0678],\n",
       "         [-0.3612, -0.3405,  0.0427,  ...,  0.1003, -0.2545, -0.0049],\n",
       "         [-0.3058, -0.3190,  0.0499,  ...,  0.0834, -0.2110,  0.0618]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9b9dda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -9.7832,  -9.9557,  -9.8267,  ...,  -9.7276,  -9.6644,  13.7010],\n",
       "         [-10.2775, -10.3579, -10.2925,  ..., -10.2027, -10.1951,  14.8642],\n",
       "         [-11.0394, -11.0347, -11.0767,  ..., -11.0623, -10.9542,  16.0627],\n",
       "         ...,\n",
       "         [-11.1355, -11.6218, -11.1786,  ..., -11.2304, -10.7997,   8.2224],\n",
       "         [-10.5122, -10.8811, -10.5795,  ..., -10.6125, -10.1673,   8.1692],\n",
       "         [ -9.7803, -10.1018,  -9.8347,  ...,  -9.8411,  -9.4568,   7.9284]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cde6dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exact same inputs as the original model:\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speecht5(\n",
    "#          inputs_embeds=encoder_input,\n",
    "#          attention_mask=(~encoder_padding_mask),\n",
    "#      )\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28f529a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.wrapped_encoder(\n",
    "         hidden_states=hf_encoder_input,\n",
    "         attention_mask=hf_encoder_attention_mask,\n",
    "#          input_values=inputs.input_values,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb12e71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.3372, -0.0506, -0.3856,  ...,  0.1234, -0.5690,  0.0807],\n",
       "         [-0.2310, -0.0536, -0.4906,  ...,  0.0510, -0.4996,  0.0065],\n",
       "         [-0.2983, -0.0891, -0.5549,  ...,  0.0920, -0.4049, -0.1221],\n",
       "         ...,\n",
       "         [-0.4527, -0.3337, -0.0625,  ...,  0.0698, -0.2376, -0.0678],\n",
       "         [-0.3612, -0.3405,  0.0427,  ...,  0.1003, -0.2545, -0.0049],\n",
       "         [-0.3058, -0.3190,  0.0499,  ...,  0.0834, -0.2110,  0.0618]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06fb675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8fe9914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 292, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69eda6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3372, -0.0506, -0.3856,  ...,  0.1234, -0.5690,  0.0807],\n",
       "         [-0.2310, -0.0536, -0.4906,  ...,  0.0510, -0.4996,  0.0065],\n",
       "         [-0.2983, -0.0891, -0.5549,  ...,  0.0920, -0.4049, -0.1221],\n",
       "         ...,\n",
       "         [-0.4527, -0.3337, -0.0625,  ...,  0.0698, -0.2376, -0.0678],\n",
       "         [-0.3612, -0.3405,  0.0427,  ...,  0.1003, -0.2545, -0.0049],\n",
       "         [-0.3058, -0.3190,  0.0499,  ...,  0.0834, -0.2110,  0.0618]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a747e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518e152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca490a88",
   "metadata": {},
   "source": [
    "## Verify CTC model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98055be9",
   "metadata": {},
   "source": [
    "This model only needs the encoder portion.\n",
    "\n",
    "This uses the same checkpoint as before: `speecht5_base_asr.pt` from https://huggingface.co/ajyy/SpeechT5\n",
    "\n",
    "Run the following to convert, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task ctc \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_base_asr.pt \n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69ca25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model_ctc = SpeechT5ForCTC(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a32978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_ctc = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc\"\n",
    "hf_model_ctc = SpeechT5ForCTC.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b28453ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc were not used when initializing SpeechT5Model: ['speecht5.encoder.prenet.feature_projection.projection.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.6.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.bias', 'speecht5.encoder.prenet.masked_spec_embed', 'speecht5.encoder.prenet.feature_encoder.conv_layers.2.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.bias', 'speecht5.encoder.prenet.pos_sinusoidal_embed.weights', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.conv.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_v', 'speecht5.encoder.prenet.feature_encoder.conv_layers.5.conv.weight', 'lm_head.weight', 'speecht5.encoder.prenet.feature_projection.layer_norm.weight', 'speecht5.encoder.prenet.pos_conv_embed.conv.weight_g', 'speecht5.encoder.prenet.feature_projection.layer_norm.bias', 'speecht5.encoder.prenet.feature_encoder.conv_layers.0.layer_norm.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.3.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.1.conv.weight', 'speecht5.encoder.prenet.feature_encoder.conv_layers.4.conv.weight', 'lm_head.bias', 'speecht5.encoder.prenet.feature_projection.projection.bias']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SpeechT5Model were not initialized from the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_base_ctc and are newly initialized: ['speecht5.decoder.wrapped_decoder.layers.1.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.1.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.intermediate_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.v_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.feed_forward.intermediate_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.2.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.5.self_attn.k_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.2.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.2.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.final_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.q_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.4.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.feed_forward.output_dense.bias', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.2.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.out_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.0.encoder_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.4.self_attn.k_proj.weight', 'speecht5.decoder.wrapped_decoder.layers.0.feed_forward.output_dense.weight', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.q_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.5.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.self_attn_layer_norm.bias', 'speecht5.decoder.wrapped_decoder.layers.3.final_layer_norm.weight', 'speecht5.decoder.wrapped_decoder.layers.3.encoder_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.v_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.self_attn.out_proj.bias', 'speecht5.decoder.wrapped_decoder.layers.1.encoder_attn.v_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# This should also work:\n",
    "hf_model_ctc_naked = SpeechT5Model.from_pretrained(model_checkpoint_ctc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d81d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutput"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the full model:\n",
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(**inputs)\n",
    "\n",
    "# Run without attention_mask:\n",
    "# with torch.no_grad():\n",
    "#     hf_outputs = hf_model_ctc(input_values=inputs[\"input_values\"])\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de48ff1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2)) - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32fabb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(hf_outputs[0], dim=-1, dtype=torch.float32)\n",
    "probs = probs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "267f80ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[80, 80, 80, 80, 80, 80, 80,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4, 18, 18, 10, 10, 80, 12, 12, 80,  6,\n",
       "         80,  5, 13, 13,  4,  4, 45, 45, 16, 16, 10, 10, 15, 15, 15, 80, 80,  6,\n",
       "          6, 80,  5, 13, 13, 80, 80,  4,  4, 80, 10, 10, 80, 12, 12,  4,  4,  6,\n",
       "         11, 11,  5,  5,  4,  4, 80,  7, 24, 80, 80, 24, 80, 80, 80, 80,  8, 12,\n",
       "         12, 80, 80, 80, 12, 80, 80, 80,  5, 15, 15, 15, 80, 80, 80,  4,  4, 80,\n",
       "         80,  8, 19, 19,  4,  4,  6, 11, 11,  5,  4,  4, 18, 80, 10, 14, 80, 14,\n",
       "         15, 15, 15, 15, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
       "         80, 80, 80, 80]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "90ca3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tgt_dict.string(probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5bcf370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    mmiissterr  qquuiillltterr  iiss  thhee  appossselll  off  thhe  middllll\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(probs.shape[0]):\n",
    "    print(tokenizer.decode(tgt_dict.string(probs[i])).replace(\"<ctc_blank>\", \"\"))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc5fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60229b2c",
   "metadata": {},
   "source": [
    "Calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6f22609",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model_ctc(\n",
    "         **inputs, \n",
    "         labels=torch.tensor(\n",
    "           [[ 46, 16, 12,  6,  4,  6, 11, 13, 16, 12,  6,  4,  7,  9, 14,  4,\n",
    "         24,  7, 13, 13, 22,  4,  7,  9, 14,  4, 27, 10, 17,  6,  8, 13, 22,  4,\n",
    "          6,  8,  4,  6, 11,  5,  4, 12,  6, 13,  8,  9, 21,  5, 13, ]]\n",
    "         ),\n",
    "         output_hidden_states=True,\n",
    "         return_dict=True,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f57c285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(942.3365)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9dd3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {},
   "source": [
    "## Verify text decoder prenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ba48d",
   "metadata": {},
   "source": [
    "First this calls `text_decoder_prenet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3747a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inputs.input_values.size(0)\n",
    "beam_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28dcb259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.tensor([2, 4, 46, 16, 12, 16] * beam_size * batch_size).reshape(beam_size * batch_size, -1)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6bd8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one has padding (token_id = 1)\n",
    "# The results will be different with the HF implementation because\n",
    "# we don't set the attention_mask to 0 for padding tokens\n",
    "# tokens = torch.tensor([2, 4, 46, 16, 1, 12] * beam_size * batch_size).reshape(beam_size * batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f09f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60cc6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a sequence length of 1\n",
    "# with torch.no_grad():\n",
    "#     prev_output_tokens, tgt_mask, incremental_state = orig_model.text_decoder_prenet(tokens, incremental_state={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "725cfc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 768])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: misleading name; these are not the actual tokens but their embeddings!\n",
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "84764c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "81026d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77134734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.prenet(\n",
    "         input_ids=tokens,\n",
    "#          attention_mask=hf_encoder_attention_mask,\n",
    "#          attention_mask=inputs.attention_mask,\n",
    "#          past_key_values=[(torch.ones(5, 1, 1), torch.ones(5, 1, 1))],\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8f007852",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds_hf, decoder_attention_mask = hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea4dd4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 768])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeds_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "21f6193b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(prev_output_tokens - token_embeds_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a827caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d7a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {},
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42a4fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = source.size(0)\n",
    "new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "encoder_outs = orig_model.encoder.reorder_encoder_out(encoder_output, new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a98652fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]],\n",
       "\n",
       "        [[ 1.1074,  1.1042,  1.0387,  ...,  0.9619,  1.0463,  0.8678],\n",
       "         [ 0.0890,  0.1702,  0.3139,  ...,  0.8629,  0.7833,  0.9348],\n",
       "         [-0.7402, -0.4274, -0.5774,  ...,  1.3103,  0.5403,  0.7292],\n",
       "         [-0.8579, -0.9214, -0.9064,  ...,  1.0331,  0.8949,  0.9723],\n",
       "         [-0.2291, -0.4457, -0.5671,  ...,  0.9991,  0.8768,  0.9128],\n",
       "         [ 0.7580,  0.5875,  0.4706,  ...,  1.0331,  0.8949,  0.9723]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6229b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_outs,\n",
    "        incremental_state=incremental_state,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a190958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 768])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fea0351e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4183, -0.3443, -0.0507,  ..., -0.2428, -0.3456,  0.0726],\n",
       "         [-0.3772, -0.6195, -1.4690,  ..., -0.2137,  0.0358,  0.1250],\n",
       "         [ 0.1990, -0.8667, -0.9731,  ...,  0.1872,  0.2286,  0.4505],\n",
       "         [ 0.7156, -1.1673, -1.4388,  ...,  0.3681,  0.5455, -0.4201],\n",
       "         [ 0.6464, -1.1596, -0.5881,  ..., -0.5057,  0.6083, -0.2097],\n",
       "         [ 0.6498, -0.5418, -0.7879,  ...,  0.5718,  0.6503, -0.1804]],\n",
       "\n",
       "        [[-0.4183, -0.3443, -0.0507,  ..., -0.2428, -0.3456,  0.0726],\n",
       "         [-0.3772, -0.6195, -1.4690,  ..., -0.2137,  0.0358,  0.1250],\n",
       "         [ 0.1990, -0.8667, -0.9731,  ...,  0.1872,  0.2286,  0.4505],\n",
       "         [ 0.7156, -1.1673, -1.4388,  ...,  0.3681,  0.5455, -0.4201],\n",
       "         [ 0.6464, -1.1596, -0.5881,  ..., -0.5057,  0.6083, -0.2097],\n",
       "         [ 0.6498, -0.5418, -0.7879,  ...,  0.5718,  0.6503, -0.1804]],\n",
       "\n",
       "        [[-0.4183, -0.3443, -0.0507,  ..., -0.2428, -0.3456,  0.0726],\n",
       "         [-0.3772, -0.6195, -1.4690,  ..., -0.2137,  0.0358,  0.1250],\n",
       "         [ 0.1990, -0.8667, -0.9731,  ...,  0.1872,  0.2286,  0.4505],\n",
       "         [ 0.7156, -1.1673, -1.4388,  ...,  0.3681,  0.5455, -0.4201],\n",
       "         [ 0.6464, -1.1596, -0.5881,  ..., -0.5057,  0.6083, -0.2097],\n",
       "         [ 0.6498, -0.5418, -0.7879,  ...,  0.5718,  0.6503, -0.1804]],\n",
       "\n",
       "        [[-0.4183, -0.3443, -0.0507,  ..., -0.2428, -0.3456,  0.0726],\n",
       "         [-0.3772, -0.6195, -1.4690,  ..., -0.2137,  0.0358,  0.1250],\n",
       "         [ 0.1990, -0.8667, -0.9731,  ...,  0.1872,  0.2286,  0.4505],\n",
       "         [ 0.7156, -1.1673, -1.4388,  ...,  0.3681,  0.5455, -0.4201],\n",
       "         [ 0.6464, -1.1596, -0.5881,  ..., -0.5057,  0.6083, -0.2097],\n",
       "         [ 0.6498, -0.5418, -0.7879,  ...,  0.5718,  0.6503, -0.1804]],\n",
       "\n",
       "        [[-0.4183, -0.3443, -0.0507,  ..., -0.2428, -0.3456,  0.0726],\n",
       "         [-0.3772, -0.6195, -1.4690,  ..., -0.2137,  0.0358,  0.1250],\n",
       "         [ 0.1990, -0.8667, -0.9731,  ...,  0.1872,  0.2286,  0.4505],\n",
       "         [ 0.7156, -1.1673, -1.4388,  ...,  0.3681,  0.5455, -0.4201],\n",
       "         [ 0.6464, -1.1596, -0.5881,  ..., -0.5057,  0.6083, -0.2097],\n",
       "         [ 0.6498, -0.5418, -0.7879,  ...,  0.5718,  0.6503, -0.1804]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05117956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b593692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d486f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.wrapped_decoder(\n",
    "         hidden_states=prev_output_tokens,\n",
    "         attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_outs[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea512cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'past_key_values']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d82c369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 768])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb6282bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4183, -0.3443, -0.0507,  ..., -0.2428, -0.3456,  0.0726],\n",
       "         [-0.3772, -0.6195, -1.4690,  ..., -0.2137,  0.0358,  0.1250],\n",
       "         [ 0.1990, -0.8667, -0.9731,  ...,  0.1872,  0.2286,  0.4505],\n",
       "         [ 0.7156, -1.1673, -1.4388,  ...,  0.3681,  0.5455, -0.4201],\n",
       "         [ 0.6464, -1.1596, -0.5881,  ..., -0.5057,  0.6083, -0.2097],\n",
       "         [ 0.6498, -0.5418, -0.7879,  ...,  0.5718,  0.6503, -0.1804]],\n",
       "\n",
       "        [[-0.4183, -0.3443, -0.0507,  ..., -0.2428, -0.3456,  0.0726],\n",
       "         [-0.3772, -0.6195, -1.4690,  ..., -0.2137,  0.0358,  0.1250],\n",
       "         [ 0.1990, -0.8667, -0.9731,  ...,  0.1872,  0.2286,  0.4505],\n",
       "         [ 0.7156, -1.1673, -1.4388,  ...,  0.3681,  0.5455, -0.4201],\n",
       "         [ 0.6464, -1.1596, -0.5881,  ..., -0.5057,  0.6083, -0.2097],\n",
       "         [ 0.6498, -0.5418, -0.7879,  ...,  0.5718,  0.6503, -0.1804]],\n",
       "\n",
       "        [[-0.4183, -0.3443, -0.0507,  ..., -0.2428, -0.3456,  0.0726],\n",
       "         [-0.3772, -0.6195, -1.4690,  ..., -0.2137,  0.0358,  0.1250],\n",
       "         [ 0.1990, -0.8667, -0.9731,  ...,  0.1872,  0.2286,  0.4505],\n",
       "         [ 0.7156, -1.1673, -1.4388,  ...,  0.3681,  0.5455, -0.4201],\n",
       "         [ 0.6464, -1.1596, -0.5881,  ..., -0.5057,  0.6083, -0.2097],\n",
       "         [ 0.6498, -0.5418, -0.7879,  ...,  0.5718,  0.6503, -0.1804]],\n",
       "\n",
       "        [[-0.4183, -0.3443, -0.0507,  ..., -0.2428, -0.3456,  0.0726],\n",
       "         [-0.3772, -0.6195, -1.4690,  ..., -0.2137,  0.0358,  0.1250],\n",
       "         [ 0.1990, -0.8667, -0.9731,  ...,  0.1872,  0.2286,  0.4505],\n",
       "         [ 0.7156, -1.1673, -1.4388,  ...,  0.3681,  0.5455, -0.4201],\n",
       "         [ 0.6464, -1.1596, -0.5881,  ..., -0.5057,  0.6083, -0.2097],\n",
       "         [ 0.6498, -0.5418, -0.7879,  ...,  0.5718,  0.6503, -0.1804]],\n",
       "\n",
       "        [[-0.4183, -0.3443, -0.0507,  ..., -0.2428, -0.3456,  0.0726],\n",
       "         [-0.3772, -0.6195, -1.4690,  ..., -0.2137,  0.0358,  0.1250],\n",
       "         [ 0.1990, -0.8667, -0.9731,  ...,  0.1872,  0.2286,  0.4505],\n",
       "         [ 0.7156, -1.1673, -1.4388,  ...,  0.3681,  0.5455, -0.4201],\n",
       "         [ 0.6464, -1.1596, -0.5881,  ..., -0.5057,  0.6083, -0.2097],\n",
       "         [ 0.6498, -0.5418, -0.7879,  ...,  0.5718,  0.6503, -0.1804]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f3e254cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5497e-06)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_output - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify text decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs = orig_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 81])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7d6de14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2955e+01, -2.2448e+01,  1.1513e+00, -2.2412e+01,  1.6948e+01,\n",
       "         -6.9703e-02,  6.5178e-01, -5.6547e-01, -9.3333e-02, -1.4754e+00,\n",
       "         -6.7230e-01,  5.1749e-01,  1.9094e-01, -9.8455e-01, -1.9626e+00,\n",
       "         -1.6577e+00, -1.0214e+00, -1.7566e+00, -1.2433e+00, -1.6198e-01,\n",
       "          1.2751e+00, -8.4831e-02, -3.7371e+00, -2.2529e+01, -6.8498e-01,\n",
       "          7.4485e-01, -2.2470e+01, -2.5235e+00, -3.7378e+00, -2.2428e+01,\n",
       "         -2.2473e+01, -1.8304e-02, -2.2567e+01, -2.2447e+01, -2.2436e+01,\n",
       "         -2.2498e+01, -2.2483e+01, -5.0941e+00, -2.2432e+01, -2.2552e+01,\n",
       "         -2.2433e+01, -2.2502e+01, -2.2476e+01, -2.2492e+01, -2.2484e+01,\n",
       "         -2.9621e+00, -2.8972e+00, -2.2413e+01, -2.2432e+01, -2.2574e+01,\n",
       "         -2.2451e+01, -2.2516e+01, -2.2420e+01, -2.2421e+01, -2.2465e+01,\n",
       "         -2.2384e+01, -2.2472e+01, -6.1619e+00, -2.2465e+01, -2.2441e+01,\n",
       "         -2.2483e+01, -2.2563e+01, -2.2459e+01, -2.2440e+01, -2.2455e+01,\n",
       "         -2.2494e+01, -2.2471e+01, -2.2517e+01, -2.2474e+01, -2.2451e+01,\n",
       "         -2.2384e+01, -2.2467e+01, -2.2437e+01, -2.2448e+01, -2.2404e+01,\n",
       "         -2.2474e+01, -2.2419e+01, -2.2447e+01, -2.2439e+01, -2.1064e+01,\n",
       "         -2.2428e+01],\n",
       "        [-1.8698e+01, -1.6557e+01, -8.2359e+00, -1.6397e+01, -1.8040e+00,\n",
       "         -2.8668e-01,  1.6484e+00,  1.3476e+00,  7.4253e-01,  2.7561e+00,\n",
       "          1.6906e+00,  1.7363e+00,  2.7432e+00,  1.5025e+00,  1.2908e+00,\n",
       "         -1.1153e-01, -2.9610e+00, -1.2520e+00,  1.6066e+01,  1.2125e+00,\n",
       "          2.5974e+00, -2.9476e-01, -5.5584e-01, -1.6447e+01, -7.1116e-01,\n",
       "          1.0200e+00, -1.6400e+01, -1.6206e-01, -1.3376e+00, -1.6336e+01,\n",
       "         -1.6464e+01, -6.8524e+00, -1.6388e+01, -1.6331e+01, -1.6431e+01,\n",
       "         -1.6420e+01, -1.6409e+01, -7.9096e+00, -1.6309e+01, -1.6418e+01,\n",
       "         -1.6474e+01, -1.6374e+01, -1.6476e+01, -1.6430e+01, -1.6352e+01,\n",
       "         -3.7211e+00,  1.4060e-01, -1.6335e+01, -1.6468e+01, -1.6318e+01,\n",
       "         -1.6369e+01, -1.6408e+01, -1.6391e+01, -1.6285e+01, -1.6380e+01,\n",
       "         -1.6263e+01, -1.6397e+01, -3.7186e+00, -1.6434e+01, -1.6429e+01,\n",
       "         -1.6453e+01, -1.6476e+01, -1.6451e+01, -1.6307e+01, -1.6284e+01,\n",
       "         -1.6460e+01, -1.6386e+01, -1.6379e+01, -1.6364e+01, -1.6467e+01,\n",
       "         -1.6320e+01, -1.6307e+01, -1.6390e+01, -1.6243e+01, -1.6427e+01,\n",
       "         -1.6357e+01, -1.6444e+01, -1.6478e+01, -1.6394e+01, -1.4985e+01,\n",
       "         -1.6366e+01],\n",
       "        [-2.3494e+01, -1.8340e+01, -2.7424e+00, -1.8772e+01,  4.6078e+00,\n",
       "          7.2360e+00, -1.4261e+00,  7.4019e+00,  6.2607e+00,  1.7274e-01,\n",
       "          1.0242e+01, -4.3955e+00, -1.6000e+00, -1.0095e+00, -5.4686e+00,\n",
       "         -5.0234e+00,  1.1790e+01, -4.0826e+00,  7.6779e-01, -4.9068e+00,\n",
       "         -6.6869e+00, -6.7202e+00, -1.0456e-01, -1.8787e+01, -3.1044e+00,\n",
       "         -7.8688e+00, -1.8793e+01, -2.0736e+00, -7.3106e+00, -1.8734e+01,\n",
       "         -1.8837e+01, -6.4803e-02, -1.8582e+01, -1.8648e+01, -1.8681e+01,\n",
       "         -1.8733e+01, -1.8780e+01, -8.8740e+00, -1.8780e+01, -1.8688e+01,\n",
       "         -1.8828e+01, -1.8686e+01, -1.8786e+01, -1.8835e+01, -1.8747e+01,\n",
       "         -7.1741e+00, -5.9040e+00, -1.8726e+01, -1.8672e+01, -1.8695e+01,\n",
       "         -1.8693e+01, -1.8734e+01, -1.8779e+01, -1.8559e+01, -1.8794e+01,\n",
       "         -1.8603e+01, -1.8726e+01, -8.0954e+00, -1.8838e+01, -1.8807e+01,\n",
       "         -1.8729e+01, -1.8790e+01, -1.8845e+01, -1.8722e+01, -1.8752e+01,\n",
       "         -1.8754e+01, -1.8828e+01, -1.8692e+01, -1.8644e+01, -1.8842e+01,\n",
       "         -1.8774e+01, -1.8749e+01, -1.8798e+01, -1.8690e+01, -1.8664e+01,\n",
       "         -1.8605e+01, -1.8766e+01, -1.8805e+01, -1.8754e+01, -1.7998e+01,\n",
       "         -1.8666e+01],\n",
       "        [-2.1837e+01, -1.6587e+01, -4.5763e+00, -1.6860e+01,  1.2189e+00,\n",
       "         -3.8247e+00,  1.6421e+00,  5.1812e-01, -5.3800e+00,  7.0832e+00,\n",
       "          2.0015e+00, -1.1491e+00,  1.1711e+01,  3.4049e+00,  7.0236e+00,\n",
       "          8.0327e+00, -4.2412e+00, -2.6836e+00,  5.4403e+00, -6.0436e-01,\n",
       "         -5.9352e+00, -1.0976e-01, -3.7157e+00, -1.6862e+01,  6.0787e+00,\n",
       "          3.8306e+00, -1.6730e+01,  1.5762e+00, -3.5318e+00, -1.6916e+01,\n",
       "         -1.6834e+01, -5.8285e+00, -1.6953e+01, -1.6873e+01, -1.6856e+01,\n",
       "         -1.6951e+01, -1.7029e+01, -3.4943e+00, -1.6835e+01, -1.6858e+01,\n",
       "         -1.6891e+01, -1.6945e+01, -1.6927e+01, -1.6917e+01, -1.6983e+01,\n",
       "         -4.0244e+00, -2.5741e+00, -1.7066e+01, -1.6893e+01, -1.6819e+01,\n",
       "         -1.6907e+01, -1.6871e+01, -1.6809e+01, -1.6817e+01, -1.7037e+01,\n",
       "         -1.6874e+01, -1.6985e+01, -4.3415e+00, -1.6960e+01, -1.6917e+01,\n",
       "         -1.6991e+01, -1.6981e+01, -1.6890e+01, -1.6853e+01, -1.6898e+01,\n",
       "         -1.6964e+01, -1.6879e+01, -1.7040e+01, -1.6765e+01, -1.6893e+01,\n",
       "         -1.6979e+01, -1.6919e+01, -1.7112e+01, -1.7003e+01, -1.6896e+01,\n",
       "         -1.6870e+01, -1.6851e+01, -1.6970e+01, -1.6964e+01, -1.5540e+01,\n",
       "         -1.6757e+01],\n",
       "        [-8.7890e+00, -1.3154e+01, -6.4551e-01, -1.3098e+01,  3.3229e+00,\n",
       "          6.4389e-02,  2.0046e+01, -2.0828e+00, -7.0188e-01, -6.5345e-01,\n",
       "          1.0203e+00,  6.0030e-01,  3.7610e+00, -3.6021e+00,  2.0665e+00,\n",
       "         -4.3932e+00,  2.2074e+00,  4.3260e+00, -3.1799e+00, -2.8983e-01,\n",
       "         -4.5543e+00, -4.4368e+00, -5.4166e+00, -1.3122e+01,  7.6190e+00,\n",
       "         -3.2996e+00, -1.3014e+01, -4.9847e+00,  2.7634e+00, -1.3094e+01,\n",
       "         -1.3113e+01, -2.8202e+00, -1.3059e+01, -1.3183e+01, -1.3143e+01,\n",
       "         -1.3137e+01, -1.3085e+01, -7.5945e+00, -1.3136e+01, -1.3037e+01,\n",
       "         -1.3049e+01, -1.3081e+01, -1.3141e+01, -1.3142e+01, -1.3121e+01,\n",
       "          2.7582e+00, -5.3239e+00, -1.3081e+01, -1.3268e+01, -1.3118e+01,\n",
       "         -1.3187e+01, -1.3098e+01, -1.3098e+01, -1.2919e+01, -1.3148e+01,\n",
       "         -1.3166e+01, -1.3224e+01, -4.2674e+00, -1.3216e+01, -1.3249e+01,\n",
       "         -1.3106e+01, -1.2976e+01, -1.3105e+01, -1.2944e+01, -1.3087e+01,\n",
       "         -1.3141e+01, -1.3047e+01, -1.3253e+01, -1.3028e+01, -1.3125e+01,\n",
       "         -1.3101e+01, -1.3319e+01, -1.3203e+01, -1.3196e+01, -1.3160e+01,\n",
       "         -1.3194e+01, -1.3097e+01, -1.3179e+01, -1.3115e+01, -1.2651e+01,\n",
       "         -1.3028e+01],\n",
       "        [-2.4220e+01, -1.6895e+01,  1.7025e+00, -1.6805e+01,  7.1925e+00,\n",
       "          3.0706e+00, -7.6066e-01,  4.4025e+00, -5.4450e+00, -2.2222e+00,\n",
       "          1.2081e+00,  5.6264e-01,  5.0047e+00,  4.3109e+00, -3.2481e+00,\n",
       "          1.1275e+00, -5.9119e+00,  2.1554e+00,  1.5825e+00,  1.1244e+00,\n",
       "         -3.6356e+00,  4.2976e-01, -7.2730e+00, -1.6669e+01,  6.5047e+00,\n",
       "         -1.7697e+00, -1.6569e+01, -1.4540e+00,  3.8655e+00, -1.6772e+01,\n",
       "         -1.6772e+01, -2.4985e+00, -1.6764e+01, -1.6756e+01, -1.6639e+01,\n",
       "         -1.6852e+01, -1.6680e+01, -4.3356e+00, -1.6552e+01, -1.6778e+01,\n",
       "         -1.6687e+01, -1.6798e+01, -1.6735e+01, -1.6685e+01, -1.6785e+01,\n",
       "          1.2320e+00, -6.1120e+00, -1.6819e+01, -1.6754e+01, -1.6646e+01,\n",
       "         -1.6919e+01, -1.6657e+01, -1.6762e+01, -1.6611e+01, -1.6910e+01,\n",
       "         -1.6773e+01, -1.6912e+01, -9.3478e+00, -1.6700e+01, -1.6749e+01,\n",
       "         -1.6804e+01, -1.6835e+01, -1.6591e+01, -1.6791e+01, -1.6787e+01,\n",
       "         -1.6852e+01, -1.6802e+01, -1.6926e+01, -1.6686e+01, -1.6762e+01,\n",
       "         -1.6802e+01, -1.6664e+01, -1.6903e+01, -1.6709e+01, -1.6742e+01,\n",
       "         -1.6704e+01, -1.6784e+01, -1.6767e+01, -1.6876e+01, -1.4905e+01,\n",
       "         -1.6679e+01]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ba7571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lprobs_hf = hf_model.text_decoder_postnet(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5623a791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 81])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64f0c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2955e+01, -2.2448e+01,  1.1513e+00, -2.2412e+01,  1.6948e+01,\n",
       "         -6.9703e-02,  6.5178e-01, -5.6547e-01, -9.3333e-02, -1.4754e+00,\n",
       "         -6.7230e-01,  5.1749e-01,  1.9094e-01, -9.8455e-01, -1.9626e+00,\n",
       "         -1.6577e+00, -1.0214e+00, -1.7566e+00, -1.2433e+00, -1.6198e-01,\n",
       "          1.2751e+00, -8.4831e-02, -3.7371e+00, -2.2529e+01, -6.8498e-01,\n",
       "          7.4485e-01, -2.2470e+01, -2.5235e+00, -3.7378e+00, -2.2428e+01,\n",
       "         -2.2473e+01, -1.8304e-02, -2.2567e+01, -2.2447e+01, -2.2436e+01,\n",
       "         -2.2498e+01, -2.2483e+01, -5.0941e+00, -2.2432e+01, -2.2552e+01,\n",
       "         -2.2433e+01, -2.2502e+01, -2.2476e+01, -2.2492e+01, -2.2484e+01,\n",
       "         -2.9621e+00, -2.8972e+00, -2.2413e+01, -2.2432e+01, -2.2574e+01,\n",
       "         -2.2451e+01, -2.2516e+01, -2.2420e+01, -2.2421e+01, -2.2465e+01,\n",
       "         -2.2384e+01, -2.2472e+01, -6.1619e+00, -2.2465e+01, -2.2441e+01,\n",
       "         -2.2483e+01, -2.2563e+01, -2.2459e+01, -2.2440e+01, -2.2455e+01,\n",
       "         -2.2494e+01, -2.2471e+01, -2.2517e+01, -2.2474e+01, -2.2451e+01,\n",
       "         -2.2384e+01, -2.2467e+01, -2.2437e+01, -2.2448e+01, -2.2404e+01,\n",
       "         -2.2474e+01, -2.2419e+01, -2.2447e+01, -2.2439e+01, -2.1064e+01,\n",
       "         -2.2428e+01],\n",
       "        [-1.8698e+01, -1.6557e+01, -8.2359e+00, -1.6397e+01, -1.8040e+00,\n",
       "         -2.8668e-01,  1.6484e+00,  1.3476e+00,  7.4253e-01,  2.7561e+00,\n",
       "          1.6906e+00,  1.7363e+00,  2.7432e+00,  1.5025e+00,  1.2908e+00,\n",
       "         -1.1153e-01, -2.9610e+00, -1.2520e+00,  1.6066e+01,  1.2125e+00,\n",
       "          2.5974e+00, -2.9476e-01, -5.5584e-01, -1.6447e+01, -7.1116e-01,\n",
       "          1.0200e+00, -1.6400e+01, -1.6206e-01, -1.3376e+00, -1.6336e+01,\n",
       "         -1.6464e+01, -6.8524e+00, -1.6388e+01, -1.6331e+01, -1.6431e+01,\n",
       "         -1.6420e+01, -1.6409e+01, -7.9096e+00, -1.6309e+01, -1.6418e+01,\n",
       "         -1.6474e+01, -1.6374e+01, -1.6476e+01, -1.6430e+01, -1.6352e+01,\n",
       "         -3.7211e+00,  1.4060e-01, -1.6335e+01, -1.6468e+01, -1.6318e+01,\n",
       "         -1.6369e+01, -1.6408e+01, -1.6391e+01, -1.6285e+01, -1.6380e+01,\n",
       "         -1.6263e+01, -1.6397e+01, -3.7186e+00, -1.6434e+01, -1.6429e+01,\n",
       "         -1.6453e+01, -1.6476e+01, -1.6451e+01, -1.6307e+01, -1.6284e+01,\n",
       "         -1.6460e+01, -1.6386e+01, -1.6379e+01, -1.6364e+01, -1.6467e+01,\n",
       "         -1.6320e+01, -1.6307e+01, -1.6390e+01, -1.6243e+01, -1.6427e+01,\n",
       "         -1.6357e+01, -1.6444e+01, -1.6478e+01, -1.6394e+01, -1.4985e+01,\n",
       "         -1.6366e+01],\n",
       "        [-2.3494e+01, -1.8340e+01, -2.7424e+00, -1.8772e+01,  4.6078e+00,\n",
       "          7.2360e+00, -1.4261e+00,  7.4019e+00,  6.2607e+00,  1.7274e-01,\n",
       "          1.0242e+01, -4.3955e+00, -1.6000e+00, -1.0095e+00, -5.4686e+00,\n",
       "         -5.0234e+00,  1.1790e+01, -4.0826e+00,  7.6779e-01, -4.9068e+00,\n",
       "         -6.6869e+00, -6.7202e+00, -1.0456e-01, -1.8787e+01, -3.1044e+00,\n",
       "         -7.8688e+00, -1.8793e+01, -2.0736e+00, -7.3106e+00, -1.8734e+01,\n",
       "         -1.8837e+01, -6.4803e-02, -1.8582e+01, -1.8648e+01, -1.8681e+01,\n",
       "         -1.8733e+01, -1.8780e+01, -8.8740e+00, -1.8780e+01, -1.8688e+01,\n",
       "         -1.8828e+01, -1.8686e+01, -1.8786e+01, -1.8835e+01, -1.8747e+01,\n",
       "         -7.1741e+00, -5.9040e+00, -1.8726e+01, -1.8672e+01, -1.8695e+01,\n",
       "         -1.8693e+01, -1.8734e+01, -1.8779e+01, -1.8559e+01, -1.8794e+01,\n",
       "         -1.8603e+01, -1.8726e+01, -8.0954e+00, -1.8838e+01, -1.8807e+01,\n",
       "         -1.8729e+01, -1.8790e+01, -1.8845e+01, -1.8722e+01, -1.8752e+01,\n",
       "         -1.8754e+01, -1.8828e+01, -1.8692e+01, -1.8644e+01, -1.8842e+01,\n",
       "         -1.8774e+01, -1.8749e+01, -1.8798e+01, -1.8690e+01, -1.8664e+01,\n",
       "         -1.8605e+01, -1.8766e+01, -1.8805e+01, -1.8754e+01, -1.7998e+01,\n",
       "         -1.8666e+01],\n",
       "        [-2.1837e+01, -1.6587e+01, -4.5763e+00, -1.6860e+01,  1.2189e+00,\n",
       "         -3.8247e+00,  1.6421e+00,  5.1812e-01, -5.3800e+00,  7.0832e+00,\n",
       "          2.0015e+00, -1.1491e+00,  1.1711e+01,  3.4049e+00,  7.0236e+00,\n",
       "          8.0327e+00, -4.2412e+00, -2.6836e+00,  5.4403e+00, -6.0436e-01,\n",
       "         -5.9352e+00, -1.0976e-01, -3.7157e+00, -1.6862e+01,  6.0787e+00,\n",
       "          3.8306e+00, -1.6730e+01,  1.5762e+00, -3.5318e+00, -1.6916e+01,\n",
       "         -1.6834e+01, -5.8285e+00, -1.6953e+01, -1.6873e+01, -1.6856e+01,\n",
       "         -1.6951e+01, -1.7029e+01, -3.4943e+00, -1.6835e+01, -1.6858e+01,\n",
       "         -1.6891e+01, -1.6945e+01, -1.6927e+01, -1.6917e+01, -1.6983e+01,\n",
       "         -4.0244e+00, -2.5741e+00, -1.7066e+01, -1.6893e+01, -1.6819e+01,\n",
       "         -1.6907e+01, -1.6871e+01, -1.6809e+01, -1.6817e+01, -1.7037e+01,\n",
       "         -1.6874e+01, -1.6985e+01, -4.3415e+00, -1.6960e+01, -1.6917e+01,\n",
       "         -1.6991e+01, -1.6981e+01, -1.6890e+01, -1.6853e+01, -1.6898e+01,\n",
       "         -1.6964e+01, -1.6879e+01, -1.7040e+01, -1.6765e+01, -1.6893e+01,\n",
       "         -1.6979e+01, -1.6919e+01, -1.7112e+01, -1.7003e+01, -1.6896e+01,\n",
       "         -1.6870e+01, -1.6851e+01, -1.6970e+01, -1.6964e+01, -1.5540e+01,\n",
       "         -1.6757e+01],\n",
       "        [-8.7890e+00, -1.3154e+01, -6.4551e-01, -1.3098e+01,  3.3229e+00,\n",
       "          6.4389e-02,  2.0046e+01, -2.0828e+00, -7.0188e-01, -6.5345e-01,\n",
       "          1.0203e+00,  6.0030e-01,  3.7610e+00, -3.6021e+00,  2.0665e+00,\n",
       "         -4.3932e+00,  2.2074e+00,  4.3260e+00, -3.1799e+00, -2.8983e-01,\n",
       "         -4.5543e+00, -4.4368e+00, -5.4166e+00, -1.3122e+01,  7.6190e+00,\n",
       "         -3.2996e+00, -1.3014e+01, -4.9847e+00,  2.7634e+00, -1.3094e+01,\n",
       "         -1.3113e+01, -2.8202e+00, -1.3059e+01, -1.3183e+01, -1.3143e+01,\n",
       "         -1.3137e+01, -1.3085e+01, -7.5945e+00, -1.3136e+01, -1.3037e+01,\n",
       "         -1.3049e+01, -1.3081e+01, -1.3141e+01, -1.3142e+01, -1.3121e+01,\n",
       "          2.7582e+00, -5.3239e+00, -1.3081e+01, -1.3268e+01, -1.3118e+01,\n",
       "         -1.3187e+01, -1.3098e+01, -1.3098e+01, -1.2919e+01, -1.3148e+01,\n",
       "         -1.3166e+01, -1.3224e+01, -4.2674e+00, -1.3216e+01, -1.3249e+01,\n",
       "         -1.3106e+01, -1.2976e+01, -1.3105e+01, -1.2944e+01, -1.3087e+01,\n",
       "         -1.3141e+01, -1.3047e+01, -1.3253e+01, -1.3028e+01, -1.3125e+01,\n",
       "         -1.3101e+01, -1.3319e+01, -1.3203e+01, -1.3196e+01, -1.3160e+01,\n",
       "         -1.3194e+01, -1.3097e+01, -1.3179e+01, -1.3115e+01, -1.2651e+01,\n",
       "         -1.3028e+01],\n",
       "        [-2.4220e+01, -1.6895e+01,  1.7025e+00, -1.6805e+01,  7.1925e+00,\n",
       "          3.0706e+00, -7.6066e-01,  4.4025e+00, -5.4450e+00, -2.2222e+00,\n",
       "          1.2081e+00,  5.6264e-01,  5.0047e+00,  4.3109e+00, -3.2481e+00,\n",
       "          1.1275e+00, -5.9119e+00,  2.1554e+00,  1.5825e+00,  1.1244e+00,\n",
       "         -3.6356e+00,  4.2976e-01, -7.2730e+00, -1.6669e+01,  6.5047e+00,\n",
       "         -1.7697e+00, -1.6569e+01, -1.4540e+00,  3.8655e+00, -1.6772e+01,\n",
       "         -1.6772e+01, -2.4985e+00, -1.6764e+01, -1.6756e+01, -1.6639e+01,\n",
       "         -1.6852e+01, -1.6680e+01, -4.3356e+00, -1.6552e+01, -1.6778e+01,\n",
       "         -1.6687e+01, -1.6798e+01, -1.6735e+01, -1.6685e+01, -1.6785e+01,\n",
       "          1.2320e+00, -6.1120e+00, -1.6819e+01, -1.6754e+01, -1.6646e+01,\n",
       "         -1.6919e+01, -1.6657e+01, -1.6762e+01, -1.6611e+01, -1.6910e+01,\n",
       "         -1.6773e+01, -1.6912e+01, -9.3478e+00, -1.6700e+01, -1.6749e+01,\n",
       "         -1.6804e+01, -1.6835e+01, -1.6591e+01, -1.6791e+01, -1.6787e+01,\n",
       "         -1.6852e+01, -1.6802e+01, -1.6926e+01, -1.6686e+01, -1.6762e+01,\n",
       "         -1.6802e+01, -1.6664e+01, -1.6903e+01, -1.6709e+01, -1.6742e+01,\n",
       "         -1.6704e+01, -1.6784e+01, -1.6767e+01, -1.6876e+01, -1.4905e+01,\n",
       "         -1.6679e+01]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs_hf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6eeff463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(lprobs - lprobs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4e9dc",
   "metadata": {},
   "source": [
    "Run the full model to make sure this doesn't give any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         decoder_input_ids=torch.tensor([[3, 4, 5]]),\n",
    "         #decoder_input_ids=torch.tensor([[3, 4, 5], [2, 2, 2]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "677718c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logits', 'past_key_values', 'encoder_last_hidden_state']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f2bcec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 81])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf0244b",
   "metadata": {},
   "source": [
    "Also calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1b7bd1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.Seq2SeqLMOutput"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[2,  4, 18, 10, 12,  6,  5]]),\n",
    "         labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13]]),\n",
    "         #labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13], [4, 18, 10, 12,  6,  5, 13]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f34dd766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 81])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4edf2044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.1307e-06)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195961eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2ce14a",
   "metadata": {},
   "source": [
    "Generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "72ccdbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b2fc2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = hf_model.generate(inputs.input_values, max_length=100)\n",
    "# hf_outputs = hf_model.generate(inputs.input_values, num_beams=5, max_length=100) #, bos_token_id=2)\n",
    "# hf_outputs = hf_model.generate(torch.rand(1, 10000), num_beams=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "015c80b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 92])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce7e1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4, 18, 10, 12,  6,  5, 13,  4, 45, 16, 10, 15,  6,  5, 13,  4, 10,\n",
       "         12,  4,  6, 11,  5,  4,  7, 24,  8, 12,  6, 15,  5,  4,  8, 19,  4,  6,\n",
       "         11,  5,  4, 18, 10, 14, 14, 15,  5,  4, 17, 15,  7, 12, 12,  5, 12,  4,\n",
       "          7,  9, 14,  4, 20,  5,  4,  7, 13,  5,  4, 21, 15,  7, 14,  4,  6,  8,\n",
       "          4, 20,  5, 15, 17,  8, 18,  5,  4, 11, 10, 12,  4, 21,  8, 12, 24,  5,\n",
       "         15,  2]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "11fce9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ m i s t e r ▁ q u i l t e r ▁ i s ▁ t h e ▁ a p o s t l e ▁ o f ▁ t h e ▁ m i d d l e ▁ c l a s s e s ▁ a n d ▁ w e ▁ a r e ▁ g l a d ▁ t o ▁ w e l c o m e ▁ h i s ▁ g o s p e l\n",
      "mister quilter is the apostle of the middle classes and we are glad to welcome his gospel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(hf_outputs.shape[0]):\n",
    "    print(tgt_dict.string(hf_outputs[i]))\n",
    "    print(tokenizer.decode(tgt_dict.string(hf_outputs[i])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "997fd48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<pad>', '</s>', '<unk>', '▁']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tgt_dict[x] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a116f698",
   "metadata": {},
   "source": [
    "For comparison, Speech2Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00306c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "s2t_model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "s2t_processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f994828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthijs/anaconda3/envs/t5/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Reusing dataset librispeech_asr_dummy (/Users/matthijs/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 584, 80])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = s2t_processor(\n",
    "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n",
    ")\n",
    "input_features = inputs.input_features\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "dc8ef5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 584, 80])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test batch\n",
    "# input_features = torch.tile(input_features, dims=(2, 1, 1))\n",
    "# input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/transformers/src/transformers/models/speech_to_text/modeling_speech_to_text.py:561: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n",
      "/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/transformers/src/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = s2t_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = s2t_processor.batch_decode(generated_ids)[0]\n",
    "transcription\n",
    "\n",
    "#'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "31a21206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/s2t-small-librispeech-asr were not used when initializing Speech2TextModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Speech2TextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Speech2TextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Speech2TextModel were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** has_prefix_module True expects_prefix_module False\n",
      "*** remove_prefix_from_model False add_prefix_to_model True\n"
     ]
    }
   ],
   "source": [
    "from transformers import Speech2TextModel\n",
    "\n",
    "s2t_model = Speech2TextModel.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50884f2",
   "metadata": {},
   "source": [
    "Test other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cd38a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART, Speech2Text, Wav2Vec2 don't have pruning\n",
    "#hf_model.prune_heads({1: [0, 2], 2: [2,3 ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9778431c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(81, 768, padding_idx=1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "44be3e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=81, bias=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f954bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.resize_token_embeddings(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b17c7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e8369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0974c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0b945156",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_ctc.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "59c092a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_naked.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
