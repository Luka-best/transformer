{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 931.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'embeddings'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "retrieval_vector_size = 768\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"id\": [\"0\", \"1\"] * 5,\n",
    "        \"text\": [\"My favourite number is 3455\", \"The secret word is FROG\"] * 5,\n",
    "        \"embeddings\": [\n",
    "            0.1 * np.ones(retrieval_vector_size),\n",
    "            0.9 * np.ones(retrieval_vector_size),\n",
    "        ] * 5,\n",
    "    }\n",
    ")\n",
    "dataset.add_faiss_index(\"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_passage_encoder DualEncoderRetriever(\n",
      "  (contriever): Contriever(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "generator FiD(\n",
      "  (shared): Embedding(32128, 768)\n",
      "  (encoder): FiDStack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AtlasModel, AtlasTokenizer\n",
    "\n",
    "\n",
    "# tokenizer = AtlasTokenizer.from_pretrained('data/atlas-pretrained')\n",
    "atlas = AtlasModel.from_pretrained('data/atlas-pretrained', index=dataset)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0104,  0.0103,  0.0042,  ..., -0.0640,  0.0247, -0.0228],\n",
      "        [-0.0708,  0.0181,  0.0579,  ..., -0.0396, -0.0310,  0.0322],\n",
      "        [ 0.0053,  0.0258,  0.0131,  ..., -0.0067,  0.0291, -0.0206],\n",
      "        ...,\n",
      "        [-0.0684, -0.0747,  0.0236,  ..., -0.0172, -0.0154,  0.0140],\n",
      "        [ 0.0713, -0.0287,  0.0121,  ..., -0.0211,  0.0136,  0.0103],\n",
      "        [-0.0063,  0.0234,  0.0178,  ...,  0.0117,  0.0115,  0.0515]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(atlas.query_passage_encoder.contriever.encoder.layer[0].intermediate.dense.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AtlasTokenizer\n",
    "bertModelString = \"facebook/contriever\"\n",
    "t5ModelString = \"google/t5-base-lm-adapt\"\n",
    "bertTokenizer = AutoTokenizer.from_pretrained(bertModelString)\n",
    "t5Tokenizer = AutoTokenizer.from_pretrained(t5ModelString)\n",
    "\n",
    "tokenizer = AtlasTokenizer(bertTokenizer, t5Tokenizer)\n",
    "atlas.index = dataset\n",
    "atlas.query_encoder_tokenizer = tokenizer.query_encoder\n",
    "atlas.generator_tokenizer = tokenizer.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.11ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3692.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'embeddings'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reindex(examples):\n",
    "    tokenized = tokenizer(examples['text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    hidden_states = atlas.query_passage_encoder.embed_passages(input_ids=tokenized[\"input_ids\"], attention_mask=tokenized[\"attention_mask\"])\n",
    "    examples['embeddings'] = hidden_states.cpu().detach().numpy()\n",
    "    return examples\n",
    "\n",
    "atlas.index = atlas.index.map(reindex, batched=True)\n",
    "atlas.index.add_faiss_index(\"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'text': 'My favourite number is 3455',\n",
       " 'embeddings': [0.16717126965522766,\n",
       "  -0.1048618033528328,\n",
       "  -0.0965828076004982,\n",
       "  -0.031876325607299805,\n",
       "  -0.06867621093988419,\n",
       "  0.05314997583627701,\n",
       "  -0.09164512902498245,\n",
       "  -0.12781034409999847,\n",
       "  0.11803703010082245,\n",
       "  -0.09109914302825928,\n",
       "  -0.05130354315042496,\n",
       "  -0.04570767655968666,\n",
       "  0.08756228536367416,\n",
       "  0.026555202901363373,\n",
       "  0.022263843566179276,\n",
       "  0.02476108819246292,\n",
       "  0.02554764226078987,\n",
       "  0.1662392020225525,\n",
       "  -0.018088677898049355,\n",
       "  0.17237502336502075,\n",
       "  0.008265756070613861,\n",
       "  0.045811355113983154,\n",
       "  0.06819970905780792,\n",
       "  0.04561295360326767,\n",
       "  -0.021892188116908073,\n",
       "  -0.054893020540475845,\n",
       "  0.036949463188648224,\n",
       "  -0.013635585084557533,\n",
       "  -0.019827701151371002,\n",
       "  -0.06447269022464752,\n",
       "  0.01264115422964096,\n",
       "  0.059305477887392044,\n",
       "  0.038500599563121796,\n",
       "  0.04173417389392853,\n",
       "  -0.014832627028226852,\n",
       "  -0.05303023010492325,\n",
       "  0.11089310795068741,\n",
       "  0.0209842287003994,\n",
       "  0.02175111323595047,\n",
       "  0.0946677029132843,\n",
       "  -0.09384378790855408,\n",
       "  -0.11464206874370575,\n",
       "  -0.01871468871831894,\n",
       "  -0.041148073971271515,\n",
       "  -0.0228657815605402,\n",
       "  0.04508322477340698,\n",
       "  -0.14835697412490845,\n",
       "  -0.031397633254528046,\n",
       "  0.050030868500471115,\n",
       "  -0.044995732605457306,\n",
       "  0.05867411941289902,\n",
       "  0.06418468058109283,\n",
       "  -0.02149016782641411,\n",
       "  -0.14789575338363647,\n",
       "  -0.018126286566257477,\n",
       "  0.030852094292640686,\n",
       "  -0.13060130178928375,\n",
       "  -0.004990465939044952,\n",
       "  0.024407051503658295,\n",
       "  -0.16590158641338348,\n",
       "  0.05872982740402222,\n",
       "  0.008152961730957031,\n",
       "  0.04402922838926315,\n",
       "  0.06654603034257889,\n",
       "  -0.08675984293222427,\n",
       "  -0.06249158829450607,\n",
       "  -0.10902617126703262,\n",
       "  0.04675007611513138,\n",
       "  0.0015246439725160599,\n",
       "  -0.18741443753242493,\n",
       "  -0.10947256535291672,\n",
       "  -0.05817126855254173,\n",
       "  0.04840785264968872,\n",
       "  -0.12332406640052795,\n",
       "  -0.050911352038383484,\n",
       "  -0.17590215802192688,\n",
       "  0.05378573015332222,\n",
       "  0.11393356323242188,\n",
       "  0.058778174221515656,\n",
       "  -0.09807388484477997,\n",
       "  -0.05688154324889183,\n",
       "  -0.016559503972530365,\n",
       "  -0.04644319787621498,\n",
       "  -0.13045886158943176,\n",
       "  0.018503345549106598,\n",
       "  -0.0914556160569191,\n",
       "  0.020526986569166183,\n",
       "  -0.0731692984700203,\n",
       "  0.009925104677677155,\n",
       "  -0.061492275446653366,\n",
       "  0.029278604313731194,\n",
       "  0.04206777736544609,\n",
       "  0.04407999664545059,\n",
       "  -0.10873177647590637,\n",
       "  0.05705513805150986,\n",
       "  -0.04545965790748596,\n",
       "  -0.07024434208869934,\n",
       "  -0.10504601150751114,\n",
       "  0.15662924945354462,\n",
       "  -0.13908439874649048,\n",
       "  0.11616931855678558,\n",
       "  -0.012075135484337807,\n",
       "  -0.029159167781472206,\n",
       "  -0.07197140157222748,\n",
       "  -0.04043316841125488,\n",
       "  0.018523212522268295,\n",
       "  0.17039503157138824,\n",
       "  -0.05543727055191994,\n",
       "  -0.13962940871715546,\n",
       "  -0.06383917480707169,\n",
       "  0.03676310181617737,\n",
       "  -0.22060568630695343,\n",
       "  0.06286967545747757,\n",
       "  -0.020111767575144768,\n",
       "  -0.05559482425451279,\n",
       "  -0.12471768260002136,\n",
       "  -0.14406174421310425,\n",
       "  -0.03938528895378113,\n",
       "  0.07055654376745224,\n",
       "  0.09864743798971176,\n",
       "  -0.021312985569238663,\n",
       "  -0.08418749272823334,\n",
       "  -0.14363479614257812,\n",
       "  -0.0800165981054306,\n",
       "  -0.1278427094221115,\n",
       "  -0.11821654438972473,\n",
       "  -0.15095114707946777,\n",
       "  0.010159086436033249,\n",
       "  0.01779397949576378,\n",
       "  0.05077004432678223,\n",
       "  0.059038832783699036,\n",
       "  -0.09870757162570953,\n",
       "  0.051014263182878494,\n",
       "  0.026865873485803604,\n",
       "  -0.018937550485134125,\n",
       "  0.09157323092222214,\n",
       "  -0.12068318575620651,\n",
       "  0.030539099127054214,\n",
       "  0.194668248295784,\n",
       "  0.22644028067588806,\n",
       "  0.018510526046156883,\n",
       "  -0.06920180469751358,\n",
       "  0.04663803055882454,\n",
       "  0.01199948787689209,\n",
       "  0.1829950362443924,\n",
       "  -0.041039109230041504,\n",
       "  0.03470258414745331,\n",
       "  -0.11863242834806442,\n",
       "  -0.035145968198776245,\n",
       "  0.15604151785373688,\n",
       "  -0.08317605406045914,\n",
       "  -0.030360858887434006,\n",
       "  0.1072671040892601,\n",
       "  -0.05345534905791283,\n",
       "  0.020887508988380432,\n",
       "  0.040233779698610306,\n",
       "  -0.1137225478887558,\n",
       "  -0.029529523104429245,\n",
       "  0.05244390666484833,\n",
       "  0.08600026369094849,\n",
       "  -0.00940646417438984,\n",
       "  -0.0786154493689537,\n",
       "  -0.0721556544303894,\n",
       "  -0.1462002843618393,\n",
       "  -0.20461145043373108,\n",
       "  0.006686544045805931,\n",
       "  -0.04834762588143349,\n",
       "  -0.29865166544914246,\n",
       "  0.11824780702590942,\n",
       "  0.07714731246232986,\n",
       "  0.10052909702062607,\n",
       "  -0.07211704552173615,\n",
       "  0.10705462098121643,\n",
       "  -0.0425841361284256,\n",
       "  -0.04468587040901184,\n",
       "  -0.10571122914552689,\n",
       "  0.1751347780227661,\n",
       "  0.17329397797584534,\n",
       "  -0.0994580090045929,\n",
       "  -0.09117516875267029,\n",
       "  -0.09242208302021027,\n",
       "  -0.0744912326335907,\n",
       "  -0.009259846061468124,\n",
       "  0.0012208521366119385,\n",
       "  -0.035603441298007965,\n",
       "  0.07755985856056213,\n",
       "  -0.22519153356552124,\n",
       "  0.00022418983280658722,\n",
       "  -0.043120503425598145,\n",
       "  -0.04538600891828537,\n",
       "  0.042695239186286926,\n",
       "  -0.05029362440109253,\n",
       "  0.09990578889846802,\n",
       "  -0.056186217814683914,\n",
       "  -0.10687322914600372,\n",
       "  0.003522537648677826,\n",
       "  0.10094048082828522,\n",
       "  -0.09026433527469635,\n",
       "  0.08137185126543045,\n",
       "  0.1325688362121582,\n",
       "  -0.12201521545648575,\n",
       "  0.06457005441188812,\n",
       "  -0.08730130642652512,\n",
       "  -0.14770258963108063,\n",
       "  -0.001580345444381237,\n",
       "  -0.09106744825839996,\n",
       "  0.01166430115699768,\n",
       "  -0.009614571928977966,\n",
       "  -0.08596106618642807,\n",
       "  0.028264597058296204,\n",
       "  -0.054825276136398315,\n",
       "  0.00818371120840311,\n",
       "  0.09696996212005615,\n",
       "  -0.0721030905842781,\n",
       "  -0.03954429551959038,\n",
       "  0.045673031359910965,\n",
       "  -0.07290539890527725,\n",
       "  0.020039081573486328,\n",
       "  -0.1092376857995987,\n",
       "  -0.07483937591314316,\n",
       "  -0.011617660522460938,\n",
       "  -0.0010575316846370697,\n",
       "  -0.0538678839802742,\n",
       "  0.07037452608346939,\n",
       "  0.17276988923549652,\n",
       "  -0.18754620850086212,\n",
       "  0.003956723492592573,\n",
       "  -0.02432955428957939,\n",
       "  0.2107849270105362,\n",
       "  0.04405989125370979,\n",
       "  0.054879747331142426,\n",
       "  -0.18004503846168518,\n",
       "  0.03633805364370346,\n",
       "  -0.06865397840738297,\n",
       "  0.028428450226783752,\n",
       "  -0.02150132693350315,\n",
       "  0.01765066385269165,\n",
       "  0.001848369836807251,\n",
       "  -0.10689215362071991,\n",
       "  0.037667132914066315,\n",
       "  0.10133721679449081,\n",
       "  -0.03050101548433304,\n",
       "  -0.018100354820489883,\n",
       "  0.010029282420873642,\n",
       "  0.0924723744392395,\n",
       "  -0.14852535724639893,\n",
       "  -0.002798924222588539,\n",
       "  -0.010569630190730095,\n",
       "  0.11370907723903656,\n",
       "  0.015946930274367332,\n",
       "  -0.12400534749031067,\n",
       "  -0.0962933823466301,\n",
       "  0.058418553322553635,\n",
       "  0.041037335991859436,\n",
       "  -0.11906346678733826,\n",
       "  0.014012360945343971,\n",
       "  -0.0376310721039772,\n",
       "  -0.05660386383533478,\n",
       "  0.13290125131607056,\n",
       "  0.12149997055530548,\n",
       "  -0.014871923252940178,\n",
       "  0.07133649289608002,\n",
       "  0.022779347375035286,\n",
       "  0.04369685798883438,\n",
       "  -0.02746305614709854,\n",
       "  -0.0958736389875412,\n",
       "  -0.08165369927883148,\n",
       "  -0.004252877086400986,\n",
       "  0.05508672818541527,\n",
       "  -0.0008211266249418259,\n",
       "  0.06423312425613403,\n",
       "  -0.013614676892757416,\n",
       "  -0.12614965438842773,\n",
       "  -0.18931187689304352,\n",
       "  0.07991787791252136,\n",
       "  -0.010594673454761505,\n",
       "  0.1288715898990631,\n",
       "  0.047784291207790375,\n",
       "  0.08661343157291412,\n",
       "  -0.024797700345516205,\n",
       "  0.1391637772321701,\n",
       "  -0.09138472378253937,\n",
       "  -0.15587179362773895,\n",
       "  -0.03666519373655319,\n",
       "  0.06169932335615158,\n",
       "  0.1954171359539032,\n",
       "  0.05518057569861412,\n",
       "  -0.011564282700419426,\n",
       "  0.07804358005523682,\n",
       "  -0.07019113749265671,\n",
       "  0.032096199691295624,\n",
       "  0.11387115716934204,\n",
       "  -0.097512386739254,\n",
       "  -0.08553901314735413,\n",
       "  0.032640665769577026,\n",
       "  -0.03384886309504509,\n",
       "  -0.0022580623626708984,\n",
       "  -0.04262033849954605,\n",
       "  -0.018788158893585205,\n",
       "  0.0020414739847183228,\n",
       "  -0.006837308406829834,\n",
       "  -0.04628945142030716,\n",
       "  -0.08262285590171814,\n",
       "  -0.11295008659362793,\n",
       "  -0.08934709429740906,\n",
       "  0.16142114996910095,\n",
       "  -0.057523809373378754,\n",
       "  0.050081390887498856,\n",
       "  -0.28869956731796265,\n",
       "  -0.03623779118061066,\n",
       "  -0.10978938639163971,\n",
       "  -0.027048856019973755,\n",
       "  -0.04652860760688782,\n",
       "  -0.23280440270900726,\n",
       "  -0.10184023529291153,\n",
       "  0.003890649415552616,\n",
       "  -0.17647704482078552,\n",
       "  0.026640931144356728,\n",
       "  -0.07191330939531326,\n",
       "  -0.09903173893690109,\n",
       "  0.06646793335676193,\n",
       "  -0.0929543748497963,\n",
       "  0.012269953265786171,\n",
       "  -0.03419610485434532,\n",
       "  0.047586631029844284,\n",
       "  -0.03973754495382309,\n",
       "  0.05171425640583038,\n",
       "  -0.14922039210796356,\n",
       "  0.03229716047644615,\n",
       "  -0.03646084666252136,\n",
       "  0.10206520557403564,\n",
       "  -0.05065840482711792,\n",
       "  0.006138993427157402,\n",
       "  0.0006560012698173523,\n",
       "  0.0450877919793129,\n",
       "  -0.04118364304304123,\n",
       "  -0.013043411076068878,\n",
       "  0.03625822067260742,\n",
       "  -0.07020533829927444,\n",
       "  0.2654015123844147,\n",
       "  -0.14146393537521362,\n",
       "  0.12477022409439087,\n",
       "  0.1424575299024582,\n",
       "  -0.18853315711021423,\n",
       "  0.012841831892728806,\n",
       "  -0.17103959619998932,\n",
       "  -0.09381710737943649,\n",
       "  -0.06325376033782959,\n",
       "  -0.04588970169425011,\n",
       "  -0.12091904133558273,\n",
       "  0.11139992624521255,\n",
       "  -0.010946003720164299,\n",
       "  0.021804258227348328,\n",
       "  -0.043901365250349045,\n",
       "  -0.03977201506495476,\n",
       "  -0.04112981632351875,\n",
       "  -0.0629696249961853,\n",
       "  -0.09478522092103958,\n",
       "  -0.1066313087940216,\n",
       "  -0.07913744449615479,\n",
       "  -0.021581020206212997,\n",
       "  -0.0746573805809021,\n",
       "  0.12218698114156723,\n",
       "  -0.1975969523191452,\n",
       "  -0.13247224688529968,\n",
       "  -0.01991640403866768,\n",
       "  -0.0892634317278862,\n",
       "  -0.06384602934122086,\n",
       "  0.13365687429904938,\n",
       "  0.012325620278716087,\n",
       "  0.12774014472961426,\n",
       "  0.06060703843832016,\n",
       "  -0.19417348504066467,\n",
       "  0.056895770132541656,\n",
       "  -0.09603631496429443,\n",
       "  0.03257258981466293,\n",
       "  -0.03734412044286728,\n",
       "  -0.08962223678827286,\n",
       "  -0.09660237282514572,\n",
       "  -0.14604048430919647,\n",
       "  -0.11850040405988693,\n",
       "  -0.1023360937833786,\n",
       "  -0.041461531072854996,\n",
       "  -0.18148580193519592,\n",
       "  -0.045012518763542175,\n",
       "  -0.0336274728178978,\n",
       "  0.14611467719078064,\n",
       "  0.002664569765329361,\n",
       "  0.15113040804862976,\n",
       "  -0.05426075682044029,\n",
       "  0.07813899964094162,\n",
       "  0.033767346292734146,\n",
       "  -0.007114941254258156,\n",
       "  -0.05147435888648033,\n",
       "  -0.03619314730167389,\n",
       "  -0.08253053575754166,\n",
       "  0.013077298179268837,\n",
       "  0.05773849040269852,\n",
       "  -0.007695573382079601,\n",
       "  0.1163937970995903,\n",
       "  -0.0069597139954566956,\n",
       "  0.04324018210172653,\n",
       "  0.018497534096240997,\n",
       "  0.20045822858810425,\n",
       "  0.037469811737537384,\n",
       "  0.10501140356063843,\n",
       "  -0.09355175495147705,\n",
       "  -0.10035479813814163,\n",
       "  0.16869789361953735,\n",
       "  -0.03454159200191498,\n",
       "  -0.08917143940925598,\n",
       "  -0.05606589466333389,\n",
       "  0.02649080380797386,\n",
       "  -0.01940680854022503,\n",
       "  -0.10283714532852173,\n",
       "  0.029354721307754517,\n",
       "  -0.1257222294807434,\n",
       "  -0.022942595183849335,\n",
       "  0.031261079013347626,\n",
       "  0.02874448150396347,\n",
       "  0.05682115629315376,\n",
       "  0.06291110813617706,\n",
       "  -0.06080910190939903,\n",
       "  -0.23604045808315277,\n",
       "  0.09472237527370453,\n",
       "  -0.05965059995651245,\n",
       "  -0.1808469295501709,\n",
       "  -0.09243234992027283,\n",
       "  0.1485498547554016,\n",
       "  0.03215241804718971,\n",
       "  -0.05840449035167694,\n",
       "  0.2050313800573349,\n",
       "  -0.05795861780643463,\n",
       "  -0.0203329399228096,\n",
       "  -0.0475228987634182,\n",
       "  -0.08304472267627716,\n",
       "  0.10948150604963303,\n",
       "  -0.056801702827215195,\n",
       "  -0.05119428038597107,\n",
       "  -0.08127011358737946,\n",
       "  -0.06598721444606781,\n",
       "  -0.030178390443325043,\n",
       "  -0.028544453904032707,\n",
       "  -0.02541739121079445,\n",
       "  0.05046655237674713,\n",
       "  0.09640733152627945,\n",
       "  0.15164464712142944,\n",
       "  -0.057297058403491974,\n",
       "  0.011545786634087563,\n",
       "  0.004867196083068848,\n",
       "  -0.11140209436416626,\n",
       "  0.06942283362150192,\n",
       "  0.02724013850092888,\n",
       "  0.00839298963546753,\n",
       "  0.0024710940197110176,\n",
       "  0.015291906893253326,\n",
       "  -0.08361195772886276,\n",
       "  -0.037774063646793365,\n",
       "  -0.03831477835774422,\n",
       "  -0.1776381880044937,\n",
       "  -0.11545705795288086,\n",
       "  0.15222527086734772,\n",
       "  -0.148018017411232,\n",
       "  -0.08758530020713806,\n",
       "  0.10409127920866013,\n",
       "  0.0350620299577713,\n",
       "  -0.15048496425151825,\n",
       "  0.05780348181724548,\n",
       "  0.13989514112472534,\n",
       "  -0.009792841970920563,\n",
       "  -0.10642920434474945,\n",
       "  -0.003875168040394783,\n",
       "  -0.15660850703716278,\n",
       "  -0.13190943002700806,\n",
       "  0.028727304190397263,\n",
       "  -0.07828053832054138,\n",
       "  -0.08964672684669495,\n",
       "  -0.08210514485836029,\n",
       "  -0.06282041221857071,\n",
       "  -0.04591740295290947,\n",
       "  0.09607817232608795,\n",
       "  -0.0910971388220787,\n",
       "  -0.0820910781621933,\n",
       "  -0.18090152740478516,\n",
       "  -0.10017768293619156,\n",
       "  0.09266816079616547,\n",
       "  -0.206353560090065,\n",
       "  0.012229733169078827,\n",
       "  0.024191103875637054,\n",
       "  0.12438784539699554,\n",
       "  -0.02232370153069496,\n",
       "  0.020951654762029648,\n",
       "  0.03828829526901245,\n",
       "  0.2056400328874588,\n",
       "  0.09844909608364105,\n",
       "  0.03450833261013031,\n",
       "  0.21308892965316772,\n",
       "  -0.16161459684371948,\n",
       "  0.03945758938789368,\n",
       "  0.1901705414056778,\n",
       "  -0.12868636846542358,\n",
       "  -0.15574710071086884,\n",
       "  -0.074390709400177,\n",
       "  0.0753445029258728,\n",
       "  -0.04932616278529167,\n",
       "  0.07609084248542786,\n",
       "  -0.10261133313179016,\n",
       "  0.07281296700239182,\n",
       "  -0.022203925997018814,\n",
       "  0.007247639819979668,\n",
       "  0.05004042014479637,\n",
       "  0.10187409818172455,\n",
       "  -0.20313195884227753,\n",
       "  0.06452767550945282,\n",
       "  -0.10331548750400543,\n",
       "  0.03420461341738701,\n",
       "  0.05765704810619354,\n",
       "  -0.09125904738903046,\n",
       "  0.07444740831851959,\n",
       "  -0.033124856650829315,\n",
       "  0.0036545973271131516,\n",
       "  0.04698275774717331,\n",
       "  -0.056949637830257416,\n",
       "  -0.05631377547979355,\n",
       "  0.00507049635052681,\n",
       "  -0.04686596244573593,\n",
       "  0.05228748545050621,\n",
       "  0.011327952146530151,\n",
       "  0.02426384575664997,\n",
       "  0.013285262510180473,\n",
       "  -0.09941992163658142,\n",
       "  -0.0394088551402092,\n",
       "  -0.08104663342237473,\n",
       "  -0.040728624910116196,\n",
       "  -0.11833538860082626,\n",
       "  0.0374811515212059,\n",
       "  -0.04486129432916641,\n",
       "  0.012669245712459087,\n",
       "  0.025322221219539642,\n",
       "  -0.005844719707965851,\n",
       "  0.3221811354160309,\n",
       "  0.021834393963217735,\n",
       "  0.10812506824731827,\n",
       "  -0.080406054854393,\n",
       "  -0.040579136461019516,\n",
       "  0.07561632990837097,\n",
       "  -0.04743105545639992,\n",
       "  -0.08864656835794449,\n",
       "  -0.11153262108564377,\n",
       "  -0.01813909411430359,\n",
       "  -0.07716231793165207,\n",
       "  0.07650494575500488,\n",
       "  0.02662770263850689,\n",
       "  -0.025447824969887733,\n",
       "  0.06276914477348328,\n",
       "  0.14701063930988312,\n",
       "  -0.02152305841445923,\n",
       "  -0.02170497737824917,\n",
       "  -0.13117535412311554,\n",
       "  -0.0031303009018301964,\n",
       "  -0.06934837996959686,\n",
       "  -0.04083176702260971,\n",
       "  0.06222272664308548,\n",
       "  -0.03609965369105339,\n",
       "  0.027350831776857376,\n",
       "  -0.013080866076052189,\n",
       "  -0.016122940927743912,\n",
       "  -0.29223746061325073,\n",
       "  -0.14234855771064758,\n",
       "  -0.10600593686103821,\n",
       "  -0.05192052945494652,\n",
       "  -0.00787482038140297,\n",
       "  0.007441077381372452,\n",
       "  -0.05341009423136711,\n",
       "  0.09940764307975769,\n",
       "  -0.2312963902950287,\n",
       "  0.03242848813533783,\n",
       "  0.03109990432858467,\n",
       "  -0.018183177337050438,\n",
       "  0.0729164108633995,\n",
       "  -0.052136510610580444,\n",
       "  0.05766575410962105,\n",
       "  -0.06932052224874496,\n",
       "  -0.2669619917869568,\n",
       "  0.08046950399875641,\n",
       "  0.010951913893222809,\n",
       "  -0.012434657663106918,\n",
       "  0.09947264194488525,\n",
       "  -0.10880053788423538,\n",
       "  0.01475498080253601,\n",
       "  0.038222987204790115,\n",
       "  -0.03172464668750763,\n",
       "  -0.08461658656597137,\n",
       "  0.007478658575564623,\n",
       "  0.1429469883441925,\n",
       "  -0.1734144389629364,\n",
       "  -0.17934224009513855,\n",
       "  0.14075613021850586,\n",
       "  0.001725420355796814,\n",
       "  -0.02914811670780182,\n",
       "  -0.017321841791272163,\n",
       "  -0.0010093748569488525,\n",
       "  0.0025823041796684265,\n",
       "  0.05306997895240784,\n",
       "  0.07169221341609955,\n",
       "  -0.10443412512540817,\n",
       "  -0.14329099655151367,\n",
       "  0.13163062930107117,\n",
       "  -0.10969369113445282,\n",
       "  -0.08512674272060394,\n",
       "  -0.04868323355913162,\n",
       "  0.03924509137868881,\n",
       "  -0.053285278379917145,\n",
       "  0.03701430559158325,\n",
       "  -0.07371551543474197,\n",
       "  -0.012351293116807938,\n",
       "  0.020484725013375282,\n",
       "  0.008552816696465015,\n",
       "  -0.05458863079547882,\n",
       "  -0.04559723660349846,\n",
       "  0.18222419917583466,\n",
       "  -0.10863570868968964,\n",
       "  -0.007971696555614471,\n",
       "  -0.09003840386867523,\n",
       "  0.04417179897427559,\n",
       "  0.06739316880702972,\n",
       "  0.030098509043455124,\n",
       "  -0.12701423466205597,\n",
       "  -0.07737185060977936,\n",
       "  0.07217124104499817,\n",
       "  0.016300994902849197,\n",
       "  0.07946808636188507,\n",
       "  -0.008241936564445496,\n",
       "  0.07507538050413132,\n",
       "  0.12557804584503174,\n",
       "  0.00799461081624031,\n",
       "  -0.10409543663263321,\n",
       "  0.02751794457435608,\n",
       "  -0.004954259842634201,\n",
       "  0.008501628413796425,\n",
       "  0.06059928238391876,\n",
       "  0.05726580321788788,\n",
       "  0.027284037321805954,\n",
       "  -0.23747502267360687,\n",
       "  -0.016568373888731003,\n",
       "  -0.09220843017101288,\n",
       "  -0.042762208729982376,\n",
       "  0.016384679824113846,\n",
       "  -0.009978018701076508,\n",
       "  -0.1087455004453659,\n",
       "  -0.1399739682674408,\n",
       "  0.03505638241767883,\n",
       "  -0.06048129126429558,\n",
       "  0.06109445542097092,\n",
       "  -0.02078760415315628,\n",
       "  -0.02568388544023037,\n",
       "  0.07501695305109024,\n",
       "  -0.051218319684267044,\n",
       "  0.106557697057724,\n",
       "  0.02274247631430626,\n",
       "  0.04934658110141754,\n",
       "  -0.22351416945457458,\n",
       "  -0.011354073882102966,\n",
       "  0.09157499670982361,\n",
       "  -0.056418657302856445,\n",
       "  0.06507813930511475,\n",
       "  -0.08184722065925598,\n",
       "  -0.17606940865516663,\n",
       "  -0.05568630248308182,\n",
       "  -0.0030845217406749725,\n",
       "  -0.16868716478347778,\n",
       "  0.04959213733673096,\n",
       "  -0.0941731333732605,\n",
       "  -0.07093364000320435,\n",
       "  0.0023608822375535965,\n",
       "  0.07718215137720108,\n",
       "  0.1212155818939209,\n",
       "  -0.1691429316997528,\n",
       "  -0.15648925304412842,\n",
       "  0.05637403577566147,\n",
       "  -0.05729217082262039,\n",
       "  0.04807828366756439,\n",
       "  -0.022858858108520508,\n",
       "  -0.022129181772470474,\n",
       "  -0.10474435985088348,\n",
       "  -0.024921612814068794,\n",
       "  0.05832650884985924,\n",
       "  0.12349005788564682,\n",
       "  -0.057328104972839355,\n",
       "  -0.09606326371431351,\n",
       "  0.009873882867395878,\n",
       "  0.11433838307857513,\n",
       "  0.14970813691616058,\n",
       "  0.020968284457921982,\n",
       "  -0.08108837902545929,\n",
       "  0.13181748986244202,\n",
       "  0.12301366031169891,\n",
       "  -0.07598899304866791,\n",
       "  0.03347405791282654,\n",
       "  -0.18599343299865723,\n",
       "  0.028000544756650925,\n",
       "  0.01948678307235241,\n",
       "  -0.09955146908760071,\n",
       "  0.021798381581902504,\n",
       "  -0.0847225934267044,\n",
       "  -0.03945189714431763,\n",
       "  0.08295010030269623,\n",
       "  0.13887962698936462,\n",
       "  0.06877148151397705,\n",
       "  0.00014129653573036194,\n",
       "  -0.15544043481349945,\n",
       "  -0.049576811492443085,\n",
       "  -0.05465748533606529,\n",
       "  -0.010036103427410126,\n",
       "  0.03928490728139877,\n",
       "  -0.06796464323997498,\n",
       "  0.06250603497028351,\n",
       "  0.16378244757652283,\n",
       "  0.044584643095731735,\n",
       "  -0.08427691459655762,\n",
       "  0.10083015263080597,\n",
       "  -0.0229119174182415,\n",
       "  0.08472535014152527,\n",
       "  -0.10815177857875824,\n",
       "  -0.036533042788505554,\n",
       "  -0.004595741629600525,\n",
       "  0.018399298191070557,\n",
       "  -0.14217296242713928,\n",
       "  0.08458919823169708,\n",
       "  -0.01691168174147606,\n",
       "  -0.055039502680301666,\n",
       "  0.08667731285095215,\n",
       "  0.04182521998882294,\n",
       "  -0.05118509382009506,\n",
       "  0.05492260307073593,\n",
       "  -0.05381680279970169,\n",
       "  -0.0016102232038974762,\n",
       "  -0.014000624418258667,\n",
       "  -0.18668155372142792,\n",
       "  -0.22182998061180115,\n",
       "  0.052757490426301956,\n",
       "  -0.012982307001948357,\n",
       "  -0.025414178147912025,\n",
       "  -0.04275094345211983,\n",
       "  0.019213993102312088,\n",
       "  -0.12162546813488007,\n",
       "  -0.06311744451522827,\n",
       "  -0.03464948385953903,\n",
       "  -0.03410205990076065,\n",
       "  0.05147609859704971,\n",
       "  -0.05222444236278534,\n",
       "  -0.02281196042895317,\n",
       "  0.26908043026924133,\n",
       "  -0.15056443214416504,\n",
       "  -0.010284505784511566,\n",
       "  0.10174560546875,\n",
       "  -0.062209758907556534,\n",
       "  -0.16488946974277496,\n",
       "  -0.048332422971725464,\n",
       "  0.014470487833023071,\n",
       "  -0.05466938018798828,\n",
       "  -0.1485411822795868,\n",
       "  -0.0054195597767829895,\n",
       "  0.0023217573761940002,\n",
       "  0.06624144315719604,\n",
       "  0.0618843249976635,\n",
       "  -0.20934191346168518,\n",
       "  0.12006433308124542]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atlas.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(0.0117, grad_fn=<NllLossBackward0>), logits=tensor([[[-25.2537, -11.2296,  -8.5146,  ..., -26.4157, -24.2000, -25.8787],\n",
       "         [-38.6698,  -9.2458, -13.9714,  ..., -38.4948, -38.6632, -38.2607],\n",
       "         [-45.0214, -13.7125, -13.7919,  ..., -44.6558, -45.3571, -44.9504],\n",
       "         [-35.5092,   0.5112,  -8.9547,  ..., -34.9981, -35.5459, -35.1949],\n",
       "         [-33.0204,  -2.6576,  -6.8508,  ..., -32.5283, -33.0761, -32.5818]],\n",
       "\n",
       "        [[-21.3347,  -9.7461,  -6.9815,  ..., -22.4861, -20.1532, -21.9368],\n",
       "         [-38.0355, -11.0931, -11.7789,  ..., -37.8584, -38.3653, -37.8470],\n",
       "         [-47.7797, -14.6103, -11.9611,  ..., -47.1814, -48.3027, -47.4551],\n",
       "         [-83.5846, -24.5328, -26.5067,  ..., -82.6250, -84.3740, -83.8924],\n",
       "         [-35.4897,   1.0183,  -8.4577,  ..., -35.0151, -35.6077, -35.1869]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.1032,  0.1641, -0.0708,  ...,  0.0842, -0.0786, -0.1330],\n",
       "         [-0.0276, -0.0377,  0.0749,  ...,  0.0157,  0.1054,  0.0394],\n",
       "         [ 0.0250,  0.2266,  0.0805,  ..., -0.0614, -0.1844,  0.0169],\n",
       "         ...,\n",
       "         [-0.0789, -0.2180, -0.0810,  ...,  0.1568,  0.0985, -0.0664],\n",
       "         [ 0.0121,  0.0101,  0.0182,  ...,  0.0016, -0.0054, -0.0009],\n",
       "         [-0.0257,  0.0045, -0.0577,  ...,  0.0889, -0.1261,  0.1641]],\n",
       "\n",
       "        [[-0.1063,  0.1104, -0.1089,  ..., -0.0012, -0.0780, -0.0781],\n",
       "         [-0.0302, -0.0934,  0.0512,  ..., -0.0189,  0.1214,  0.1322],\n",
       "         [-0.0699,  0.1624, -0.0226,  ..., -0.0017, -0.2509,  0.0693],\n",
       "         ...,\n",
       "         [ 0.1663,  0.0707, -0.0935,  ..., -0.0882, -0.0540,  0.0201],\n",
       "         [-0.1271, -0.0280, -0.0711,  ...,  0.1340,  0.2446, -0.1127],\n",
       "         [ 0.0138,  0.0111,  0.0198,  ...,  0.0020, -0.0064, -0.0004]]],\n",
       "       grad_fn=<ViewBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [\"What is my favourite number?\", \"What is the secret word?\"]\n",
    "target = [\"3455\", \"FROG\"]\n",
    "\n",
    "inputs = [f\"question: {question} answer: <extra_id_0>\" for question in inputs]\n",
    "target = [f\"<extra_id_0> {answer}\" for answer in target]\n",
    "\n",
    "atlas.forward(\n",
    "    inputs,\n",
    "    target,\n",
    "    None,\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     examples[\u001b[39m'\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m examples\n\u001b[0;32m----> 9\u001b[0m atlas\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m atlas\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mmap(reindex, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     10\u001b[0m atlas\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39madd_faiss_index(\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m, metric_type\u001b[39m=\u001b[39mfaiss\u001b[39m.\u001b[39mMETRIC_INNER_PRODUCT)\n",
      "File \u001b[0;32m~/projects/transformers/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py:2585\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2582\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   2584\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   2586\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   2587\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   2588\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   2589\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   2590\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   2591\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2592\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   2593\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   2594\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   2595\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   2596\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   2597\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   2598\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2599\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   2600\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   2601\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   2602\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   2603\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   2604\u001b[0m     )\n\u001b[1;32m   2605\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2607\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/projects/transformers/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py:585\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    586\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    587\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    588\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/transformers/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py:552\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    546\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    547\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    548\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    549\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    550\u001b[0m }\n\u001b[1;32m    551\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    553\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    554\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/transformers/.env/lib/python3.10/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/transformers/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py:2982\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2978\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2979\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   2980\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   2981\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2982\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   2983\u001b[0m         batch,\n\u001b[1;32m   2984\u001b[0m         indices,\n\u001b[1;32m   2985\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2986\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   2987\u001b[0m     )\n\u001b[1;32m   2988\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   2989\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   2990\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2991\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/projects/transformers/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py:2865\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2863\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   2864\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 2865\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   2866\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2867\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   2868\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "File \u001b[0;32m~/projects/transformers/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py:2545\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2541\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m   2543\u001b[0m )\n\u001b[1;32m   2544\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> 2545\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2546\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mreindex\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreindex\u001b[39m(examples):\n\u001b[0;32m----> 4\u001b[0m     tokenized \u001b[39m=\u001b[39m tokenizer(examples[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m)\n\u001b[1;32m      5\u001b[0m     hidden_states \u001b[39m=\u001b[39m atlas\u001b[39m.\u001b[39mquery_passage_encoder\u001b[39m.\u001b[39membed_passages(input_ids\u001b[39m=\u001b[39mtokenized[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m], attention_mask\u001b[39m=\u001b[39mtokenized[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m     examples[\u001b[39m'\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# reencode the dataset\n",
    "\n",
    "def reindex(examples):\n",
    "    tokenized = tokenizer(examples['text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    hidden_states = atlas.query_passage_encoder.embed_passages(input_ids=tokenized[\"input_ids\"], attention_mask=tokenized[\"attention_mask\"])\n",
    "    examples['embeddings'] = hidden_states.cpu().detach().numpy()\n",
    "    return examples\n",
    "\n",
    "atlas.index = atlas.index.map(reindex, batched=True)\n",
    "atlas.index.add_faiss_index(\"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "inputs = [\"What is my favourite number?\", \"What is the secret word?\"]\n",
    "target = [\"3455\", \"FROG\"]\n",
    "\n",
    "inputs = [f\"question: {question} answer: <extra_id_0>\" for question in inputs]\n",
    "target = [f\"<extra_id_0> {answer}\" for answer in target]\n",
    "print(inputs, target)\n",
    "\n",
    "self = atlas\n",
    "queries = inputs\n",
    "topk = config.n_context\n",
    "\n",
    "\n",
    "\n",
    "bsz = len(queries)\n",
    "\n",
    "\n",
    "queries_tokens = self.query_encoder_tokenizer(queries, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "\n",
    "query_hidden_states = self.query_passage_encoder(input_ids=queries_tokens[\"input_ids\"], attention_mask=queries_tokens[\"attention_mask\"])\n",
    "\n",
    "query_hidden_states = query_hidden_states.cpu().detach().numpy()\n",
    "_, passage_ids = self.index.search_batch(\"embeddings\", query_hidden_states, topk)\n",
    "\n",
    "\n",
    "docs = [self.index[[i for i in indices if i >= 0]] for indices in passage_ids]\n",
    "\n",
    "\n",
    "\n",
    "passages = [[f'{queries[i]} context: {passage}' for passage in doc[\"text\"]] for i, doc in enumerate(docs)]\n",
    "\n",
    "def encode_passages(batch, tokenizer, max_length):\n",
    "    bsz = len(batch)\n",
    "    n = max([len(example) for example in batch])\n",
    "    batch = [example + [\"\"] * (n - len(example)) for example in batch]\n",
    "    batch = reduce(lambda a, b: a + b, batch)\n",
    "    tokens = tokenizer(\n",
    "        batch,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    tokens = {k: v.view(bsz, n, -1) for k, v in tokens.items()}\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "reader_tokens = encode_passages(passages, self.generator_tokenizer, 512)\n",
    "labels = self.generator_tokenizer(target, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)['input_ids']\n",
    "labels[labels == self.generator_tokenizer.pad_token_id] = -100\n",
    "\n",
    "reader_ids = reader_tokens[\"input_ids\"]  # FIXME\n",
    "reader_mask = reader_tokens[\"attention_mask\"].bool()\n",
    "\n",
    "n_context_training = min(topk, reader_ids.size(1))\n",
    "cfg = self.generator.encoder.config\n",
    "cfg.bsz = reader_ids.size(0)\n",
    "cfg.n_context = n_context_training\n",
    "\n",
    "reader_ids_training = reader_ids[:, :n_context_training].contiguous()\n",
    "reader_mask_training = reader_mask[:, :n_context_training].contiguous()\n",
    "\n",
    "reader_ids_training = reader_ids_training.view(reader_ids.size(0), -1)\n",
    "reader_mask_training = reader_mask_training.view(reader_mask.size(0), -1)\n",
    "\n",
    "\n",
    "\n",
    "reader_output = self.generator(\n",
    "            input_ids=reader_ids_training,\n",
    "            attention_mask=reader_mask_training,\n",
    "            decoder_input_ids=None,\n",
    "            labels=labels,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "reader_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_output_for_loss = self.generator(\n",
    "    input_ids=reader_ids.view(reader_ids.size(0), -1),\n",
    "    attention_mask=reader_mask.view(reader_mask.size(0), -1),\n",
    "    decoder_input_ids=None,\n",
    "    labels=labels,\n",
    "    use_cache=False,\n",
    ")\n",
    "reader_output.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = self.generator.generate(\n",
    "        input_ids=reader_ids_training,\n",
    "        attention_mask=reader_mask_training,\n",
    ")\n",
    "\n",
    "tokenizer.generator.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question: What is my favourite number? answer: <extra_id_0>\n",
    "# question: What is my favourite number? answer: <extra_id_0>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4 (default, Apr  5 2021, 01:49:30) \n[Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
