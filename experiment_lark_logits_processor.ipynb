{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "from lark import UnexpectedInput, Lark, UnexpectedCharacters, UnexpectedToken, UnexpectedEOF, UnexpectedInput\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import numpy as np\n",
    "from transformers import LogitsProcessor, AutoModelForCausalLM, AutoTokenizer, BeamSearchScorer, LogitsProcessorList, MaxLengthCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(1, {'RSQB', 'COMMA'})\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class IntermediateParsingState: \n",
    "    active_terminal_names: List[str]\n",
    "    active_terminal_patterns: List[regex.Regex]\n",
    "    current_terminal_start_index: int\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"({self.current_terminal_start_index}, {self.active_terminal_names})\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "class ParsingStepper():\n",
    "    def __init__(self, parser: Lark, vocab, eos_token):\n",
    "        self.parser: Lark = parser\n",
    "        self.partial_token = \"\"\n",
    "        self.vocab = vocab\n",
    "        self.eos_token = eos_token\n",
    "        self.regex_map = self._create_terminal_regexes()\n",
    "\n",
    "    def _create_terminal_regexes(self):\n",
    "        \"\"\"\n",
    "        Create a map from terminal names to regexes that match the terminal\n",
    "        \"\"\"\n",
    "        terminal_regexes = {}\n",
    "        for terminal in self.parser.terminals:\n",
    "            if terminal.pattern:\n",
    "                terminal_regexes[terminal.name] = regex.compile(terminal.pattern.to_regexp())\n",
    "        terminal_regexes['$END'] = regex.compile(self.eos_token)\n",
    "        return terminal_regexes\n",
    "\n",
    "    def get_parsing_state(self, current_generation: str): \n",
    "\n",
    "        # Get the next parser tokens that would be valid to add to the input string according to the CFG\n",
    "        next_parser_tokens, token_start_index = self._get_next_parser_tokens(current_generation)\n",
    "        # Get the regexes for the next parser tokens\n",
    "        next_patterns = [self.regex_map[terminal] for terminal in next_parser_tokens]\n",
    "        \n",
    "        return IntermediateParsingState(next_parser_tokens, next_patterns, token_start_index)\n",
    "    \n",
    "    def _get_next_parser_tokens(self, input_str):\n",
    "        \"\"\"\n",
    "        Get the next tokens that would be valid to add to the input string\n",
    "        :return: A list of tokens that would be valid to add to the input string, and the position in the input string where the next token would start\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try parsing until error or end of input\n",
    "            self.parser.parse(input_str)\n",
    "        except UnexpectedInput as e:\n",
    "            interactive = self.parser.parse_interactive(input_str)\n",
    "            try: \n",
    "                # Get the set of tokens that would be valid next\n",
    "                interactive.exhaust_lexer()\n",
    "            except UnexpectedInput as ee: \n",
    "                # Now, this exception means that we have characters that do not match any of the terminals (yet). \n",
    "                # This means that we have a partial token.\n",
    "                # Return the set of tokens that would be valid before that partial token \n",
    "                print(\"Second catch\")\n",
    "                return interactive.accepts(), ee.pos_in_stream\n",
    "            # Return the token \n",
    "            return interactive.accepts(), e.pos_in_stream\n",
    " \n",
    "        # If we get here, the input is complete\n",
    "        return [], len(input_str)\n",
    "\n",
    "\n",
    "with open(\"cfg_json.lark\", \"r\") as f:\n",
    "    cfg_json = f.read()\n",
    "\n",
    "json_parser = Lark(\n",
    "    cfg_json, \n",
    "    parser='lalr',\n",
    "    # Using the basic lexer isn't required, and isn't usually recommended.\n",
    "    # But, it's good enough for JSON, and it's slightly faster.\n",
    "    lexer='basic',\n",
    "    # Disabling propagate_positions and placeholders slightly improves speed\n",
    "    propagate_positions=False,\n",
    "    maybe_placeholders=False,\n",
    "    regex=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\") \n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "state = ParsingStepper(json_parser, vocab, tokenizer.eos_token)\n",
    "str(state.get_parsing_state('[null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'' -> (0, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "'{' -> (0, {'RBRACE', 'ESCAPED_STRING'})\n",
      "Second catch\n",
      "'{\"' -> (1, {'RBRACE', 'ESCAPED_STRING'})\n",
      "Second catch\n",
      "'{\"a' -> (1, {'RBRACE', 'ESCAPED_STRING'})\n",
      "'{\"a\"' -> (1, {'COLON'})\n",
      "'{\"a\":' -> (4, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "'{\"a\": ' -> (4, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "'{\"a\": [' -> (6, {'RSQB', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL', 'ESCAPED_STRING'})\n",
      "Second catch\n",
      "'{\"a\": [\"' -> (7, {'RSQB', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL', 'ESCAPED_STRING'})\n",
      "Second catch\n",
      "'{\"a\": [\"1' -> (7, {'RSQB', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL', 'ESCAPED_STRING'})\n",
      "'{\"a\": [\"1\"' -> (7, {'RSQB', 'COMMA'})\n",
      "'{\"a\": [\"1\",' -> (10, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "'{\"a\": [\"1\", ' -> (10, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"' -> (12, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b' -> (12, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "'{\"a\": [\"1\", \"b\"' -> (12, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\":' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": ' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\",' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", ' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\",' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", ' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3\"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3\"]' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3\"]]' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3\"]]}' -> (15, {'RSQB', 'COMMA'})\n"
     ]
    }
   ],
   "source": [
    "s = '{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3\"]]}'\n",
    "for i in range(len(s)+1):\n",
    "    cfg_state = state.get_parsing_state(s[:i])\n",
    "    print(f\"'{s[:i]}' -> {cfg_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, {'ESCAPED_STRING'})\n",
      "{\"num_values\": _\"4\", \"values\": [\"1\", \"2\",\">\",\">\"],\n"
     ]
    }
   ],
   "source": [
    "s = '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",\">\",\">\"],'\n",
    "print(state.get_parsing_state(s))\n",
    "print(s[:15] + \"_\" + s[15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 15:28:11.464215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-01 15:28:13.605689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:58<00:00, 29.22s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",']\n",
      "Parsing states: [(39, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (39, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], ['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '[']]\n",
      "Argmax: tensor([4304, 4304], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",\">']\n",
      "Second catch\n",
      "Parsing states: [(40, {'RSQB', 'COMMA'}), (40, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})]\n",
      "Valid tokens: [[',', ']'], ['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '[']]\n",
      "Argmax: tensor([29892,   613], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",\">\",']\n",
      "Parsing states: [(44, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (43, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], ['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '[']]\n",
      "Argmax: tensor([4304, 1013], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",\">\",\">']\n",
      "Second catch\n",
      "Parsing states: [(45, {'RSQB', 'COMMA'}), (44, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})]\n",
      "Valid tokens: [[',', ']'], ['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '[']]\n",
      "Argmax: tensor([29892,  3108], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null]']\n",
      "Parsing states: [(49, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (49, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,{']\n",
      "Parsing states: [(50, {'RSQB', 'COMMA'}), (50, {'RBRACE', 'ESCAPED_STRING'})]\n",
      "Valid tokens: [[',', ']'], ['\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', '\"]', '\"><', '\",\"', '\"/>', '\":\"', '\"),', '\"></', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', '\"', '}']]\n",
      "Argmax: tensor([29892, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null]']\n",
      "Parsing states: [(54, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (54, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,false']\n",
      "Parsing states: [(55, {'RSQB', 'COMMA'}), (55, {'RSQB', 'COMMA'})]\n",
      "Valid tokens: [[',', ']'], [',', ']']]\n",
      "Argmax: tensor([29892, 29892], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null]']\n",
      "Parsing states: [(59, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (59, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,false']\n",
      "Parsing states: [(60, {'RSQB', 'COMMA'}), (60, {'RSQB', 'COMMA'})]\n",
      "Valid tokens: [[',', ']'], [',', ']']]\n",
      "Argmax: tensor([29892, 29892], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null]']\n",
      "Parsing states: [(64, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (64, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,false']\n",
      "Parsing states: [(65, {'RSQB', 'COMMA'}), (65, {'RSQB', 'COMMA'})]\n",
      "Valid tokens: [[',', ']'], [',', ']']]\n",
      "Argmax: tensor([29892, 29892], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null]']\n",
      "Parsing states: [(69, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (69, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,false']\n",
      "Parsing states: [(70, {'RSQB', 'COMMA'}), (70, {'RSQB', 'COMMA'})]\n",
      "Valid tokens: [[',', ']'], [',', ']']]\n",
      "Argmax: tensor([29892, 29892], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null]']\n",
      "Parsing states: [(74, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (74, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null,false']\n",
      "Parsing states: [(75, {'RSQB', 'COMMA'}), (75, {'RSQB', 'COMMA'})]\n",
      "Valid tokens: [[',', ']'], [',', ']']]\n",
      "Argmax: tensor([29892, 29892], device='cuda:0')\n",
      "--------\n",
      "{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null,null,\n"
     ]
    }
   ],
   "source": [
    "class LogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.all_tokens = self.tokenizer.convert_ids_to_tokens(range(self.tokenizer.vocab_size))\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "\n",
    "        # input_ids: B * num_beams x T\n",
    "        # scores: B * num_beams x V\n",
    "\n",
    "        # Decode sequences\n",
    "        decoded_sequences = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Decoded sequences: {decoded_sequences}\")\n",
    "\n",
    "        # Get parsing states per sequence \n",
    "        parsing_states = [state.get_parsing_state(seq) for seq in decoded_sequences]\n",
    "        print(f\"Parsing states: {parsing_states}\")\n",
    "\n",
    "        valid_tokens = [self._filter_tokens_by_regex(self.all_tokens, state.active_terminal_patterns) for state in parsing_states]\n",
    "        valid_token_ids = [self.tokenizer.convert_tokens_to_ids(tokens) for tokens in valid_tokens]  # list of lists of token ids\n",
    "        print(f\"Valid tokens: {valid_tokens}\")\n",
    "\n",
    "        # Mask out scores \n",
    "        scores_mask = torch.ones_like(scores) * float('inf') * -1\n",
    "        for sequence_index, valid_token_ids_for_sequence in enumerate(valid_token_ids):\n",
    "            scores_mask[sequence_index, valid_token_ids_for_sequence] = 0\n",
    "\n",
    "        scores = scores + scores_mask\n",
    "\n",
    "        print(f\"Argmax: {scores.argmax(dim=-1)}\")\n",
    "\n",
    "        print(\"-\" * 8)\n",
    "        return scores\n",
    "        \n",
    "\n",
    "    def _filter_tokens_by_regex(self, tokens, regexes):\n",
    "        \"\"\"\n",
    "        Filter tokens by regexes\n",
    "        \"\"\"\n",
    "        return [\n",
    "            token \n",
    "            for token in tokens \n",
    "            if any(regex.fullmatch(token, partial=True) for regex in regexes)\n",
    "        ]\n",
    "\n",
    "\n",
    "num_beams = 2\n",
    "input_prompt = '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",'\n",
    "max_length = 35\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    input_prompt, \n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "input_ids = torch.stack([input_ids] * num_beams, dim=0).reshape(num_beams, -1).to(model.device)\n",
    "bos_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long) * model.config.bos_token_id\n",
    "input_ids = torch.cat([bos_ids, input_ids], dim=-1)\n",
    "\n",
    "final_sentence = model.beam_search(\n",
    "    input_ids, \n",
    "    beam_scorer=BeamSearchScorer(\n",
    "        batch_size=1,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        device=\"cuda\",\n",
    "        length_penalty=1.0,\n",
    "        do_early_stopping=True,\n",
    "    ),\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        LogitsProcessor(tokenizer)\n",
    "    ]),\n",
    "    stopping_criteria = StoppingCriteriaList([\n",
    "        MaxLengthCriteria(max_length=max_length)\n",
    "    ]),\n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    ")\n",
    "\n",
    "final_sentence_str = tokenizer.batch_decode(final_sentence, skip_special_tokens=True)[0]\n",
    "print(final_sentence_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
