{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79e6e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/dnn_forecasting/conda_envs/envs/fm/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PatchTSMixerConfig, PatchTSMixerForPretraining, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d90416",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eff085",
   "metadata": {},
   "source": [
    "Generate and prepare dummy data to test the pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c48f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "class Dataset_ETT_minute(Dataset):\n",
    "    def __init__(self, root_path='/dccstor/dnn_forecasting/FM/data/ETDataset/ETT-small/', split='train', size=None,\n",
    "                 features='M', data_path='ETTm1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='t',\n",
    "                 use_time_features=False\n",
    "                 ):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        # info\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert split in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[split]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.use_time_features = use_time_features\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n",
    "        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
    "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "#         if self.use_time_features: return _torch(seq_x, seq_y, seq_x_mark, seq_y_mark)\n",
    "#         else: return _torch(seq_x, seq_y)\n",
    "        return {\"context_values\": torch.Tensor(seq_x), \"target_values\": torch.Tensor(seq_y)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f209cf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>5.827</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.462</td>\n",
       "      <td>4.203</td>\n",
       "      <td>1.340</td>\n",
       "      <td>30.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>5.760</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.264</td>\n",
       "      <td>1.401</td>\n",
       "      <td>30.459999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 00:30:00</td>\n",
       "      <td>5.760</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.391</td>\n",
       "      <td>4.234</td>\n",
       "      <td>1.310</td>\n",
       "      <td>30.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 00:45:00</td>\n",
       "      <td>5.760</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.234</td>\n",
       "      <td>1.310</td>\n",
       "      <td>27.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>5.693</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.142</td>\n",
       "      <td>1.371</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69675</th>\n",
       "      <td>2018-06-26 18:45:00</td>\n",
       "      <td>9.310</td>\n",
       "      <td>3.550</td>\n",
       "      <td>5.437</td>\n",
       "      <td>1.670</td>\n",
       "      <td>3.868</td>\n",
       "      <td>1.462</td>\n",
       "      <td>9.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69676</th>\n",
       "      <td>2018-06-26 19:00:00</td>\n",
       "      <td>10.114</td>\n",
       "      <td>3.550</td>\n",
       "      <td>6.183</td>\n",
       "      <td>1.564</td>\n",
       "      <td>3.716</td>\n",
       "      <td>1.462</td>\n",
       "      <td>9.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69677</th>\n",
       "      <td>2018-06-26 19:15:00</td>\n",
       "      <td>10.784</td>\n",
       "      <td>3.349</td>\n",
       "      <td>7.000</td>\n",
       "      <td>1.635</td>\n",
       "      <td>3.746</td>\n",
       "      <td>1.432</td>\n",
       "      <td>9.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69678</th>\n",
       "      <td>2018-06-26 19:30:00</td>\n",
       "      <td>11.655</td>\n",
       "      <td>3.617</td>\n",
       "      <td>7.533</td>\n",
       "      <td>1.706</td>\n",
       "      <td>4.173</td>\n",
       "      <td>1.523</td>\n",
       "      <td>9.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69679</th>\n",
       "      <td>2018-06-26 19:45:00</td>\n",
       "      <td>12.994</td>\n",
       "      <td>3.818</td>\n",
       "      <td>8.244</td>\n",
       "      <td>1.777</td>\n",
       "      <td>4.721</td>\n",
       "      <td>1.523</td>\n",
       "      <td>9.778000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69680 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date    HUFL   HULL   MUFL   MULL   LUFL   LULL  \\\n",
       "0      2016-07-01 00:00:00   5.827  2.009  1.599  0.462  4.203  1.340   \n",
       "1      2016-07-01 00:15:00   5.760  2.076  1.492  0.426  4.264  1.401   \n",
       "2      2016-07-01 00:30:00   5.760  1.942  1.492  0.391  4.234  1.310   \n",
       "3      2016-07-01 00:45:00   5.760  1.942  1.492  0.426  4.234  1.310   \n",
       "4      2016-07-01 01:00:00   5.693  2.076  1.492  0.426  4.142  1.371   \n",
       "...                    ...     ...    ...    ...    ...    ...    ...   \n",
       "69675  2018-06-26 18:45:00   9.310  3.550  5.437  1.670  3.868  1.462   \n",
       "69676  2018-06-26 19:00:00  10.114  3.550  6.183  1.564  3.716  1.462   \n",
       "69677  2018-06-26 19:15:00  10.784  3.349  7.000  1.635  3.746  1.432   \n",
       "69678  2018-06-26 19:30:00  11.655  3.617  7.533  1.706  4.173  1.523   \n",
       "69679  2018-06-26 19:45:00  12.994  3.818  8.244  1.777  4.721  1.523   \n",
       "\n",
       "              OT  \n",
       "0      30.531000  \n",
       "1      30.459999  \n",
       "2      30.038000  \n",
       "3      27.013000  \n",
       "4      27.787001  \n",
       "...          ...  \n",
       "69675   9.567000  \n",
       "69676   9.567000  \n",
       "69677   9.426000  \n",
       "69678   9.426000  \n",
       "69679   9.778000  \n",
       "\n",
       "[69680 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx=pd.read_csv(\"/dccstor/dnn_forecasting/FM/data/ETDataset/ETT-small/ETTm1.csv\")\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964242e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512\n",
    "FORECAST_LEN = 96\n",
    "SIZE = [SEQ_LEN, 0, FORECAST_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee48108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_946920/1575213415.py:68: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data_stamp = df_stamp.drop(['date'], 1).values\n",
      "/tmp/ipykernel_946920/1575213415.py:68: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data_stamp = df_stamp.drop(['date'], 1).values\n",
      "/tmp/ipykernel_946920/1575213415.py:68: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data_stamp = df_stamp.drop(['date'], 1).values\n"
     ]
    }
   ],
   "source": [
    "dset_train = Dataset_ETT_minute(split=\"train\", size=SIZE)\n",
    "dset_val = Dataset_ETT_minute(split=\"val\", size=SIZE)\n",
    "dset_test = Dataset_ETT_minute(split=\"test\", size=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "842cfbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 7]), torch.Size([96, 7]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd=dset_val.__getitem__(0)\n",
    "dd[\"context_values\"].shape, dd[\"target_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f36fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 7\n",
    "seq_len = SEQ_LEN\n",
    "patch_len = 16\n",
    "stride = patch_len//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ab7c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_patches should be (no need to specify)\n",
    "num_patches = seq_len//patch_len\n",
    "num_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e682ba8",
   "metadata": {},
   "source": [
    "### Pretrain the model with HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ed8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PatchTSMixerConfig(\n",
    "    in_channels=n_features,\n",
    "    seq_len=seq_len,\n",
    "    patch_len=patch_len,\n",
    "    stride=stride,\n",
    "    num_features=32,\n",
    "    num_layers=3,\n",
    "    dropout=0.7,\n",
    "    mode=\"common_channel\",\n",
    "    revin=True,\n",
    "    expansion_factor=2,\n",
    "    head_dropout=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22ca7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PatchTSMixerForPretraining(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73478450",
   "metadata": {},
   "source": [
    "Define the training arguments and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e193a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./checkpoint',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=256,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./logs',  # Make sure to specify a logging directory\n",
    "        log_level=\"info\",  # Explicitly set the logging level\n",
    "        \n",
    "#         load_best_model_at_end=True\n",
    "#         no_cuda=True,\n",
    "#         use_mps_device = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1472471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "# metric = evaluate.load(\"mse\")\n",
    "# def compute_metrics(eval_pred):\n",
    "#     print(\"here\")\n",
    "#     logits, labels = eval_pred\n",
    "#     loss = metric(logits, labels)\n",
    "#     print(\"val mse =\", loss)\n",
    "#     return {\"mse\": loss}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "#     compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867bced2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 33,953\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13,300\n",
      "  Number of trainable parameters = 77,491\n",
      "The following columns in the training set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13300' max='13300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13300/13300 08:56, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.628800</td>\n",
       "      <td>0.390499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0.341388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.451100</td>\n",
       "      <td>0.310980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.435800</td>\n",
       "      <td>0.299069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.427100</td>\n",
       "      <td>0.286348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.274971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.266568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.409200</td>\n",
       "      <td>0.259803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.258976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>0.261824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>0.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.399900</td>\n",
       "      <td>0.252566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.398400</td>\n",
       "      <td>0.247894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.396500</td>\n",
       "      <td>0.248120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.257731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.258067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.392900</td>\n",
       "      <td>0.247686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.392100</td>\n",
       "      <td>0.253074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.391100</td>\n",
       "      <td>0.252482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.257287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.388700</td>\n",
       "      <td>0.253626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.387800</td>\n",
       "      <td>0.244335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.387100</td>\n",
       "      <td>0.240185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.239002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>0.239007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.240652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.240154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.242176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>0.241212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.241636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.381900</td>\n",
       "      <td>0.243011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.239975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.239776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.380100</td>\n",
       "      <td>0.241184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.241787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.379500</td>\n",
       "      <td>0.242711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>0.241159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.243213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>0.244165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.378200</td>\n",
       "      <td>0.245473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.377600</td>\n",
       "      <td>0.245832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.245618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>0.247762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.250329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.376600</td>\n",
       "      <td>0.250967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.252561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.376300</td>\n",
       "      <td>0.253445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>0.250177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>0.253977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>0.250997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.375200</td>\n",
       "      <td>0.258874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.375600</td>\n",
       "      <td>0.251817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.375200</td>\n",
       "      <td>0.255778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.375200</td>\n",
       "      <td>0.256549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.261033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.255649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.374900</td>\n",
       "      <td>0.256015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>0.259593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.374400</td>\n",
       "      <td>0.254624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.374300</td>\n",
       "      <td>0.256682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.374300</td>\n",
       "      <td>0.257488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.374200</td>\n",
       "      <td>0.254001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.258451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.259745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.259835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.373700</td>\n",
       "      <td>0.260153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.259351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.264890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.258480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.373400</td>\n",
       "      <td>0.273286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.373200</td>\n",
       "      <td>0.261668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.373200</td>\n",
       "      <td>0.263987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.268571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.259244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.264460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.262892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.372700</td>\n",
       "      <td>0.267599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.264471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.372300</td>\n",
       "      <td>0.270517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.267537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.270394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.269204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.269536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.371900</td>\n",
       "      <td>0.269909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.270543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.269151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.371900</td>\n",
       "      <td>0.273398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.275403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>0.275125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>0.275164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.275710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.275548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>0.276980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.274010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.275533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.278084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>0.276166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.276448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>0.277735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>0.277136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-133\n",
      "Configuration saved in ./checkpoint/checkpoint-133/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-133/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-266\n",
      "Configuration saved in ./checkpoint/checkpoint-266/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-266/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-24948] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-399\n",
      "Configuration saved in ./checkpoint/checkpoint-399/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-399/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-25200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-532\n",
      "Configuration saved in ./checkpoint/checkpoint-532/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-532/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-133] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-665\n",
      "Configuration saved in ./checkpoint/checkpoint-665/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-665/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-266] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-798\n",
      "Configuration saved in ./checkpoint/checkpoint-798/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-798/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-399] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-931\n",
      "Configuration saved in ./checkpoint/checkpoint-931/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-931/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-532] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1064\n",
      "Configuration saved in ./checkpoint/checkpoint-1064/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1064/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-665] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1197\n",
      "Configuration saved in ./checkpoint/checkpoint-1197/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1197/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-798] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1330\n",
      "Configuration saved in ./checkpoint/checkpoint-1330/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1330/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-931] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1463\n",
      "Configuration saved in ./checkpoint/checkpoint-1463/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1463/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1064] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1596\n",
      "Configuration saved in ./checkpoint/checkpoint-1596/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1596/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1197] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1729\n",
      "Configuration saved in ./checkpoint/checkpoint-1729/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1729/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1330] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1862\n",
      "Configuration saved in ./checkpoint/checkpoint-1862/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1862/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1463] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1995\n",
      "Configuration saved in ./checkpoint/checkpoint-1995/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1995/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1596] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2128\n",
      "Configuration saved in ./checkpoint/checkpoint-2128/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1729] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2261\n",
      "Configuration saved in ./checkpoint/checkpoint-2261/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2261/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1862] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2394\n",
      "Configuration saved in ./checkpoint/checkpoint-2394/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2394/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1995] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2527\n",
      "Configuration saved in ./checkpoint/checkpoint-2527/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2527/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2660\n",
      "Configuration saved in ./checkpoint/checkpoint-2660/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2660/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2261] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2793\n",
      "Configuration saved in ./checkpoint/checkpoint-2793/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2793/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2394] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2926\n",
      "Configuration saved in ./checkpoint/checkpoint-2926/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2926/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2527] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3059\n",
      "Configuration saved in ./checkpoint/checkpoint-3059/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3059/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2660] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3192\n",
      "Configuration saved in ./checkpoint/checkpoint-3192/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2793] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3325\n",
      "Configuration saved in ./checkpoint/checkpoint-3325/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3325/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2926] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3458\n",
      "Configuration saved in ./checkpoint/checkpoint-3458/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3458/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3059] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3591\n",
      "Configuration saved in ./checkpoint/checkpoint-3591/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3591/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3724\n",
      "Configuration saved in ./checkpoint/checkpoint-3724/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3724/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3325] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3857\n",
      "Configuration saved in ./checkpoint/checkpoint-3857/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3857/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3458] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3990\n",
      "Configuration saved in ./checkpoint/checkpoint-3990/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3990/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3591] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4123\n",
      "Configuration saved in ./checkpoint/checkpoint-4123/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4123/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3724] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4256\n",
      "Configuration saved in ./checkpoint/checkpoint-4256/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4256/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3857] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4389\n",
      "Configuration saved in ./checkpoint/checkpoint-4389/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4389/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3990] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4522\n",
      "Configuration saved in ./checkpoint/checkpoint-4522/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4522/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4123] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4655\n",
      "Configuration saved in ./checkpoint/checkpoint-4655/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4655/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4256] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4788\n",
      "Configuration saved in ./checkpoint/checkpoint-4788/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4788/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4389] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4921\n",
      "Configuration saved in ./checkpoint/checkpoint-4921/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4921/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4522] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5054\n",
      "Configuration saved in ./checkpoint/checkpoint-5054/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5054/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4655] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5187\n",
      "Configuration saved in ./checkpoint/checkpoint-5187/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5187/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4788] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5320\n",
      "Configuration saved in ./checkpoint/checkpoint-5320/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5320/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4921] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5453\n",
      "Configuration saved in ./checkpoint/checkpoint-5453/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5453/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5054] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5586\n",
      "Configuration saved in ./checkpoint/checkpoint-5586/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5586/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5187] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5719\n",
      "Configuration saved in ./checkpoint/checkpoint-5719/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5719/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5852\n",
      "Configuration saved in ./checkpoint/checkpoint-5852/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5852/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5453] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5985\n",
      "Configuration saved in ./checkpoint/checkpoint-5985/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5985/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5586] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6118\n",
      "Configuration saved in ./checkpoint/checkpoint-6118/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6118/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5719] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6251\n",
      "Configuration saved in ./checkpoint/checkpoint-6251/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6251/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5852] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6384\n",
      "Configuration saved in ./checkpoint/checkpoint-6384/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6384/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5985] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6517\n",
      "Configuration saved in ./checkpoint/checkpoint-6517/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6517/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6118] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6650\n",
      "Configuration saved in ./checkpoint/checkpoint-6650/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6650/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6251] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6783\n",
      "Configuration saved in ./checkpoint/checkpoint-6783/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6783/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6384] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6916\n",
      "Configuration saved in ./checkpoint/checkpoint-6916/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6916/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6517] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7049\n",
      "Configuration saved in ./checkpoint/checkpoint-7049/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7049/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6650] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7182\n",
      "Configuration saved in ./checkpoint/checkpoint-7182/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7182/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6783] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7315\n",
      "Configuration saved in ./checkpoint/checkpoint-7315/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7315/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6916] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7448\n",
      "Configuration saved in ./checkpoint/checkpoint-7448/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7448/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7049] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7581\n",
      "Configuration saved in ./checkpoint/checkpoint-7581/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7581/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7182] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7714\n",
      "Configuration saved in ./checkpoint/checkpoint-7714/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7714/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7315] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7847\n",
      "Configuration saved in ./checkpoint/checkpoint-7847/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7847/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7448] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7980\n",
      "Configuration saved in ./checkpoint/checkpoint-7980/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7980/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7581] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8113\n",
      "Configuration saved in ./checkpoint/checkpoint-8113/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8113/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7714] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8246\n",
      "Configuration saved in ./checkpoint/checkpoint-8246/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8246/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7847] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8379\n",
      "Configuration saved in ./checkpoint/checkpoint-8379/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8379/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7980] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8512\n",
      "Configuration saved in ./checkpoint/checkpoint-8512/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8113] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8645\n",
      "Configuration saved in ./checkpoint/checkpoint-8645/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8645/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8246] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8778\n",
      "Configuration saved in ./checkpoint/checkpoint-8778/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8778/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8379] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8911\n",
      "Configuration saved in ./checkpoint/checkpoint-8911/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8911/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9044\n",
      "Configuration saved in ./checkpoint/checkpoint-9044/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9044/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8645] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9177\n",
      "Configuration saved in ./checkpoint/checkpoint-9177/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9177/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8778] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9310\n",
      "Configuration saved in ./checkpoint/checkpoint-9310/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9310/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8911] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9443\n",
      "Configuration saved in ./checkpoint/checkpoint-9443/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9443/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9044] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9576\n",
      "Configuration saved in ./checkpoint/checkpoint-9576/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9177] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9709\n",
      "Configuration saved in ./checkpoint/checkpoint-9709/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9709/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9310] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9842\n",
      "Configuration saved in ./checkpoint/checkpoint-9842/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9842/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9443] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9975\n",
      "Configuration saved in ./checkpoint/checkpoint-9975/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9975/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10108\n",
      "Configuration saved in ./checkpoint/checkpoint-10108/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10108/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9709] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10241\n",
      "Configuration saved in ./checkpoint/checkpoint-10241/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10241/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9842] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10374\n",
      "Configuration saved in ./checkpoint/checkpoint-10374/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10374/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9975] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10507\n",
      "Configuration saved in ./checkpoint/checkpoint-10507/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10507/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10108] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10640\n",
      "Configuration saved in ./checkpoint/checkpoint-10640/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10640/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10241] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10773\n",
      "Configuration saved in ./checkpoint/checkpoint-10773/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10773/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10374] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10906\n",
      "Configuration saved in ./checkpoint/checkpoint-10906/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10906/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10507] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11039\n",
      "Configuration saved in ./checkpoint/checkpoint-11039/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11039/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10640] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11172\n",
      "Configuration saved in ./checkpoint/checkpoint-11172/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11172/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10773] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11305\n",
      "Configuration saved in ./checkpoint/checkpoint-11305/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11305/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10906] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11438\n",
      "Configuration saved in ./checkpoint/checkpoint-11438/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11438/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11039] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11571\n",
      "Configuration saved in ./checkpoint/checkpoint-11571/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11571/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11172] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11704\n",
      "Configuration saved in ./checkpoint/checkpoint-11704/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11704/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11305] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11837\n",
      "Configuration saved in ./checkpoint/checkpoint-11837/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11837/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11438] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11970\n",
      "Configuration saved in ./checkpoint/checkpoint-11970/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11970/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11571] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12103\n",
      "Configuration saved in ./checkpoint/checkpoint-12103/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12103/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11704] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12236\n",
      "Configuration saved in ./checkpoint/checkpoint-12236/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12236/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11837] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12369\n",
      "Configuration saved in ./checkpoint/checkpoint-12369/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12369/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11970] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12502\n",
      "Configuration saved in ./checkpoint/checkpoint-12502/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12502/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12103] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12635\n",
      "Configuration saved in ./checkpoint/checkpoint-12635/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12635/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12236] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12768\n",
      "Configuration saved in ./checkpoint/checkpoint-12768/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12768/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12369] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12901\n",
      "Configuration saved in ./checkpoint/checkpoint-12901/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12901/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12502] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-13034\n",
      "Configuration saved in ./checkpoint/checkpoint-13034/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-13034/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12635] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-13167\n",
      "Configuration saved in ./checkpoint/checkpoint-13167/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-13167/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12768] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-13300\n",
      "Configuration saved in ./checkpoint/checkpoint-13300/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-13300/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12901] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13300, training_loss=0.3848540278843471, metrics={'train_runtime': 536.9971, 'train_samples_per_second': 6322.753, 'train_steps_per_second': 24.767, 'total_flos': 5657814055219200.0, 'train_loss': 0.3848540278843471, 'epoch': 100.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94a4c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.27754929661750793,\n",
       " 'eval_runtime': 0.5696,\n",
       " 'eval_samples_per_second': 20058.763,\n",
       " 'eval_steps_per_second': 21.068,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = trainer.evaluate(dset_val)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f03f42de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3110661804676056,\n",
       " 'eval_runtime': 0.6789,\n",
       " 'eval_samples_per_second': 16829.267,\n",
       " 'eval_steps_per_second': 17.676,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = trainer.evaluate(dset_test)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77ac7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p pytest_data/ettm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "815a9aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to pytest_data/ettm1/patchtsmixer_pretrained_ettm1\n",
      "Configuration saved in pytest_data/ettm1/patchtsmixer_pretrained_ettm1/config.json\n",
      "Model weights saved in pytest_data/ettm1/patchtsmixer_pretrained_ettm1/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('pytest_data/ettm1/patchtsmixer_pretrained_ettm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949d12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00d53ea9",
   "metadata": {},
   "source": [
    "# Use the pretrained model to finetune for a forecasting task\n",
    "## TODO: Loading the backbone weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "549d339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytest_data/ettm1/patchtsmixer_pretrained_ettm1/pytorch_model.bin\n",
      "Some weights of the model checkpoint at pytest_data/ettm1/patchtsmixer_pretrained_ettm1 were not used when initializing PatchTSMixerForForecasting: ['head.head.base_pt_block.1.weight', 'head.head.base_pt_block.1.bias']\n",
      "- This IS expected if you are initializing PatchTSMixerForForecasting from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PatchTSMixerForForecasting from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PatchTSMixerForForecasting were not initialized from the model checkpoint at pytest_data/ettm1/patchtsmixer_pretrained_ettm1 and are newly initialized: ['head.head.base_forecast_block.1.bias', 'head.head.base_forecast_block.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.patchtsmixer.modeling_patchtsmixer import PatchTSMixerForForecasting\n",
    "\n",
    "config.update({\"forecast_len\": FORECAST_LEN})\n",
    "finetune_model = PatchTSMixerForForecasting.from_pretrained(\"pytest_data/ettm1/patchtsmixer_pretrained_ettm1\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cefd63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "finetune_args = TrainingArguments(\n",
    "        output_dir='./checkpoint_ftune',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./logs_ftune',  # Make sure to specify a logging directory\n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6262033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Create the early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.0001,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "\n",
    "finetune_trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=finetune_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8eb599b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 33,953\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 424,500\n",
      "  Number of trainable parameters = 270,595\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55185' max='424500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 55185/424500 12:27 < 1:23:21, 73.84 it/s, Epoch 13/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.325100</td>\n",
       "      <td>0.443362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300500</td>\n",
       "      <td>0.442807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>0.436477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.438686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>0.747019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.443275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.292600</td>\n",
       "      <td>0.545229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.779547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.290400</td>\n",
       "      <td>0.566062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>0.614435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.289700</td>\n",
       "      <td>1.032972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.437045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.857858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-4245\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-4245/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-4245/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-3024] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-8490\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-8490/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-8490/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-4032] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-12735\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-12735/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-12735/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-4284] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-16980\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-16980/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-16980/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-4245] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-21225\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-21225/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-21225/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-8490] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-25470\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-25470/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-25470/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-16980] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-29715\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-29715/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-29715/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-21225] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-33960\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-33960/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-33960/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-25470] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-38205\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-38205/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-38205/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-29715] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-42450\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-42450/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-42450/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-33960] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-46695\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-46695/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-46695/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-38205] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-50940\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-50940/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-50940/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-42450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-55185\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-55185/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-55185/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-46695] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./checkpoint_ftune/checkpoint-12735 (score: 0.43647706508636475).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=55185, training_loss=0.295167497678264, metrics={'train_runtime': 747.3511, 'train_samples_per_second': 4543.112, 'train_steps_per_second': 568.006, 'total_flos': 2568387364408320.0, 'train_loss': 0.295167497678264, 'epoch': 13.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4feb8a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 11425\n",
      "  Batch size = 1024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.33519473671913147,\n",
       " 'eval_runtime': 0.5992,\n",
       " 'eval_samples_per_second': 19067.15,\n",
       " 'eval_steps_per_second': 20.027,\n",
       " 'epoch': 13.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_trainer.evaluate(dset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
