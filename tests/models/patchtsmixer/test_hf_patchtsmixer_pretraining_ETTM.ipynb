{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3d1e0f",
   "metadata": {},
   "source": [
    "# PatchTSMixer workflow examples on ETTM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b84c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/dnn_forecasting/conda_envs/envs/fm/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PatchTSMixerConfig, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d41ca4",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e508a5",
   "metadata": {},
   "source": [
    "Generate and prepare dummy data to test the pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1e5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "class Dataset_ETT_minute(Dataset):\n",
    "    def __init__(self, root_path='/dccstor/dnn_forecasting/FM/data/ETDataset/ETT-small/', split='train', size=None,\n",
    "                 features='M', data_path='ETTm1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='t',\n",
    "                 use_time_features=False\n",
    "                 ):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        # info\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert split in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[split]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.use_time_features = use_time_features\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n",
    "        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
    "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "#         if self.use_time_features: return _torch(seq_x, seq_y, seq_x_mark, seq_y_mark)\n",
    "#         else: return _torch(seq_x, seq_y)\n",
    "        return {\"context_values\": torch.Tensor(seq_x), \"target_values\": torch.Tensor(seq_y)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf94c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORECAST_LEN = 96\n",
    "n_features = 7\n",
    "SEQ_LEN = 512\n",
    "seq_len = SEQ_LEN\n",
    "patch_len = 16\n",
    "stride = patch_len\n",
    "SIZE = [SEQ_LEN, 0, FORECAST_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e803489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1568655/1575213415.py:68: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data_stamp = df_stamp.drop(['date'], 1).values\n",
      "/tmp/ipykernel_1568655/1575213415.py:68: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data_stamp = df_stamp.drop(['date'], 1).values\n",
      "/tmp/ipykernel_1568655/1575213415.py:68: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data_stamp = df_stamp.drop(['date'], 1).values\n"
     ]
    }
   ],
   "source": [
    "dset_train = Dataset_ETT_minute(split=\"train\", size=SIZE)\n",
    "dset_val = Dataset_ETT_minute(split=\"val\", size=SIZE)\n",
    "dset_test = Dataset_ETT_minute(split=\"test\", size=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d236923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 7]), torch.Size([96, 7]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd=dset_val.__getitem__(0)\n",
    "dd[\"context_values\"].shape, dd[\"target_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfc3829d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_patches should be (no need to specify)\n",
    "num_patches = seq_len//patch_len\n",
    "num_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e940a8",
   "metadata": {},
   "source": [
    "## 1. Directly train a `PatchTSMixer` forecasting model, and evaluate the test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca93b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PatchTSMixerForForecasting\n",
    "\n",
    "forecast_config = PatchTSMixerConfig(\n",
    "    in_channels=n_features,\n",
    "    seq_len=SEQ_LEN,\n",
    "    patch_len=patch_len,\n",
    "    stride=stride,\n",
    "    num_features=48,\n",
    "    num_layers=2,\n",
    "    dropout=0.5,\n",
    "    mode=\"common_channel\",\n",
    "    revin=True,\n",
    "    expansion_factor=3,\n",
    "    head_dropout=0.7,\n",
    "    forecast_len=FORECAST_LEN\n",
    ")\n",
    "\n",
    "forecast_model = PatchTSMixerForForecasting(forecast_config)\n",
    "\n",
    "forecast_args = TrainingArguments(\n",
    "        output_dir='./dump/ettm1/direct_forecast/checkpoint',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./dump/ettm1/direct_forecast/logs',  # Make sure to specify a logging directory\n",
    "        \n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "    )\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Create the early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.0001,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "\n",
    "forecast_trainer = Trainer(\n",
    "    model=forecast_model,\n",
    "    args=forecast_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61b7fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56286' max='106200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56286/106200 15:45 < 13:58, 59.54 it/s, Epoch 53/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>0.405758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>0.395581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.388832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.389827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.387664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.382318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.381695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.380462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.380906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.380674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>0.380037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.378583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.376096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.379085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.377867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.259100</td>\n",
       "      <td>0.377646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.258300</td>\n",
       "      <td>0.377943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.375733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.256900</td>\n",
       "      <td>0.373310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>0.372340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.373946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.373776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.375281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.375635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.253300</td>\n",
       "      <td>0.377370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>0.371686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.252400</td>\n",
       "      <td>0.374334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.376118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.373721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.376536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.250700</td>\n",
       "      <td>0.375641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.371595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.370481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.373222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.368892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.375492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.372075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.247400</td>\n",
       "      <td>0.372599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>0.372262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>0.371491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.246800</td>\n",
       "      <td>0.371940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.376403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.245500</td>\n",
       "      <td>0.368147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.372481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.245900</td>\n",
       "      <td>0.371568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.244900</td>\n",
       "      <td>0.373158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.370059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0.369040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.243400</td>\n",
       "      <td>0.372665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.369600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.243400</td>\n",
       "      <td>0.369263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.369429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.370079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=56286, training_loss=0.2560363618803063, metrics={'train_runtime': 947.3042, 'train_samples_per_second': 3584.171, 'train_steps_per_second': 112.108, 'total_flos': 7590114057437184.0, 'train_loss': 0.2560363618803063, 'epoch': 53.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0526ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2869480848312378,\n",
       " 'eval_runtime': 0.731,\n",
       " 'eval_samples_per_second': 15628.272,\n",
       " 'eval_steps_per_second': 16.415,\n",
       " 'epoch': 53.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_trainer.evaluate(dset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8ac55",
   "metadata": {},
   "source": [
    "## 2. Pretrain a `PatchTSMixer` model with HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af54e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PatchTSMixerForPretraining\n",
    "\n",
    "pretrain_config = PatchTSMixerConfig(\n",
    "    in_channels=n_features,\n",
    "    seq_len=SEQ_LEN,\n",
    "    patch_len=patch_len,\n",
    "    stride=stride,\n",
    "    num_features=48,\n",
    "    num_layers=2,\n",
    "    dropout=0.5,\n",
    "    mode=\"common_channel\",\n",
    "    revin=True,\n",
    "    expansion_factor=3,\n",
    "    head_dropout=0.7,\n",
    "    forecast_len=FORECAST_LEN\n",
    ")\n",
    "\n",
    "pretrain_model = PatchTSMixerForPretraining(pretrain_config)\n",
    "\n",
    "pretrain_args = TrainingArguments(\n",
    "        output_dir='./dump/ettm1/pretrain/checkpoint',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./dump/ettm1/pretrain/logs',  # Make sure to specify a logging directory\n",
    "        \n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "    )\n",
    "\n",
    "pretrain_trainer = Trainer(\n",
    "    model=pretrain_model,\n",
    "    args=pretrain_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ab436cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='106200' max='106200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [106200/106200 29:37, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.745400</td>\n",
       "      <td>0.423507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.510700</td>\n",
       "      <td>0.356104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.465200</td>\n",
       "      <td>0.339474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.448400</td>\n",
       "      <td>0.333207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.330550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.326977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.323584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.430200</td>\n",
       "      <td>0.321314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.428400</td>\n",
       "      <td>0.317115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.314510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.312271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.421300</td>\n",
       "      <td>0.309786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.304997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.302834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>0.299399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>0.299186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.412600</td>\n",
       "      <td>0.297604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>0.295356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.410600</td>\n",
       "      <td>0.294280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>0.292257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.408200</td>\n",
       "      <td>0.289409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.288673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.405800</td>\n",
       "      <td>0.286595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>0.285239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.404100</td>\n",
       "      <td>0.284648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>0.282145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.401800</td>\n",
       "      <td>0.281802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.278465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.400100</td>\n",
       "      <td>0.278701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.276186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.398200</td>\n",
       "      <td>0.275647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.272886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.396300</td>\n",
       "      <td>0.273048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.273062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.270090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.394400</td>\n",
       "      <td>0.269781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>0.268673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.268573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.392200</td>\n",
       "      <td>0.266052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.265212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.391100</td>\n",
       "      <td>0.264605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.263602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.263408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.261428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.388400</td>\n",
       "      <td>0.262328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.387800</td>\n",
       "      <td>0.259620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.387100</td>\n",
       "      <td>0.260973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.257585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.259621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.385800</td>\n",
       "      <td>0.259932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.257063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.258949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.258216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.256922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.384100</td>\n",
       "      <td>0.256693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.255540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.383700</td>\n",
       "      <td>0.257426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.383100</td>\n",
       "      <td>0.256255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.383100</td>\n",
       "      <td>0.255232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.383000</td>\n",
       "      <td>0.256973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.382800</td>\n",
       "      <td>0.254708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.254793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.382200</td>\n",
       "      <td>0.255409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.255457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.255763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.254729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.381400</td>\n",
       "      <td>0.254363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>0.254308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.254351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.254729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.253622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.255102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.252517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>0.254027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.380400</td>\n",
       "      <td>0.252790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.252772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.253798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.252425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.380100</td>\n",
       "      <td>0.252453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.379800</td>\n",
       "      <td>0.253246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>0.251830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.251759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.251510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>0.251758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.251453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.252320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.252109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.251957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>0.251272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>0.251642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>0.251330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.379200</td>\n",
       "      <td>0.251676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.251267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.250592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.251915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.250831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.251213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.251385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>0.251363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.251915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=106200, training_loss=0.3983063304491636, metrics={'train_runtime': 1777.9815, 'train_samples_per_second': 1909.637, 'train_steps_per_second': 59.731, 'total_flos': 3605066740531200.0, 'train_loss': 0.3983063304491636, 'epoch': 100.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68686adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2903543710708618,\n",
       " 'eval_runtime': 0.5352,\n",
       " 'eval_samples_per_second': 21347.697,\n",
       " 'eval_steps_per_second': 22.422,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_trainer.evaluate(dset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6151dbc",
   "metadata": {},
   "source": [
    "### Save batch and output for pytests (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5afbcb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.data_loader.DataLoaderShard at 0x14c048dd9dc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train dataloader\n",
    "eval_dataloader = pretrain_trainer.get_eval_dataloader()\n",
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99682358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context_values'])\n",
      "torch.Size([1024, 512, 7])\n"
     ]
    }
   ],
   "source": [
    "for X in eval_dataloader:\n",
    "    print(X.keys())\n",
    "    print(X[\"context_values\"].shape)\n",
    "    torch.save(X, \"./dump/ettm1/pretrain/batch.pt\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea8ff5",
   "metadata": {},
   "source": [
    "### Saving and loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e536149",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_trainer.save_model(\"./dump/ettm1/pretrain/patchtsmixer_pretrained_ettm1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc15e991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = PatchTSMixerForPretraining.from_pretrained('./dump/ettm1/pretrain/patchtsmixer_pretrained_ettm1').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a6d14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8842]],\n",
      "\n",
      "        [[-0.3338]],\n",
      "\n",
      "        [[-0.8968]],\n",
      "\n",
      "        [[-0.3681]],\n",
      "\n",
      "        [[ 0.8460]],\n",
      "\n",
      "        [[ 1.0702]],\n",
      "\n",
      "        [[ 1.5765]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    d_out = loaded_model.forward(X['context_values'].reshape(1024,seq_len,n_features).to(\"cuda\"))\n",
    "print(d_out.prediction_logits[0, :7, :1, :1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08524494",
   "metadata": {},
   "source": [
    "Copy the above values in a pytest and use the dumped model to verify (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0f04e",
   "metadata": {},
   "source": [
    "## 3. Use the pretrained model (step 2) to finetune for a forecasting task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c08aaf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PatchTSMixerForForecasting were not initialized from the model checkpoint at ./dump/ettm1/pretrain/patchtsmixer_pretrained_ettm1 and are newly initialized: ['head.head.base_forecast_block.1.weight', 'head.head.base_forecast_block.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "finetune_forecast_model = PatchTSMixerForForecasting.from_pretrained('./dump/ettm1/pretrain/patchtsmixer_pretrained_ettm1')\n",
    "\n",
    "finetune_forecast_args = TrainingArguments(\n",
    "        output_dir='./dump/ettm1/finetune_forecast/checkpoint',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./dump/ettm1/finetune_forecast/logs',  # Make sure to specify a logging directory\n",
    "        \n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "    )\n",
    "\n",
    "finetune_forecast_trainer = Trainer(\n",
    "    model=finetune_forecast_model,\n",
    "    args=finetune_forecast_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1e2514d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56286' max='106200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56286/106200 15:46 < 13:59, 59.45 it/s, Epoch 53/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.326400</td>\n",
       "      <td>0.410178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>0.398290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.393878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.390991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.266300</td>\n",
       "      <td>0.387785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.379523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.378832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.261100</td>\n",
       "      <td>0.377267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>0.378713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.378195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.256700</td>\n",
       "      <td>0.375563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.255400</td>\n",
       "      <td>0.374348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.254300</td>\n",
       "      <td>0.368943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.253500</td>\n",
       "      <td>0.371656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.373897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.370089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.250300</td>\n",
       "      <td>0.372633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.369233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.366511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.248300</td>\n",
       "      <td>0.369182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>0.367190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.246300</td>\n",
       "      <td>0.367431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.369154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.371956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.244400</td>\n",
       "      <td>0.373563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.367020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>0.366983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.370598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.242300</td>\n",
       "      <td>0.365521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.368795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.367023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.240500</td>\n",
       "      <td>0.365055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>0.366165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>0.369737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.364893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.371060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>0.367478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.237700</td>\n",
       "      <td>0.367498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.369143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.369090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.365921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.371453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>0.364545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.367815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.368056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.369271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.366210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.369134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.367437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.364768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.232000</td>\n",
       "      <td>0.366937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.366994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.367430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=56286, training_loss=0.24735123676662907, metrics={'train_runtime': 946.8288, 'train_samples_per_second': 3585.97, 'train_steps_per_second': 112.164, 'total_flos': 7590114057437184.0, 'train_loss': 0.24735123676662907, 'epoch': 53.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_forecast_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4de67e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.28315261006355286,\n",
       " 'eval_runtime': 0.7417,\n",
       " 'eval_samples_per_second': 15403.62,\n",
       " 'eval_steps_per_second': 16.179,\n",
       " 'epoch': 53.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_forecast_trainer.evaluate(dset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b42a94",
   "metadata": {},
   "source": [
    "### Save/Load and dump outputs for pytest (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "241bfac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_forecast_trainer.save_model(\"./dump/ettm1/finetune_forecast/patchtsmixer_finetune_forecast_ettm1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbdb0034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.data_loader.DataLoaderShard at 0x14c049361ee0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train dataloader\n",
    "eval_dataloader = finetune_forecast_trainer.get_eval_dataloader()\n",
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cf32447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = PatchTSMixerForForecasting.from_pretrained('./dump/ettm1/finetune_forecast/patchtsmixer_finetune_forecast_ettm1').to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b729819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context_values', 'target_values'])\n",
      "torch.Size([1024, 512, 7]) torch.Size([1024, 96, 7])\n"
     ]
    }
   ],
   "source": [
    "for X in eval_dataloader:\n",
    "    print(X.keys())\n",
    "    print(X[\"context_values\"].shape, X[\"target_values\"].shape)\n",
    "    torch.save(X, \"./dump/ettm1/finetune_forecast/batch_forecast.pt\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cab1c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5633,  0.0606,  0.6069,  0.8175,  0.0398, -2.1611,  0.4113]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_batch = loaded_model(X[\"context_values\"])\n",
    "print(output_batch.prediction_logits[0, :1, :7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea7580",
   "metadata": {},
   "source": [
    "Copy the above values in a pytest and use the dumped model to verify (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf65f39",
   "metadata": {},
   "source": [
    "## 4. Use pretrained model to finetune for a prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39687ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PatchTSMixerForForecasting were not initialized from the model checkpoint at ./dump/ettm1/pretrain/patchtsmixer_pretrained_ettm1 and are newly initialized: ['head.head.base_forecast_block.1.weight', 'head.head.base_forecast_block.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We can either provide the forecast channel indices during pretraining\n",
    "# Or, we can update the config and pass it again\n",
    "pretrain_config.update({\"forecast_channel_indices\": [3,5]})\n",
    "finetune_prediction_model = PatchTSMixerForForecasting.from_pretrained('./dump/ettm1/pretrain/patchtsmixer_pretrained_ettm1', config=pretrain_config)\n",
    "\n",
    "\n",
    "finetune_prediction_args = TrainingArguments(\n",
    "        output_dir='./dump/ettm1/finetune_prediction/checkpoint',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./dump/ettm1/finetune_prediction/logs',  # Make sure to specify a logging directory\n",
    "        \n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "    )\n",
    "\n",
    "finetune_prediction_trainer = Trainer(\n",
    "    model=finetune_prediction_model,\n",
    "    args=finetune_prediction_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34423cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30798' max='106200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 30798/106200 08:46 < 21:29, 58.47 it/s, Epoch 29/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.372380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.204200</td>\n",
       "      <td>0.358782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>0.355695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>0.348001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.351776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.346249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.343175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>0.343602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.183900</td>\n",
       "      <td>0.345523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.347197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.182100</td>\n",
       "      <td>0.345135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.342813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.344650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.179700</td>\n",
       "      <td>0.341691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.344048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.178700</td>\n",
       "      <td>0.342618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.347811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.344566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.177600</td>\n",
       "      <td>0.341364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.176900</td>\n",
       "      <td>0.344953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>0.343822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.176400</td>\n",
       "      <td>0.343213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.348452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>0.345388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.350452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.342277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.174300</td>\n",
       "      <td>0.346205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>0.344144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.347411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30798, training_loss=0.1831576220206885, metrics={'train_runtime': 526.8925, 'train_samples_per_second': 6444.009, 'train_steps_per_second': 201.559, 'total_flos': 4153081276710912.0, 'train_loss': 0.1831576220206885, 'epoch': 29.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_prediction_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8595c7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12233199924230576,\n",
       " 'eval_runtime': 0.7232,\n",
       " 'eval_samples_per_second': 15798.143,\n",
       " 'eval_steps_per_second': 16.593,\n",
       " 'epoch': 29.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_prediction_trainer.evaluate(dset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36ffd8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.34136372804641724,\n",
       " 'eval_runtime': 0.7282,\n",
       " 'eval_samples_per_second': 15690.168,\n",
       " 'eval_steps_per_second': 16.48,\n",
       " 'epoch': 29.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_prediction_trainer.evaluate(dset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a3f30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_prediction_trainer.save_model(\"./dump/ettm1/finetune_prediction/patchtsmixer_finetune_prediction_ettm1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adc04e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.data_loader.DataLoaderShard at 0x14c049217280>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train dataloader\n",
    "eval_dataloader = finetune_prediction_trainer.get_eval_dataloader()\n",
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14ddd6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = PatchTSMixerForForecasting.from_pretrained('./dump/ettm1/finetune_prediction/patchtsmixer_finetune_prediction_ettm1').to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70bf8d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context_values', 'target_values'])\n",
      "torch.Size([1024, 512, 7]) torch.Size([1024, 96, 7])\n"
     ]
    }
   ],
   "source": [
    "for X in eval_dataloader:\n",
    "    print(X.keys())\n",
    "    print(X[\"context_values\"].shape, X[\"target_values\"].shape)\n",
    "    torch.save(X, \"./dump/ettm1/finetune_prediction/batch_prediction.pt\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0351988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 2])\n",
      "tensor([[ 0.8108, -2.2082]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_batch = loaded_model(X[\"context_values\"])\n",
    "print(output_batch.prediction_logits.shape)\n",
    "print(output_batch.prediction_logits[0, :1, :7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669a40a",
   "metadata": {},
   "source": [
    "Note that, the output has only 2 channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac30d9",
   "metadata": {},
   "source": [
    "## 5. Register model under Auto Classes and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "884a40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoModelForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3aaab6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoConfig.register(\"patchtsmixer\", PatchTSMixerConfig)\n",
    "AutoModelForPreTraining.register(PatchTSMixerConfig, PatchTSMixerForPretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42a7dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_model = AutoModelForPreTraining.from_pretrained('./dump/ettm1/pretrain/patchtsmixer_pretrained_ettm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d23ea7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1527,  0.2205,  0.2780,  ...,  0.3576,  0.3416,  0.3166],\n",
       "          [ 0.2882,  0.3150,  0.3378,  ...,  0.7706,  0.7765,  0.7621],\n",
       "          [ 0.8379,  0.8387,  0.8289,  ...,  0.7195,  0.6866,  0.6433],\n",
       "          ...,\n",
       "          [-1.4859, -1.4634, -1.4249,  ..., -0.9950, -1.0377, -1.0746],\n",
       "          [-0.4655, -0.3501, -0.1981,  ...,  0.3212,  0.3111,  0.3068],\n",
       "          [ 0.2615,  0.2717,  0.2818,  ...,  0.3389,  0.3420,  0.3430]],\n",
       "\n",
       "         [[-0.8495, -0.8907, -0.9494,  ..., -1.0969, -1.0805, -1.0625],\n",
       "          [-1.2213, -1.2126, -1.1874,  ...,  0.4999,  0.5721,  0.6187],\n",
       "          [ 0.7422,  0.7223,  0.7173,  ...,  0.7334,  0.7324,  0.7170],\n",
       "          ...,\n",
       "          [-0.3005, -0.3168, -0.3259,  ..., -0.6412, -0.6928, -0.7043],\n",
       "          [-0.8660, -0.9424, -1.0336,  ..., -1.4231, -1.3920, -1.3620],\n",
       "          [-0.7871, -0.7731, -0.7394,  ...,  0.0594,  0.0977,  0.1063]],\n",
       "\n",
       "         [[ 0.2057,  0.2584,  0.2959,  ...,  0.3389,  0.3248,  0.2995],\n",
       "          [ 0.3061,  0.3338,  0.3591,  ...,  0.8264,  0.8378,  0.8291],\n",
       "          [ 0.9158,  0.9148,  0.9047,  ...,  0.8311,  0.8116,  0.7801],\n",
       "          ...,\n",
       "          [-1.4226, -1.4031, -1.3684,  ..., -0.9574, -0.9993, -1.0348],\n",
       "          [-0.4672, -0.3629, -0.2257,  ...,  0.1978,  0.1820,  0.1775],\n",
       "          [ 0.1412,  0.1504,  0.1617,  ...,  0.2767,  0.2839,  0.2883]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2929, -0.1379,  0.0331,  ...,  0.2898,  0.2900,  0.2831],\n",
       "          [-0.0943, -0.2003, -0.3419,  ..., -0.9269, -1.0520, -1.1410],\n",
       "          [-1.2320, -1.2201, -1.2067,  ..., -1.1315, -1.0527, -0.9861],\n",
       "          ...,\n",
       "          [-1.3621, -1.3443, -1.3247,  ..., -0.8272, -0.7445, -0.6645],\n",
       "          [-0.1477,  0.1107,  0.4237,  ...,  2.0512,  2.1226,  2.1428],\n",
       "          [ 1.0436,  1.0685,  1.0664,  ...,  0.5344,  0.4929,  0.4569]],\n",
       "\n",
       "         [[-0.8689, -0.9106, -0.9702,  ..., -0.7585, -0.6457, -0.5939],\n",
       "          [-0.8426, -0.8549, -0.8526,  ..., -0.5168, -0.5746, -0.6109],\n",
       "          [-0.7072, -0.6929, -0.6809,  ..., -0.4947, -0.3923, -0.2888],\n",
       "          ...,\n",
       "          [-0.6951, -0.6756, -0.6325,  ...,  0.0477,  0.1290,  0.2142],\n",
       "          [ 0.0234,  0.1055,  0.2121,  ...,  0.9475,  1.0997,  1.1591],\n",
       "          [ 0.9190,  0.9506,  0.9662,  ...,  0.7673,  0.7414,  0.7111]],\n",
       "\n",
       "         [[-0.4003, -0.4239, -0.4592,  ..., -0.5705, -0.5928, -0.5936],\n",
       "          [-1.1530, -1.2659, -1.4091,  ..., -1.3623, -1.0896, -0.8505],\n",
       "          [-0.5905, -0.6389, -0.6982,  ..., -0.6198, -0.5103, -0.4080],\n",
       "          ...,\n",
       "          [ 0.8568,  0.8770,  0.8830,  ...,  0.8084,  0.8242,  0.8223],\n",
       "          [ 0.8000,  0.8113,  0.8129,  ...,  0.2285,  0.1725,  0.1097],\n",
       "          [ 0.1115,  0.1008,  0.0871,  ..., -0.1228, -0.1347, -0.1412]]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# auto_model(test_data.to('mps')).prediction_logits\n",
    "auto_model(dset_test.__getitem__(0)['context_values'].reshape(1,seq_len,n_features)).prediction_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8a6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
