{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f169d4e",
   "metadata": {},
   "source": [
    "# PatchTSMixer workflow examples on ETTM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40dde658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/dnn_forecasting/conda_envs/envs/fm/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PatchTSMixerConfig, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad0645",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c692d",
   "metadata": {},
   "source": [
    "Generate and prepare dummy data to test the pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95144884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "class Dataset_ETT_minute(Dataset):\n",
    "    def __init__(self, root_path='/dccstor/dnn_forecasting/FM/data/ETDataset/ETT-small/', split='train', size=None,\n",
    "                 features='M', data_path='ETTm2.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='t',\n",
    "                 use_time_features=False\n",
    "                 ):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        # info\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert split in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[split]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.use_time_features = use_time_features\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n",
    "        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
    "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "#         if self.use_time_features: return _torch(seq_x, seq_y, seq_x_mark, seq_y_mark)\n",
    "#         else: return _torch(seq_x, seq_y)\n",
    "        return {\"context_values\": torch.Tensor(seq_x), \"target_values\": torch.Tensor(seq_y)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb0ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORECAST_LEN = 96\n",
    "n_features = 7\n",
    "SEQ_LEN = 512\n",
    "seq_len = SEQ_LEN\n",
    "patch_len = 16\n",
    "stride = patch_len\n",
    "SIZE = [SEQ_LEN, 0, FORECAST_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f1540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_633965/3017613476.py:68: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data_stamp = df_stamp.drop(['date'], 1).values\n",
      "/tmp/ipykernel_633965/3017613476.py:68: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data_stamp = df_stamp.drop(['date'], 1).values\n",
      "/tmp/ipykernel_633965/3017613476.py:68: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data_stamp = df_stamp.drop(['date'], 1).values\n"
     ]
    }
   ],
   "source": [
    "dset_train = Dataset_ETT_minute(split=\"train\", size=SIZE)\n",
    "dset_val = Dataset_ETT_minute(split=\"val\", size=SIZE)\n",
    "dset_test = Dataset_ETT_minute(split=\"test\", size=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ed102a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 7]), torch.Size([96, 7]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd=dset_val.__getitem__(0)\n",
    "dd[\"context_values\"].shape, dd[\"target_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28d85d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_patches should be (no need to specify)\n",
    "num_patches = seq_len//patch_len\n",
    "num_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc772c2",
   "metadata": {},
   "source": [
    "## 1. Directly train a `PatchTSMixer` forecasting model, and evaluate the test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e18e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PatchTSMixerForForecasting\n",
    "\n",
    "forecast_config = PatchTSMixerConfig(\n",
    "    in_channels=n_features,\n",
    "    seq_len=SEQ_LEN,\n",
    "    patch_len=patch_len,\n",
    "    stride=stride,\n",
    "    num_features=48,\n",
    "    num_layers=2,\n",
    "    dropout=0.5,\n",
    "    mode=\"common_channel\",\n",
    "    revin=True,\n",
    "    expansion_factor=3,\n",
    "    head_dropout=0.7,\n",
    "    forecast_len=FORECAST_LEN\n",
    ")\n",
    "\n",
    "forecast_model = PatchTSMixerForForecasting(forecast_config)\n",
    "\n",
    "forecast_args = TrainingArguments(\n",
    "        output_dir='./dump/ettm2/direct_forecast/checkpoint',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./dump/ettm2/direct_forecast/logs',  # Make sure to specify a logging directory\n",
    "        \n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "    )\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Create the early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.0001,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "\n",
    "forecast_trainer = Trainer(\n",
    "    model=forecast_model,\n",
    "    args=forecast_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c6a2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18054' max='106200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 18054/106200 03:11 < 15:37, 94.04 it/s, Epoch 17/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.117871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>0.116620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.116322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>0.115694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.212600</td>\n",
       "      <td>0.115662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.209600</td>\n",
       "      <td>0.115555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.207100</td>\n",
       "      <td>0.115166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.116261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.115753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.201800</td>\n",
       "      <td>0.115477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>0.115819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.116089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.198400</td>\n",
       "      <td>0.116058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.197100</td>\n",
       "      <td>0.115847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.116112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.115978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.115631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18054, training_loss=0.2100857529408303, metrics={'train_runtime': 193.1515, 'train_samples_per_second': 17578.43, 'train_steps_per_second': 549.827, 'total_flos': 2434564886347776.0, 'train_loss': 0.2100857529408303, 'epoch': 17.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "987482a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1638096570968628,\n",
       " 'eval_runtime': 0.5798,\n",
       " 'eval_samples_per_second': 19706.517,\n",
       " 'eval_steps_per_second': 20.698,\n",
       " 'epoch': 17.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_trainer.evaluate(dset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716f804",
   "metadata": {},
   "source": [
    "## 2. Pretrain a `PatchTSMixer` model with HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd629743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PatchTSMixerForPretraining\n",
    "\n",
    "pretrain_config = PatchTSMixerConfig(\n",
    "    in_channels=n_features,\n",
    "    seq_len=SEQ_LEN,\n",
    "    patch_len=patch_len,\n",
    "    stride=stride,\n",
    "    num_features=48,\n",
    "    num_layers=2,\n",
    "    dropout=0.5,\n",
    "    mode=\"common_channel\",\n",
    "    revin=True,\n",
    "    expansion_factor=3,\n",
    "    head_dropout=0.7,\n",
    "    forecast_len=FORECAST_LEN\n",
    ")\n",
    "\n",
    "pretrain_model = PatchTSMixerForPretraining(pretrain_config)\n",
    "\n",
    "pretrain_args = TrainingArguments(\n",
    "        output_dir='./dump/ettm2/pretrain/checkpoint',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./dump/ettm2/pretrain/logs',  # Make sure to specify a logging directory\n",
    "        \n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "    )\n",
    "\n",
    "pretrain_trainer = Trainer(\n",
    "    model=pretrain_model,\n",
    "    args=pretrain_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd123020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77526' max='106200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 77526/106200 14:02 < 05:11, 92.02 it/s, Epoch 73/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.477664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.410036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.393636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.462800</td>\n",
       "      <td>0.387728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.455200</td>\n",
       "      <td>0.384844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.450800</td>\n",
       "      <td>0.381542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.378573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.446400</td>\n",
       "      <td>0.379075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>0.377123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.376751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.441900</td>\n",
       "      <td>0.373669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.373107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.439200</td>\n",
       "      <td>0.371362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>0.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.368403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.435400</td>\n",
       "      <td>0.367485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.365748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.433900</td>\n",
       "      <td>0.363392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.364305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.431900</td>\n",
       "      <td>0.362821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>0.362064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.430100</td>\n",
       "      <td>0.360025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.429400</td>\n",
       "      <td>0.358185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.356105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.427300</td>\n",
       "      <td>0.355871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.353595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>0.354341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.424900</td>\n",
       "      <td>0.352536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.423900</td>\n",
       "      <td>0.351414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.350347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.350203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.347824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>0.348772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.346699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.420100</td>\n",
       "      <td>0.345870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.419600</td>\n",
       "      <td>0.345962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.346218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.345037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.345481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.417800</td>\n",
       "      <td>0.343438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.344219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.342345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.344468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>0.343221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.416100</td>\n",
       "      <td>0.342427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.415500</td>\n",
       "      <td>0.341298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.415400</td>\n",
       "      <td>0.341344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.341796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>0.342076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>0.342641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.414300</td>\n",
       "      <td>0.342548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.341482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.413600</td>\n",
       "      <td>0.341183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.413300</td>\n",
       "      <td>0.341590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.340884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.413300</td>\n",
       "      <td>0.341704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.341323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.341309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.341459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.412400</td>\n",
       "      <td>0.341116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.341531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.412400</td>\n",
       "      <td>0.340884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.412300</td>\n",
       "      <td>0.340081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.340815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>0.340861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.340751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.340087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.411200</td>\n",
       "      <td>0.340969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.340164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.340118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.340725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.411300</td>\n",
       "      <td>0.340793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.340280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=77526, training_loss=0.4298172266715864, metrics={'train_runtime': 842.6717, 'train_samples_per_second': 4029.208, 'train_steps_per_second': 126.028, 'total_flos': 2631698720587776.0, 'train_loss': 0.4298172266715864, 'epoch': 73.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a10355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3604498505592346,\n",
       " 'eval_runtime': 0.4458,\n",
       " 'eval_samples_per_second': 25626.354,\n",
       " 'eval_steps_per_second': 26.916,\n",
       " 'epoch': 73.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_trainer.evaluate(dset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bba021",
   "metadata": {},
   "source": [
    "### Save batch and output for pytests (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "012aa9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.data_loader.DataLoaderShard at 0x14dcae3d33a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train dataloader\n",
    "eval_dataloader = pretrain_trainer.get_eval_dataloader()\n",
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d26cc664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context_values'])\n",
      "torch.Size([1024, 512, 7])\n"
     ]
    }
   ],
   "source": [
    "for X in eval_dataloader:\n",
    "    print(X.keys())\n",
    "    print(X[\"context_values\"].shape)\n",
    "    torch.save(X, \"./dump/ettm2/pretrain/batch.pt\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb1bdc6",
   "metadata": {},
   "source": [
    "### Saving and loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "185310a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_trainer.save_model(\"./dump/ettm2/pretrain/patchtsmixer_pretrained_ettm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d26d094b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = PatchTSMixerForPretraining.from_pretrained('./dump/ettm2/pretrain/patchtsmixer_pretrained_ettm2').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25ae08a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.1514]],\n",
      "\n",
      "        [[1.0473]],\n",
      "\n",
      "        [[0.9348]],\n",
      "\n",
      "        [[0.8902]],\n",
      "\n",
      "        [[0.6631]],\n",
      "\n",
      "        [[0.8166]],\n",
      "\n",
      "        [[1.6246]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    d_out = loaded_model.forward(X['context_values'].reshape(1024,seq_len,n_features).to(\"cuda\"))\n",
    "print(d_out.prediction_logits[0, :7, :1, :1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c5561",
   "metadata": {},
   "source": [
    "Copy the above values in a pytest and use the dumped model to verify (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4265cc",
   "metadata": {},
   "source": [
    "## 3. Use the pretrained model (step 2) to finetune for a forecasting task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd72f28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PatchTSMixerForForecasting were not initialized from the model checkpoint at ./dump/ettm2/pretrain/patchtsmixer_pretrained_ettm2 and are newly initialized: ['head.head.base_forecast_block.1.bias', 'head.head.base_forecast_block.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "finetune_forecast_model = PatchTSMixerForForecasting.from_pretrained('./dump/ettm2/pretrain/patchtsmixer_pretrained_ettm2')\n",
    "\n",
    "finetune_forecast_args = TrainingArguments(\n",
    "        output_dir='./dump/ettm2/finetune_forecast/checkpoint',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./dump/ettm2/finetune_forecast/logs',  # Make sure to specify a logging directory\n",
    "        \n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "    )\n",
    "\n",
    "finetune_forecast_trainer = Trainer(\n",
    "    model=finetune_forecast_model,\n",
    "    args=finetune_forecast_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "298d2793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21240' max='106200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 21240/106200 03:49 < 15:17, 92.58 it/s, Epoch 20/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.119586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.115957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>0.115903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.114983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>0.114495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.114195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.203500</td>\n",
       "      <td>0.113950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.199600</td>\n",
       "      <td>0.114402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.115237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.113769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.114860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.189600</td>\n",
       "      <td>0.114895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.114834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>0.114623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.115015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>0.114909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>0.115216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.114960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>0.115303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.180900</td>\n",
       "      <td>0.116188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21240, training_loss=0.20082706228712408, metrics={'train_runtime': 229.4165, 'train_samples_per_second': 14799.717, 'train_steps_per_second': 462.913, 'total_flos': 2864193983938560.0, 'train_loss': 0.20082706228712408, 'epoch': 20.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_forecast_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01328a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16320058703422546,\n",
       " 'eval_runtime': 0.4929,\n",
       " 'eval_samples_per_second': 23178.925,\n",
       " 'eval_steps_per_second': 24.345,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_forecast_trainer.evaluate(dset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7467c00",
   "metadata": {},
   "source": [
    "### Save/Load and dump outputs for pytest (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42958cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_forecast_trainer.save_model(\"./dump/ettm2/finetune_forecast/patchtsmixer_finetune_forecast_ettm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dc328ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.data_loader.DataLoaderShard at 0x14dcae39dd90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train dataloader\n",
    "eval_dataloader = finetune_forecast_trainer.get_eval_dataloader()\n",
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "571c9047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = PatchTSMixerForForecasting.from_pretrained('./dump/ettm2/finetune_forecast/patchtsmixer_finetune_forecast_ettm2').to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a22a8c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context_values', 'target_values'])\n",
      "torch.Size([1024, 512, 7]) torch.Size([1024, 96, 7])\n"
     ]
    }
   ],
   "source": [
    "for X in eval_dataloader:\n",
    "    print(X.keys())\n",
    "    print(X[\"context_values\"].shape, X[\"target_values\"].shape)\n",
    "    torch.save(X, \"./dump/ettm2/finetune_forecast/batch_forecast.pt\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a085812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9803, -0.8590, -0.4304,  0.0600, -2.0267, -0.1246,  0.8570]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_batch = loaded_model(X[\"context_values\"])\n",
    "print(output_batch.prediction_logits[0, :1, :7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b322f54",
   "metadata": {},
   "source": [
    "Copy the above values in a pytest and use the dumped model to verify (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dfa02e",
   "metadata": {},
   "source": [
    "## 4. Use pretrained model to finetune for a prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d1a7778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PatchTSMixerForForecasting were not initialized from the model checkpoint at ./dump/ettm2/pretrain/patchtsmixer_pretrained_ettm2 and are newly initialized: ['head.head.base_forecast_block.1.bias', 'head.head.base_forecast_block.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We can either provide the forecast channel indices during pretraining\n",
    "# Or, we can update the config and pass it again\n",
    "pretrain_config.update({\"forecast_channel_indices\": [3,5]})\n",
    "finetune_prediction_model = PatchTSMixerForForecasting.from_pretrained('./dump/ettm2/pretrain/patchtsmixer_pretrained_ettm2', config=pretrain_config)\n",
    "\n",
    "\n",
    "finetune_prediction_args = TrainingArguments(\n",
    "        output_dir='./dump/ettm2/finetune_prediction/checkpoint',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=1024,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./dump/ettm2/finetune_prediction/logs',  # Make sure to specify a logging directory\n",
    "        \n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "    )\n",
    "\n",
    "finetune_prediction_trainer = Trainer(\n",
    "    model=finetune_prediction_model,\n",
    "    args=finetune_prediction_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0424162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18054' max='106200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 18054/106200 03:21 < 16:22, 89.69 it/s, Epoch 17/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.133379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.128137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.127083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.246200</td>\n",
       "      <td>0.126059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>0.125614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.229900</td>\n",
       "      <td>0.124916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.224100</td>\n",
       "      <td>0.124585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.125360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.216900</td>\n",
       "      <td>0.125464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.215300</td>\n",
       "      <td>0.124929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.126794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.125477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.124690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.206100</td>\n",
       "      <td>0.125884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.126102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>0.125671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.202800</td>\n",
       "      <td>0.125661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18054, training_loss=0.230753371125772, metrics={'train_runtime': 201.3603, 'train_samples_per_second': 16861.811, 'train_steps_per_second': 527.413, 'total_flos': 2434564886347776.0, 'train_loss': 0.230753371125772, 'epoch': 17.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_prediction_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b7629b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.20171673595905304,\n",
       " 'eval_runtime': 0.5134,\n",
       " 'eval_samples_per_second': 22251.699,\n",
       " 'eval_steps_per_second': 23.372,\n",
       " 'epoch': 17.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_prediction_trainer.evaluate(dset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66d91fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12458471208810806,\n",
       " 'eval_runtime': 0.5215,\n",
       " 'eval_samples_per_second': 21909.806,\n",
       " 'eval_steps_per_second': 23.012,\n",
       " 'epoch': 17.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_prediction_trainer.evaluate(dset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92705943",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_prediction_trainer.save_model(\"./dump/ettm2/finetune_prediction/patchtsmixer_finetune_prediction_ettm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d43248f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.data_loader.DataLoaderShard at 0x14dcadef0f70>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train dataloader\n",
    "eval_dataloader = finetune_prediction_trainer.get_eval_dataloader()\n",
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdec3129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = PatchTSMixerForForecasting.from_pretrained('./dump/ettm2/finetune_prediction/patchtsmixer_finetune_prediction_ettm2').to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1614a2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context_values', 'target_values'])\n",
      "torch.Size([1024, 512, 7]) torch.Size([1024, 96, 7])\n"
     ]
    }
   ],
   "source": [
    "for X in eval_dataloader:\n",
    "    print(X.keys())\n",
    "    print(X[\"context_values\"].shape, X[\"target_values\"].shape)\n",
    "    torch.save(X, \"./dump/ettm2/finetune_prediction/batch_prediction.pt\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d610df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 2])\n",
      "tensor([[ 0.0500, -0.1313]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_batch = loaded_model(X[\"context_values\"])\n",
    "print(output_batch.prediction_logits.shape)\n",
    "print(output_batch.prediction_logits[0, :1, :7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff30f1d",
   "metadata": {},
   "source": [
    "Note that, the output has only 2 channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150c2c7",
   "metadata": {},
   "source": [
    "## 5. Register model under Auto Classes and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a61d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoModelForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e56b0828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoConfig.register(\"patchtsmixer\", PatchTSMixerConfig)\n",
    "AutoModelForPreTraining.register(PatchTSMixerConfig, PatchTSMixerForPretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0a0b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_model = AutoModelForPreTraining.from_pretrained('./dump/ettm2/pretrain/patchtsmixer_pretrained_ettm2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a9b4954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.3256e+00,  1.3209e+00,  1.3326e+00,  ...,  1.0987e+00,\n",
       "            1.1460e+00,  1.1975e+00],\n",
       "          [ 1.3033e+00,  1.3028e+00,  1.2580e+00,  ...,  9.0834e-01,\n",
       "            8.8601e-01,  8.6099e-01],\n",
       "          [ 5.1945e-01,  5.4233e-01,  5.7474e-01,  ...,  6.1828e-01,\n",
       "            6.4622e-01,  6.5892e-01],\n",
       "          ...,\n",
       "          [ 6.9123e-01,  7.5057e-01,  7.9863e-01,  ...,  8.5491e-01,\n",
       "            8.3613e-01,  7.7443e-01],\n",
       "          [ 1.0362e+00,  1.0745e+00,  1.0930e+00,  ...,  1.0928e-01,\n",
       "            9.6637e-02,  7.7504e-02],\n",
       "          [ 6.7881e-01,  6.9865e-01,  7.1106e-01,  ...,  6.5421e-01,\n",
       "            6.3993e-01,  6.2467e-01]],\n",
       "\n",
       "         [[ 9.9452e-02,  1.0485e-01,  1.0052e-01,  ...,  1.3974e-01,\n",
       "            1.2174e-01,  9.8005e-02],\n",
       "          [ 1.1278e-01,  1.1193e-01,  1.0288e-01,  ...,  1.7026e-01,\n",
       "            1.5399e-01,  1.3636e-01],\n",
       "          [ 1.3699e-01,  1.3539e-01,  1.2659e-01,  ...,  1.7387e-01,\n",
       "            1.5944e-01,  1.3994e-01],\n",
       "          ...,\n",
       "          [ 1.0162e-01,  1.0239e-01,  9.3948e-02,  ...,  1.3975e-01,\n",
       "            1.2601e-01,  1.0766e-01],\n",
       "          [ 1.3299e-01,  1.3874e-01,  1.3731e-01,  ...,  1.8802e-01,\n",
       "            1.7117e-01,  1.4534e-01],\n",
       "          [ 3.1638e-01,  3.2094e-01,  3.1629e-01,  ...,  2.7788e-01,\n",
       "            2.7437e-01,  2.7522e-01]],\n",
       "\n",
       "         [[ 1.3897e+00,  1.3558e+00,  1.3313e+00,  ...,  1.1629e+00,\n",
       "            1.1997e+00,  1.2646e+00],\n",
       "          [ 1.3589e+00,  1.3456e+00,  1.3056e+00,  ...,  9.4638e-01,\n",
       "            9.1567e-01,  8.8496e-01],\n",
       "          [ 6.7899e-01,  6.4958e-01,  6.3414e-01,  ...,  6.5389e-01,\n",
       "            6.7645e-01,  6.5327e-01],\n",
       "          ...,\n",
       "          [ 7.9300e-01,  8.3609e-01,  8.5275e-01,  ...,  8.9619e-01,\n",
       "            8.7901e-01,  8.1360e-01],\n",
       "          [ 9.1913e-01,  9.6484e-01,  1.0064e+00,  ..., -3.6361e-03,\n",
       "           -1.9457e-02, -2.0699e-02],\n",
       "          [ 6.2350e-01,  6.4243e-01,  6.5261e-01,  ...,  5.8599e-01,\n",
       "            5.7160e-01,  5.5769e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.7087e-01, -7.3461e-01, -4.1363e-01,  ..., -4.9380e-01,\n",
       "           -4.3093e-01, -4.9875e-01],\n",
       "          [-7.7700e-01, -7.8469e-01, -7.8346e-01,  ..., -5.1307e-01,\n",
       "           -3.6807e-01, -4.3081e-01],\n",
       "          [-1.2232e+00, -9.9214e-01, -7.6092e-01,  ..., -1.5504e-01,\n",
       "           -1.7107e-01,  5.3101e-02],\n",
       "          ...,\n",
       "          [-5.1460e-01, -5.3281e-01, -5.1758e-01,  ..., -3.9979e-01,\n",
       "           -3.4477e-01, -3.0430e-01],\n",
       "          [ 6.4969e-01,  6.8316e-01,  6.3125e-01,  ...,  5.6021e-01,\n",
       "            7.6031e-01,  8.0343e-01],\n",
       "          [ 6.1357e-01,  6.3559e-01,  6.5073e-01,  ...,  6.7316e-01,\n",
       "            6.8156e-01,  6.8422e-01]],\n",
       "\n",
       "         [[-5.4271e-01, -5.4293e-01, -4.7766e-01,  ..., -1.4998e+00,\n",
       "           -1.4990e+00, -1.4663e+00],\n",
       "          [-1.3489e+00, -1.3845e+00, -1.4125e+00,  ..., -1.3226e+00,\n",
       "           -1.3100e+00, -1.2919e+00],\n",
       "          [-1.3030e+00, -1.3266e+00, -1.3291e+00,  ..., -7.4797e-02,\n",
       "           -1.8768e-01, -2.2444e-01],\n",
       "          ...,\n",
       "          [ 6.9611e-01,  7.3104e-01,  6.5329e-01,  ..., -2.4181e-01,\n",
       "           -2.5411e-01, -2.5058e-01],\n",
       "          [-8.7455e-02,  1.6374e-01,  4.8602e-01,  ...,  1.8946e+00,\n",
       "            2.1647e+00,  2.1400e+00],\n",
       "          [ 8.6660e-01,  8.8733e-01,  9.0389e-01,  ...,  9.4246e-01,\n",
       "            9.4130e-01,  9.4288e-01]],\n",
       "\n",
       "         [[ 4.9423e-01,  5.0097e-01,  4.8308e-01,  ...,  1.9845e-01,\n",
       "            1.7248e-01,  1.1975e-01],\n",
       "          [ 8.4968e-03,  1.7156e-03, -4.1159e-03,  ..., -2.1672e-01,\n",
       "           -2.2646e-01, -2.3427e-01],\n",
       "          [-3.0691e-01, -3.3464e-01, -3.5341e-01,  ..., -5.5658e-01,\n",
       "           -5.9007e-01, -5.8757e-01],\n",
       "          ...,\n",
       "          [ 1.0998e+00,  1.1204e+00,  1.1163e+00,  ...,  1.0830e+00,\n",
       "            1.0883e+00,  1.0586e+00],\n",
       "          [ 1.0290e+00,  1.0346e+00,  1.0221e+00,  ...,  6.0794e-02,\n",
       "           -8.1099e-03, -4.9735e-02],\n",
       "          [ 1.3576e-01,  1.4539e-01,  1.3151e-01,  ..., -1.6230e-01,\n",
       "           -1.7933e-01, -1.8124e-01]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# auto_model(test_data.to('mps')).prediction_logits\n",
    "auto_model(dset_test.__getitem__(0)['context_values'].reshape(1,seq_len,n_features)).prediction_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac1fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
