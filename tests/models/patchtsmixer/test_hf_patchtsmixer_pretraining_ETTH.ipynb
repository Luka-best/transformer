{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be37b36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/dnn_forecasting/conda_envs/envs/fm/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PatchTSMixerConfig, PatchTSMixerForPretraining, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c744e",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ad3bd",
   "metadata": {},
   "source": [
    "Generate and prepare dummy data to test the pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6807fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "class ETTDataset(Dataset):\n",
    "    def __init__(self, root_path='/dccstor/dnn_forecasting/FM/data/ETDataset/ETT-small/', data_file='ETTh1.csv', \n",
    "                 seq_len=128, pred_len=32,\n",
    "                 split='train',                  \n",
    "                 scale=True\n",
    "                 ):\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        # init\n",
    "        assert split in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[split]\n",
    "                \n",
    "        self.scale = scale                        \n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_file = data_file\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_file))\n",
    "\n",
    "        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
    "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "        \n",
    "        cols_data = df_raw.columns[1:]\n",
    "        df_data = df_raw[cols_data]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "        \n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end\n",
    "        r_end = r_begin + self.pred_len \n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]        \n",
    "        return {\"context_values\": torch.Tensor(seq_x), \"target_values\": torch.Tensor(seq_y)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f174db",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 512\n",
    "FORECAST_LEN = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "822ea8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = ETTDataset(split=\"train\", seq_len=SEQ_LEN, pred_len=FORECAST_LEN)\n",
    "dset_val = ETTDataset(split=\"val\", seq_len=SEQ_LEN, pred_len=FORECAST_LEN)\n",
    "dset_test = ETTDataset(split=\"test\", seq_len=SEQ_LEN, pred_len=FORECAST_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edbaf7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 7]), torch.Size([96, 7]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd=dset_val.__getitem__(0)\n",
    "dd[\"context_values\"].shape, dd[\"target_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e64d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 7\n",
    "seq_len = SEQ_LEN\n",
    "patch_len = 16\n",
    "stride = patch_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b5274b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_patches should be (no need to specify)\n",
    "num_patches = seq_len//patch_len\n",
    "num_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f726eac",
   "metadata": {},
   "source": [
    "### Pretrain the model with HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b43993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PatchTSMixerConfig(\n",
    "    in_channels=n_features,\n",
    "    seq_len=seq_len,\n",
    "    patch_len=patch_len,\n",
    "    stride=stride,\n",
    "    num_features=48,\n",
    "    num_layers=2,\n",
    "    dropout=0.5,\n",
    "    mode=\"common_channel\",\n",
    "    revin=True,\n",
    "    expansion_factor=3,\n",
    "    head_dropout=0.7,\n",
    "    forecast_len=FORECAST_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d874ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PatchTSMixerForPretraining(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb09677",
   "metadata": {},
   "source": [
    "Define the training arguments and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2b4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./checkpoint',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./logs',  # Make sure to specify a logging directory\n",
    "        log_level=\"info\",  # Explicitly set the logging level\n",
    "        \n",
    "#         load_best_model_at_end=True\n",
    "#         no_cuda=True,\n",
    "#         use_mps_device = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "646ebdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args.set_push_to_hub(\"ajati/patchtsmixer_pretrained_etth1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444d9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93715bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8,033\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25,200\n",
      "  Number of trainable parameters = 49,376\n",
      "The following columns in the training set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25200' max='25200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25200/25200 05:40, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.537779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.474375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>0.441245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.566400</td>\n",
       "      <td>0.422306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.557700</td>\n",
       "      <td>0.407849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.548900</td>\n",
       "      <td>0.394484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.542400</td>\n",
       "      <td>0.381240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>0.373011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.534500</td>\n",
       "      <td>0.365847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.360806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.529400</td>\n",
       "      <td>0.356849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.526800</td>\n",
       "      <td>0.352648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.525700</td>\n",
       "      <td>0.349044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>0.346310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.344566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.521700</td>\n",
       "      <td>0.341278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.339425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.518400</td>\n",
       "      <td>0.337337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.517100</td>\n",
       "      <td>0.335389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.516700</td>\n",
       "      <td>0.334670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>0.331724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.332741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>0.330242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.514300</td>\n",
       "      <td>0.328798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.328155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>0.326732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>0.327139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>0.326722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>0.326829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.325489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.509400</td>\n",
       "      <td>0.324206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>0.324681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.324621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.508200</td>\n",
       "      <td>0.321682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.507900</td>\n",
       "      <td>0.322168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.507600</td>\n",
       "      <td>0.321827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.506400</td>\n",
       "      <td>0.320041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.506100</td>\n",
       "      <td>0.321046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.507300</td>\n",
       "      <td>0.319488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>0.320891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.506200</td>\n",
       "      <td>0.319543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.320428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.320445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.505300</td>\n",
       "      <td>0.319354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.504900</td>\n",
       "      <td>0.319215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>0.319563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.504900</td>\n",
       "      <td>0.317207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.503900</td>\n",
       "      <td>0.318219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.319359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.503100</td>\n",
       "      <td>0.318530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.504600</td>\n",
       "      <td>0.318283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.316831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.503900</td>\n",
       "      <td>0.319018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.503400</td>\n",
       "      <td>0.317995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.316640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>0.316252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.502800</td>\n",
       "      <td>0.316603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>0.316953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.502300</td>\n",
       "      <td>0.316631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.316432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.503300</td>\n",
       "      <td>0.316047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.502300</td>\n",
       "      <td>0.315841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.315370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.503300</td>\n",
       "      <td>0.317111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.315906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.502500</td>\n",
       "      <td>0.316078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>0.315329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.316044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.315951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.502100</td>\n",
       "      <td>0.317025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.502500</td>\n",
       "      <td>0.315592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.315420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.314599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>0.315151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>0.314396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.314408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.314262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.315921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.314202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.315167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.501700</td>\n",
       "      <td>0.313376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.500100</td>\n",
       "      <td>0.314906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.313769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.314801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.314005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.314933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.314725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.313942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.314104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.313634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.501100</td>\n",
       "      <td>0.315845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.313836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.313371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.500900</td>\n",
       "      <td>0.313551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.500100</td>\n",
       "      <td>0.313735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.314514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.314405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>0.314519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.313807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-252\n",
      "Configuration saved in ./checkpoint/checkpoint-252/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-252/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-24892] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-504\n",
      "Configuration saved in ./checkpoint/checkpoint-504/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-504/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-25146] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-756\n",
      "Configuration saved in ./checkpoint/checkpoint-756/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-756/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-25400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1008\n",
      "Configuration saved in ./checkpoint/checkpoint-1008/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1008/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-252] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1260\n",
      "Configuration saved in ./checkpoint/checkpoint-1260/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1260/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-504] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1512\n",
      "Configuration saved in ./checkpoint/checkpoint-1512/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-756] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-1764\n",
      "Configuration saved in ./checkpoint/checkpoint-1764/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-1764/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1008] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2016\n",
      "Configuration saved in ./checkpoint/checkpoint-2016/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2016/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2268\n",
      "Configuration saved in ./checkpoint/checkpoint-2268/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2268/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2520\n",
      "Configuration saved in ./checkpoint/checkpoint-2520/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2520/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-1764] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-2772\n",
      "Configuration saved in ./checkpoint/checkpoint-2772/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-2772/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2016] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3024\n",
      "Configuration saved in ./checkpoint/checkpoint-3024/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3024/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2268] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3276\n",
      "Configuration saved in ./checkpoint/checkpoint-3276/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3276/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2520] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3528\n",
      "Configuration saved in ./checkpoint/checkpoint-3528/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3528/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-2772] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-3780\n",
      "Configuration saved in ./checkpoint/checkpoint-3780/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-3780/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3024] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4032\n",
      "Configuration saved in ./checkpoint/checkpoint-4032/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4032/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3276] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4284\n",
      "Configuration saved in ./checkpoint/checkpoint-4284/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4284/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3528] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4536\n",
      "Configuration saved in ./checkpoint/checkpoint-4536/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4536/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-3780] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-4788\n",
      "Configuration saved in ./checkpoint/checkpoint-4788/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-4788/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4032] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5040\n",
      "Configuration saved in ./checkpoint/checkpoint-5040/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5040/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4284] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5292\n",
      "Configuration saved in ./checkpoint/checkpoint-5292/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5292/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4536] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5544\n",
      "Configuration saved in ./checkpoint/checkpoint-5544/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5544/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-4788] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-5796\n",
      "Configuration saved in ./checkpoint/checkpoint-5796/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-5796/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5040] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6048\n",
      "Configuration saved in ./checkpoint/checkpoint-6048/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6048/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5292] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6300\n",
      "Configuration saved in ./checkpoint/checkpoint-6300/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6300/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5544] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6552\n",
      "Configuration saved in ./checkpoint/checkpoint-6552/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6552/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-5796] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-6804\n",
      "Configuration saved in ./checkpoint/checkpoint-6804/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-6804/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6048] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7056\n",
      "Configuration saved in ./checkpoint/checkpoint-7056/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7056/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7308\n",
      "Configuration saved in ./checkpoint/checkpoint-7308/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7308/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6552] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7560\n",
      "Configuration saved in ./checkpoint/checkpoint-7560/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7560/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-6804] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-7812\n",
      "Configuration saved in ./checkpoint/checkpoint-7812/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-7812/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7056] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8064\n",
      "Configuration saved in ./checkpoint/checkpoint-8064/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8064/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7308] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8316\n",
      "Configuration saved in ./checkpoint/checkpoint-8316/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8316/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7560] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8568\n",
      "Configuration saved in ./checkpoint/checkpoint-8568/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8568/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-7812] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-8820\n",
      "Configuration saved in ./checkpoint/checkpoint-8820/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-8820/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8064] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9072\n",
      "Configuration saved in ./checkpoint/checkpoint-9072/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9072/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8316] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9324\n",
      "Configuration saved in ./checkpoint/checkpoint-9324/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9324/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8568] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9576\n",
      "Configuration saved in ./checkpoint/checkpoint-9576/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9576/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-8820] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-9828\n",
      "Configuration saved in ./checkpoint/checkpoint-9828/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-9828/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9072] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10080\n",
      "Configuration saved in ./checkpoint/checkpoint-10080/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10080/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9324] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10332\n",
      "Configuration saved in ./checkpoint/checkpoint-10332/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10332/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10584\n",
      "Configuration saved in ./checkpoint/checkpoint-10584/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10584/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-9828] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-10836\n",
      "Configuration saved in ./checkpoint/checkpoint-10836/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-10836/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10080] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11088\n",
      "Configuration saved in ./checkpoint/checkpoint-11088/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11088/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10332] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11340\n",
      "Configuration saved in ./checkpoint/checkpoint-11340/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11340/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10584] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11592\n",
      "Configuration saved in ./checkpoint/checkpoint-11592/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11592/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-10836] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-11844\n",
      "Configuration saved in ./checkpoint/checkpoint-11844/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-11844/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11088] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12096\n",
      "Configuration saved in ./checkpoint/checkpoint-12096/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12096/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11340] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12348\n",
      "Configuration saved in ./checkpoint/checkpoint-12348/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12348/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11592] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12600\n",
      "Configuration saved in ./checkpoint/checkpoint-12600/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12600/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-11844] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-12852\n",
      "Configuration saved in ./checkpoint/checkpoint-12852/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-12852/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12096] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-13104\n",
      "Configuration saved in ./checkpoint/checkpoint-13104/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-13104/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12348] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-13356\n",
      "Configuration saved in ./checkpoint/checkpoint-13356/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-13356/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-13608\n",
      "Configuration saved in ./checkpoint/checkpoint-13608/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-13608/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-12852] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-13860\n",
      "Configuration saved in ./checkpoint/checkpoint-13860/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-13860/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-13104] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-14112\n",
      "Configuration saved in ./checkpoint/checkpoint-14112/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-14112/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-13356] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-14364\n",
      "Configuration saved in ./checkpoint/checkpoint-14364/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-14364/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-13608] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-14616\n",
      "Configuration saved in ./checkpoint/checkpoint-14616/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-14616/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-13860] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-14868\n",
      "Configuration saved in ./checkpoint/checkpoint-14868/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-14868/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-14112] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-15120\n",
      "Configuration saved in ./checkpoint/checkpoint-15120/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-15120/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-14364] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-15372\n",
      "Configuration saved in ./checkpoint/checkpoint-15372/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-15372/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-14616] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-15624\n",
      "Configuration saved in ./checkpoint/checkpoint-15624/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-15624/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-14868] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-15876\n",
      "Configuration saved in ./checkpoint/checkpoint-15876/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-15876/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-15120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-16128\n",
      "Configuration saved in ./checkpoint/checkpoint-16128/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-16128/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-15372] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-16380\n",
      "Configuration saved in ./checkpoint/checkpoint-16380/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-16380/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-15624] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-16632\n",
      "Configuration saved in ./checkpoint/checkpoint-16632/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-16632/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-15876] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-16884\n",
      "Configuration saved in ./checkpoint/checkpoint-16884/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-16884/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-16128] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-17136\n",
      "Configuration saved in ./checkpoint/checkpoint-17136/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-17136/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-16380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-17388\n",
      "Configuration saved in ./checkpoint/checkpoint-17388/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-17388/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-16632] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-17640\n",
      "Configuration saved in ./checkpoint/checkpoint-17640/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-17640/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-16884] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-17892\n",
      "Configuration saved in ./checkpoint/checkpoint-17892/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-17892/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-17136] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-18144\n",
      "Configuration saved in ./checkpoint/checkpoint-18144/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-18144/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-17388] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-18396\n",
      "Configuration saved in ./checkpoint/checkpoint-18396/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-18396/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-17640] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-18648\n",
      "Configuration saved in ./checkpoint/checkpoint-18648/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-18648/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-17892] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-18900\n",
      "Configuration saved in ./checkpoint/checkpoint-18900/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-18900/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-18144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-19152\n",
      "Configuration saved in ./checkpoint/checkpoint-19152/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-19152/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-18396] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-19404\n",
      "Configuration saved in ./checkpoint/checkpoint-19404/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-19404/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-18648] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-19656\n",
      "Configuration saved in ./checkpoint/checkpoint-19656/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-19656/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-18900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-19908\n",
      "Configuration saved in ./checkpoint/checkpoint-19908/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-19908/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-19152] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-20160\n",
      "Configuration saved in ./checkpoint/checkpoint-20160/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-20160/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-19404] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-20412\n",
      "Configuration saved in ./checkpoint/checkpoint-20412/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-20412/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-19656] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-20664\n",
      "Configuration saved in ./checkpoint/checkpoint-20664/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-20664/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-19908] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-20916\n",
      "Configuration saved in ./checkpoint/checkpoint-20916/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-20916/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-20160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-21168\n",
      "Configuration saved in ./checkpoint/checkpoint-21168/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-21168/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-20412] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-21420\n",
      "Configuration saved in ./checkpoint/checkpoint-21420/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-21420/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-20664] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-21672\n",
      "Configuration saved in ./checkpoint/checkpoint-21672/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-21672/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-20916] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-21924\n",
      "Configuration saved in ./checkpoint/checkpoint-21924/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-21924/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-21168] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-22176\n",
      "Configuration saved in ./checkpoint/checkpoint-22176/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-22176/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-21420] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-22428\n",
      "Configuration saved in ./checkpoint/checkpoint-22428/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-22428/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-21672] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-22680\n",
      "Configuration saved in ./checkpoint/checkpoint-22680/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-22680/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-21924] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-22932\n",
      "Configuration saved in ./checkpoint/checkpoint-22932/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-22932/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-22176] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-23184\n",
      "Configuration saved in ./checkpoint/checkpoint-23184/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-23184/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-22428] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-23436\n",
      "Configuration saved in ./checkpoint/checkpoint-23436/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-23436/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-22680] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-23688\n",
      "Configuration saved in ./checkpoint/checkpoint-23688/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-23688/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-22932] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-23940\n",
      "Configuration saved in ./checkpoint/checkpoint-23940/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-23940/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-23184] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-24192\n",
      "Configuration saved in ./checkpoint/checkpoint-24192/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-24192/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-23436] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-24444\n",
      "Configuration saved in ./checkpoint/checkpoint-24444/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-24444/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-23688] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-24696\n",
      "Configuration saved in ./checkpoint/checkpoint-24696/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-24696/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-23940] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-24948\n",
      "Configuration saved in ./checkpoint/checkpoint-24948/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-24948/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-24192] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./checkpoint/checkpoint-25200\n",
      "Configuration saved in ./checkpoint/checkpoint-25200/config.json\n",
      "Model weights saved in ./checkpoint/checkpoint-25200/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint/checkpoint-24444] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25200, training_loss=0.5162790231856089, metrics={'train_runtime': 341.0215, 'train_samples_per_second': 2355.57, 'train_steps_per_second': 73.896, 'total_flos': 852929082163200.0, 'train_loss': 0.5162790231856089, 'epoch': 100.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbcfa7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [88/88 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.31454017758369446,\n",
       " 'eval_runtime': 0.2412,\n",
       " 'eval_samples_per_second': 11547.349,\n",
       " 'eval_steps_per_second': 364.871,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = trainer.evaluate(dset_val)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d491f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.38878193497657776,\n",
       " 'eval_runtime': 0.4026,\n",
       " 'eval_samples_per_second': 6917.179,\n",
       " 'eval_steps_per_second': 218.568,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = trainer.evaluate(dset_test)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38bd5cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.data_loader.DataLoaderShard at 0x15536f226580>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train dataloader\n",
    "eval_dataloader = trainer.get_eval_dataloader()\n",
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249be3da",
   "metadata": {},
   "source": [
    "## Save batch and output for pytests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f911dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p pytest_data/etth1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "159d75d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PatchTSMixerForPretraining.forward` and have been ignored: target_values. If target_values are not expected by `PatchTSMixerForPretraining.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context_values'])\n",
      "torch.Size([32, 512, 7])\n",
      "tensor([[[-0.1297]],\n",
      "\n",
      "        [[ 0.6335]],\n",
      "\n",
      "        [[-0.0302]],\n",
      "\n",
      "        [[-0.3391]],\n",
      "\n",
      "        [[-0.3730]],\n",
      "\n",
      "        [[ 2.3447]],\n",
      "\n",
      "        [[ 0.1238]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for X in eval_dataloader:\n",
    "    print(X.keys())\n",
    "    print(X[\"context_values\"].shape)\n",
    "    torch.save(X, \"pytest_data/etth1/batch.pt\")\n",
    "    output_batch = model(X[\"context_values\"])\n",
    "    print(output_batch.prediction_logits[0, :7, :1, :1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb143800",
   "metadata": {},
   "source": [
    "## Saving and loading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baf79df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to pytest_data/etth1/patchtsmixer_pretrained_etth1\n",
      "Configuration saved in pytest_data/etth1/patchtsmixer_pretrained_etth1/config.json\n",
      "Model weights saved in pytest_data/etth1/patchtsmixer_pretrained_etth1/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"pytest_data/etth1/patchtsmixer_pretrained_etth1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0159c91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pytest_data/etth1/patchtsmixer_pretrained_etth1/config.json\n",
      "Model config PatchTSMixerConfig {\n",
      "  \"architectures\": [\n",
      "    \"PatchTSMixerForPretraining\"\n",
      "  ],\n",
      "  \"channel_consistent_masking\": true,\n",
      "  \"d_size\": \"4D\",\n",
      "  \"dropout\": 0.5,\n",
      "  \"expansion_factor\": 3,\n",
      "  \"forecast_channel_indices\": null,\n",
      "  \"forecast_len\": 96,\n",
      "  \"gated_attn\": true,\n",
      "  \"head_agg\": null,\n",
      "  \"head_dropout\": 0.7,\n",
      "  \"in_channels\": 7,\n",
      "  \"init_std\": 0.02,\n",
      "  \"mask_input\": true,\n",
      "  \"mask_mode\": \"mask_before_encoder\",\n",
      "  \"mask_patch_ratios\": [\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"mask_patches\": [\n",
      "    2,\n",
      "    3\n",
      "  ],\n",
      "  \"mask_ratio\": 0.5,\n",
      "  \"mask_type\": \"random\",\n",
      "  \"mask_value\": 0,\n",
      "  \"masked_loss\": false,\n",
      "  \"mode\": \"common_channel\",\n",
      "  \"model_type\": \"patchtsmixer\",\n",
      "  \"n_classes\": 3,\n",
      "  \"n_targets\": 3,\n",
      "  \"norm_mlp\": \"LayerNorm\",\n",
      "  \"num_features\": 48,\n",
      "  \"num_layers\": 2,\n",
      "  \"num_patches\": 32,\n",
      "  \"output_range\": null,\n",
      "  \"patch_last\": true,\n",
      "  \"patch_len\": 16,\n",
      "  \"revin\": true,\n",
      "  \"seed_number\": 42,\n",
      "  \"self_attn\": false,\n",
      "  \"self_attn_heads\": 1,\n",
      "  \"seq_len\": 512,\n",
      "  \"stride\": 16,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.32.0.dev0\"\n",
      "}\n",
      "\n",
      "loading weights file pytest_data/etth1/patchtsmixer_pretrained_etth1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing PatchTSMixerForPretraining.\n",
      "\n",
      "All the weights of PatchTSMixerForPretraining were initialized from the model checkpoint at pytest_data/etth1/patchtsmixer_pretrained_etth1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PatchTSMixerForPretraining for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = PatchTSMixerForPretraining.from_pretrained('pytest_data/etth1/patchtsmixer_pretrained_etth1').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06f3844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1184]],\n",
      "\n",
      "        [[ 0.6268]],\n",
      "\n",
      "        [[-0.0182]],\n",
      "\n",
      "        [[-0.3095]],\n",
      "\n",
      "        [[-0.3687]],\n",
      "\n",
      "        [[ 2.2908]],\n",
      "\n",
      "        [[ 0.1970]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# loaded_model(test_data.to('mps')).prediction_logits\n",
    "torch.manual_seed(0)\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    d_out = loaded_model.forward(X['context_values'].reshape(32,seq_len,n_features).to(\"cuda\"))\n",
    "print(d_out.prediction_logits[0, :7, :1, :1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc86dfd",
   "metadata": {},
   "source": [
    "## Use the pretrained model to finetune for a forecasting task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f7bf3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pytest_data/etth1/patchtsmixer_pretrained_etth1/config.json\n",
      "Model config PatchTSMixerConfig {\n",
      "  \"architectures\": [\n",
      "    \"PatchTSMixerForPretraining\"\n",
      "  ],\n",
      "  \"channel_consistent_masking\": true,\n",
      "  \"d_size\": \"4D\",\n",
      "  \"dropout\": 0.5,\n",
      "  \"expansion_factor\": 3,\n",
      "  \"forecast_channel_indices\": null,\n",
      "  \"forecast_len\": 96,\n",
      "  \"gated_attn\": true,\n",
      "  \"head_agg\": null,\n",
      "  \"head_dropout\": 0.7,\n",
      "  \"in_channels\": 7,\n",
      "  \"init_std\": 0.02,\n",
      "  \"mask_input\": true,\n",
      "  \"mask_mode\": \"mask_before_encoder\",\n",
      "  \"mask_patch_ratios\": [\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"mask_patches\": [\n",
      "    2,\n",
      "    3\n",
      "  ],\n",
      "  \"mask_ratio\": 0.5,\n",
      "  \"mask_type\": \"random\",\n",
      "  \"mask_value\": 0,\n",
      "  \"masked_loss\": false,\n",
      "  \"mode\": \"common_channel\",\n",
      "  \"model_type\": \"patchtsmixer\",\n",
      "  \"n_classes\": 3,\n",
      "  \"n_targets\": 3,\n",
      "  \"norm_mlp\": \"LayerNorm\",\n",
      "  \"num_features\": 48,\n",
      "  \"num_layers\": 2,\n",
      "  \"num_patches\": 32,\n",
      "  \"output_range\": null,\n",
      "  \"patch_last\": true,\n",
      "  \"patch_len\": 16,\n",
      "  \"revin\": true,\n",
      "  \"seed_number\": 42,\n",
      "  \"self_attn\": false,\n",
      "  \"self_attn_heads\": 1,\n",
      "  \"seq_len\": 512,\n",
      "  \"stride\": 16,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.32.0.dev0\"\n",
      "}\n",
      "\n",
      "loading weights file pytest_data/etth1/patchtsmixer_pretrained_etth1/pytorch_model.bin\n",
      "Some weights of the model checkpoint at pytest_data/etth1/patchtsmixer_pretrained_etth1 were not used when initializing PatchTSMixerForForecasting: ['head.head.base_pt_block.1.bias', 'head.head.base_pt_block.1.weight']\n",
      "- This IS expected if you are initializing PatchTSMixerForForecasting from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PatchTSMixerForForecasting from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PatchTSMixerForForecasting were not initialized from the model checkpoint at pytest_data/etth1/patchtsmixer_pretrained_etth1 and are newly initialized: ['head.head.base_forecast_block.1.weight', 'head.head.base_forecast_block.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.patchtsmixer.modeling_patchtsmixer import PatchTSMixerForForecasting\n",
    "\n",
    "config.update({\"forecast_len\": FORECAST_LEN})\n",
    "finetune_model = PatchTSMixerForForecasting.from_pretrained('pytest_data/etth1/patchtsmixer_pretrained_etth1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "748fcd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "finetune_args = TrainingArguments(\n",
    "        output_dir='./checkpoint_ftune',\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=0.0001,\n",
    "        num_train_epochs=100,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        report_to='tensorboard',\n",
    "        save_strategy='epoch',\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        logging_dir='./logs_ftune',  # Make sure to specify a logging directory\n",
    "        \n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8e37a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Create the early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.0001,  # Minimum improvement required to consider as improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dc5cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=finetune_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b140203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8,033\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25,200\n",
      "  Number of trainable parameters = 196,144\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5292' max='25200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5292/25200 01:13 < 04:37, 71.72 it/s, Epoch 21/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.832008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.420300</td>\n",
       "      <td>0.811938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.814699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.397400</td>\n",
       "      <td>0.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.802495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.390800</td>\n",
       "      <td>0.801148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.798693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.389300</td>\n",
       "      <td>0.808897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.804226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.800851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.381900</td>\n",
       "      <td>0.797533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.381700</td>\n",
       "      <td>0.794136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.377700</td>\n",
       "      <td>0.800460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.378300</td>\n",
       "      <td>0.797008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.809682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.793647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.796656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.800525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.798627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.807647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.374400</td>\n",
       "      <td>0.804834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-252\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-252/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-252/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-4318] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-504\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-504/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-504/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-4572] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-756\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-756/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-756/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-254] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-1008\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-1008/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-1008/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-252] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-1260\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-1260/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-1260/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-504] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-1512\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-1512/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-1512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-756] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-1764\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-1764/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-1764/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-1008] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-2016\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-2016/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-2016/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-1260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-2268\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-2268/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-2268/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-1512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-2520\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-2520/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-2520/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-2016] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-2772\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-2772/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-2772/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-1764] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-3024\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-3024/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-3024/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-2268] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-3276\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-3276/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-3276/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-2520] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-3528\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-3528/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-3528/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-2772] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-3780\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-3780/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-3780/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-3276] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-4032\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-4032/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-4032/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-3024] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-4284\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-4284/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-4284/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-3528] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-4536\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-4536/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-4536/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-3780] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-4788\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-4788/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-4788/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-4284] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-5040\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-5040/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-5040/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-4536] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-5292\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-5292/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-5292/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-4788] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./checkpoint_ftune/checkpoint-4032 (score: 0.7936473488807678).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5292, training_loss=0.3904967967615106, metrics={'train_runtime': 73.8278, 'train_samples_per_second': 10880.72, 'train_steps_per_second': 341.335, 'total_flos': 711526928007168.0, 'train_loss': 0.3904967967615106, 'epoch': 21.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15ee3dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [88/88 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3892132043838501,\n",
       " 'eval_runtime': 0.276,\n",
       " 'eval_samples_per_second': 10092.235,\n",
       " 'eval_steps_per_second': 318.893,\n",
       " 'epoch': 21.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_trainer.evaluate(dset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e64a7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to pytest_data/etth1/patchtsmixer_finetune_forecast_etth1\n",
      "Configuration saved in pytest_data/etth1/patchtsmixer_finetune_forecast_etth1/config.json\n",
      "Model weights saved in pytest_data/etth1/patchtsmixer_finetune_forecast_etth1/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "finetune_trainer.save_model(\"pytest_data/etth1/patchtsmixer_finetune_forecast_etth1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3137c7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.data_loader.DataLoaderShard at 0x155421f62d00>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train dataloader\n",
    "eval_dataloader = finetune_trainer.get_eval_dataloader()\n",
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59d36e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pytest_data/etth1/patchtsmixer_finetune_forecast_etth1/config.json\n",
      "Model config PatchTSMixerConfig {\n",
      "  \"_name_or_path\": \"pytest_data/etth1/patchtsmixer_pretrained_etth1\",\n",
      "  \"architectures\": [\n",
      "    \"PatchTSMixerForForecasting\"\n",
      "  ],\n",
      "  \"channel_consistent_masking\": true,\n",
      "  \"d_size\": \"4D\",\n",
      "  \"dropout\": 0.5,\n",
      "  \"expansion_factor\": 3,\n",
      "  \"forecast_channel_indices\": null,\n",
      "  \"forecast_len\": 96,\n",
      "  \"gated_attn\": true,\n",
      "  \"head_agg\": null,\n",
      "  \"head_dropout\": 0.7,\n",
      "  \"in_channels\": 7,\n",
      "  \"init_std\": 0.02,\n",
      "  \"mask_input\": true,\n",
      "  \"mask_mode\": \"mask_before_encoder\",\n",
      "  \"mask_patch_ratios\": [\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"mask_patches\": [\n",
      "    2,\n",
      "    3\n",
      "  ],\n",
      "  \"mask_ratio\": 0.5,\n",
      "  \"mask_type\": \"random\",\n",
      "  \"mask_value\": 0,\n",
      "  \"masked_loss\": false,\n",
      "  \"mode\": \"common_channel\",\n",
      "  \"model_type\": \"patchtsmixer\",\n",
      "  \"n_classes\": 3,\n",
      "  \"n_targets\": 3,\n",
      "  \"norm_mlp\": \"LayerNorm\",\n",
      "  \"num_features\": 48,\n",
      "  \"num_layers\": 2,\n",
      "  \"num_patches\": 32,\n",
      "  \"output_range\": null,\n",
      "  \"patch_last\": true,\n",
      "  \"patch_len\": 16,\n",
      "  \"revin\": true,\n",
      "  \"seed_number\": 42,\n",
      "  \"self_attn\": false,\n",
      "  \"self_attn_heads\": 1,\n",
      "  \"seq_len\": 512,\n",
      "  \"stride\": 16,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.32.0.dev0\"\n",
      "}\n",
      "\n",
      "loading weights file pytest_data/etth1/patchtsmixer_finetune_forecast_etth1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing PatchTSMixerForForecasting.\n",
      "\n",
      "All the weights of PatchTSMixerForForecasting were initialized from the model checkpoint at pytest_data/etth1/patchtsmixer_finetune_forecast_etth1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PatchTSMixerForForecasting for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = PatchTSMixerForForecasting.from_pretrained('pytest_data/etth1/patchtsmixer_finetune_forecast_etth1').to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c632308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context_values', 'target_values'])\n",
      "torch.Size([32, 512, 7]) torch.Size([32, 96, 7])\n"
     ]
    }
   ],
   "source": [
    "for X in eval_dataloader:\n",
    "    print(X.keys())\n",
    "    print(X[\"context_values\"].shape, X[\"target_values\"].shape)\n",
    "    torch.save(X, \"pytest_data/etth1/batch_forecast.pt\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d3f4548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5436,  0.0911,  0.6704,  0.8515, -0.4594, -2.3386,  0.3025]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_batch = loaded_model(X[\"context_values\"])\n",
    "print(output_batch.prediction_logits[0, :1, :7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c4a21",
   "metadata": {},
   "source": [
    "## Use pretrained model to finetune for a prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d58bca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytest_data/etth1/patchtsmixer_pretrained_etth1/pytorch_model.bin\n",
      "Some weights of the model checkpoint at pytest_data/etth1/patchtsmixer_pretrained_etth1 were not used when initializing PatchTSMixerForForecasting: ['head.head.base_pt_block.1.bias', 'head.head.base_pt_block.1.weight']\n",
      "- This IS expected if you are initializing PatchTSMixerForForecasting from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PatchTSMixerForForecasting from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PatchTSMixerForForecasting were not initialized from the model checkpoint at pytest_data/etth1/patchtsmixer_pretrained_etth1 and are newly initialized: ['head.head.base_forecast_block.1.weight', 'head.head.base_forecast_block.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.patchtsmixer.modeling_patchtsmixer import PatchTSMixerForForecasting\n",
    "config.update({\"forecast_channel_indices\": [1,4]})\n",
    "finetune_model_prediction = PatchTSMixerForForecasting.from_pretrained('pytest_data/etth1/patchtsmixer_pretrained_etth1', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "089e0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_trainer = Trainer(\n",
    "    model=finetune_model_prediction,\n",
    "    args=finetune_args,\n",
    "    train_dataset=dset_train,\n",
    "    eval_dataset=dset_val,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87c76116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8,033\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25,200\n",
      "  Number of trainable parameters = 196,144\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4284' max='25200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4284/25200 01:00 < 04:55, 70.81 it/s, Epoch 17/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>1.172535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>1.124103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.554100</td>\n",
       "      <td>1.112482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.542200</td>\n",
       "      <td>1.116687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.540300</td>\n",
       "      <td>1.089863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.531100</td>\n",
       "      <td>1.094732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.527600</td>\n",
       "      <td>1.082633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>1.096055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>1.083149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.519200</td>\n",
       "      <td>1.081012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>1.066906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.514700</td>\n",
       "      <td>1.065203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>1.076719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>1.077942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>1.093055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.509200</td>\n",
       "      <td>1.089898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>1.089299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-252\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-252/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-252/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-4032] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-504\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-504/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-504/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-5040] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-756\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-756/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-756/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-5292] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-1008\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-1008/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-1008/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-252] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-1260\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-1260/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-1260/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-504] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-1512\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-1512/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-1512/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-756] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-1764\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-1764/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-1764/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-1008] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-2016\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-2016/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-2016/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-1260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-2268\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-2268/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-2268/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-1512] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-2520\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-2520/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-2520/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-1764] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-2772\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-2772/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-2772/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-2016] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-3024\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-3024/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-3024/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-2268] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-3276\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-3276/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-3276/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-2520] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-3528\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-3528/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-3528/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-2772] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-3780\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-3780/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-3780/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-3276] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-4032\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-4032/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-4032/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-3528] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2785\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./checkpoint_ftune/checkpoint-4284\n",
      "Configuration saved in ./checkpoint_ftune/checkpoint-4284/config.json\n",
      "Model weights saved in ./checkpoint_ftune/checkpoint-4284/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoint_ftune/checkpoint-3780] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./checkpoint_ftune/checkpoint-3024 (score: 1.0652025938034058).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4284, training_loss=0.5330348900831492, metrics={'train_runtime': 60.488, 'train_samples_per_second': 13280.319, 'train_steps_per_second': 416.612, 'total_flos': 575997989339136.0, 'train_loss': 0.5330348900831492, 'epoch': 17.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca83f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to pytest_data/etth1/patchtsmixer_finetune_prediction_etth1\n",
      "Configuration saved in pytest_data/etth1/patchtsmixer_finetune_prediction_etth1/config.json\n",
      "Model weights saved in pytest_data/etth1/patchtsmixer_finetune_prediction_etth1/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "finetune_trainer.save_model(\"pytest_data/etth1/patchtsmixer_finetune_prediction_etth1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87d1ebe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<accelerate.data_loader.DataLoaderShard at 0x15536f203070>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train dataloader\n",
    "eval_dataloader = finetune_trainer.get_eval_dataloader()\n",
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bb02ab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pytest_data/etth1/patchtsmixer_finetune_prediction_etth1/config.json\n",
      "Model config PatchTSMixerConfig {\n",
      "  \"_name_or_path\": \"pytest_data/etth1/patchtsmixer_pretrained_etth1\",\n",
      "  \"architectures\": [\n",
      "    \"PatchTSMixerForForecasting\"\n",
      "  ],\n",
      "  \"channel_consistent_masking\": true,\n",
      "  \"d_size\": \"4D\",\n",
      "  \"dropout\": 0.5,\n",
      "  \"expansion_factor\": 3,\n",
      "  \"forecast_channel_indices\": [\n",
      "    1,\n",
      "    4\n",
      "  ],\n",
      "  \"forecast_len\": 96,\n",
      "  \"gated_attn\": true,\n",
      "  \"head_agg\": null,\n",
      "  \"head_dropout\": 0.7,\n",
      "  \"in_channels\": 7,\n",
      "  \"init_std\": 0.02,\n",
      "  \"mask_input\": true,\n",
      "  \"mask_mode\": \"mask_before_encoder\",\n",
      "  \"mask_patch_ratios\": [\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"mask_patches\": [\n",
      "    2,\n",
      "    3\n",
      "  ],\n",
      "  \"mask_ratio\": 0.5,\n",
      "  \"mask_type\": \"random\",\n",
      "  \"mask_value\": 0,\n",
      "  \"masked_loss\": false,\n",
      "  \"mode\": \"common_channel\",\n",
      "  \"model_type\": \"patchtsmixer\",\n",
      "  \"n_classes\": 3,\n",
      "  \"n_targets\": 3,\n",
      "  \"norm_mlp\": \"LayerNorm\",\n",
      "  \"num_features\": 48,\n",
      "  \"num_layers\": 2,\n",
      "  \"num_patches\": 32,\n",
      "  \"output_range\": null,\n",
      "  \"patch_last\": true,\n",
      "  \"patch_len\": 16,\n",
      "  \"revin\": true,\n",
      "  \"seed_number\": 42,\n",
      "  \"self_attn\": false,\n",
      "  \"self_attn_heads\": 1,\n",
      "  \"seq_len\": 512,\n",
      "  \"stride\": 16,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.32.0.dev0\"\n",
      "}\n",
      "\n",
      "loading weights file pytest_data/etth1/patchtsmixer_finetune_prediction_etth1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing PatchTSMixerForForecasting.\n",
      "\n",
      "All the weights of PatchTSMixerForForecasting were initialized from the model checkpoint at pytest_data/etth1/patchtsmixer_finetune_prediction_etth1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PatchTSMixerForForecasting for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = PatchTSMixerForForecasting.from_pretrained('pytest_data/etth1/patchtsmixer_finetune_prediction_etth1').to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f42ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context_values', 'target_values'])\n",
      "torch.Size([32, 512, 7]) torch.Size([32, 96, 7])\n"
     ]
    }
   ],
   "source": [
    "for X in eval_dataloader:\n",
    "    print(X.keys())\n",
    "    print(X[\"context_values\"].shape, X[\"target_values\"].shape)\n",
    "    torch.save(X, \"pytest_data/etth1/batch_prediction.pt\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfdee401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96, 2])\n",
      "tensor([[ 0.1204, -0.3356]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_batch = loaded_model(X[\"context_values\"])\n",
    "print(output_batch.prediction_logits.shape)\n",
    "print(output_batch.prediction_logits[0, :1, :7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c376de",
   "metadata": {},
   "source": [
    "### register model under Auto Classes and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26e6cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoModelForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "209797e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoConfig.register(\"patchtsmixer\", PatchTSMixerConfig)\n",
    "AutoModelForPreTraining.register(PatchTSMixerConfig, PatchTSMixerForPretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "606e26ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file pytest_data/etth1/patchtsmixer_pretrained_etth1/config.json\n",
      "Model config PatchTSMixerConfig {\n",
      "  \"_name_or_path\": \"pytest_data/etth1/patchtsmixer_pretrained_etth1\",\n",
      "  \"architectures\": [\n",
      "    \"PatchTSMixerForPretraining\"\n",
      "  ],\n",
      "  \"channel_consistent_masking\": true,\n",
      "  \"d_size\": \"4D\",\n",
      "  \"dropout\": 0.5,\n",
      "  \"expansion_factor\": 3,\n",
      "  \"forecast_channel_indices\": null,\n",
      "  \"forecast_len\": 96,\n",
      "  \"gated_attn\": true,\n",
      "  \"head_agg\": null,\n",
      "  \"head_dropout\": 0.7,\n",
      "  \"in_channels\": 7,\n",
      "  \"init_std\": 0.02,\n",
      "  \"mask_input\": true,\n",
      "  \"mask_mode\": \"mask_before_encoder\",\n",
      "  \"mask_patch_ratios\": [\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"mask_patches\": [\n",
      "    2,\n",
      "    3\n",
      "  ],\n",
      "  \"mask_ratio\": 0.5,\n",
      "  \"mask_type\": \"random\",\n",
      "  \"mask_value\": 0,\n",
      "  \"masked_loss\": false,\n",
      "  \"mode\": \"common_channel\",\n",
      "  \"model_type\": \"patchtsmixer\",\n",
      "  \"n_classes\": 3,\n",
      "  \"n_targets\": 3,\n",
      "  \"norm_mlp\": \"LayerNorm\",\n",
      "  \"num_features\": 48,\n",
      "  \"num_layers\": 2,\n",
      "  \"num_patches\": 32,\n",
      "  \"output_range\": null,\n",
      "  \"patch_last\": true,\n",
      "  \"patch_len\": 16,\n",
      "  \"revin\": true,\n",
      "  \"seed_number\": 42,\n",
      "  \"self_attn\": false,\n",
      "  \"self_attn_heads\": 1,\n",
      "  \"seq_len\": 512,\n",
      "  \"stride\": 16,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.32.0.dev0\"\n",
      "}\n",
      "\n",
      "loading weights file pytest_data/etth1/patchtsmixer_pretrained_etth1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing PatchTSMixerForPretraining.\n",
      "\n",
      "All the weights of PatchTSMixerForPretraining were initialized from the model checkpoint at pytest_data/etth1/patchtsmixer_pretrained_etth1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PatchTSMixerForPretraining for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "auto_model = AutoModelForPreTraining.from_pretrained('pytest_data/etth1/patchtsmixer_pretrained_etth1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae7a9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3eed605a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2044,  0.2675,  0.3120,  ...,  0.6149,  0.7067,  0.7208],\n",
       "          [ 0.1787,  0.0317, -0.1542,  ...,  0.3104,  0.4169,  0.5194],\n",
       "          [ 0.5177,  0.5340,  0.5295,  ..., -1.2368, -1.0592, -0.8515],\n",
       "          ...,\n",
       "          [ 0.8223,  0.8504,  0.8444,  ..., -2.4864, -2.1504, -1.7067],\n",
       "          [-0.2432, -0.0148,  0.2164,  ...,  1.0069,  0.9533,  0.8355],\n",
       "          [-0.3156, -0.4576, -0.6203,  ...,  0.3079,  0.2728,  0.2093]],\n",
       "\n",
       "         [[-1.4562, -1.5809, -1.6935,  ..., -0.9289, -0.9019, -0.8608],\n",
       "          [-0.6807, -0.7484, -0.7978,  ..., -1.8741, -1.4803, -1.0297],\n",
       "          [-0.5957, -0.6374, -0.7216,  ..., -1.2122, -1.4055, -1.4631],\n",
       "          ...,\n",
       "          [ 1.0290,  1.0744,  1.0683,  ...,  1.5893,  1.1897,  0.7693],\n",
       "          [-0.4916, -0.5638, -0.5949,  ...,  0.9490,  0.7572,  0.6340],\n",
       "          [ 0.6909,  0.7657,  0.8114,  ...,  0.0976,  0.1482,  0.1988]],\n",
       "\n",
       "         [[ 0.1302,  0.2059,  0.2690,  ...,  0.6550,  0.6677,  0.6167],\n",
       "          [ 0.0648, -0.0913, -0.2848,  ...,  0.3134,  0.4373,  0.5519],\n",
       "          [ 0.6755,  0.7137,  0.7168,  ..., -1.1592, -0.9549, -0.7341],\n",
       "          ...,\n",
       "          [ 0.7546,  0.8016,  0.8107,  ..., -2.4445, -2.0837, -1.6390],\n",
       "          [-0.2863, -0.0901,  0.1053,  ...,  0.8942,  0.8205,  0.6875],\n",
       "          [-0.3892, -0.5322, -0.6955,  ...,  0.2305,  0.2121,  0.1617]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.6883,  0.6448,  0.4847,  ..., -0.6192, -0.1108,  0.2675],\n",
       "          [ 0.2608,  0.4606,  0.6310,  ..., -0.4896, -0.9174, -1.1833],\n",
       "          [-1.7339, -1.9073, -2.0019,  ..., -0.5802, -0.8266, -0.9302],\n",
       "          ...,\n",
       "          [ 0.4687,  0.4622,  0.4179,  ..., -0.0406, -0.2835, -0.3843],\n",
       "          [ 0.8530,  1.0554,  1.3432,  ...,  1.4806,  1.7205,  1.8172],\n",
       "          [ 0.7513,  0.7717,  0.7575,  ...,  0.5641,  0.4505,  0.3613]],\n",
       "\n",
       "         [[-1.8599, -2.1218, -2.3585,  ..., -2.4653, -2.3315, -2.1221],\n",
       "          [-1.9968, -2.0746, -2.0972,  ..., -2.8616, -2.8092, -2.6652],\n",
       "          [-2.7261, -2.8704, -2.9225,  ..., -2.2985, -2.3120, -2.2925],\n",
       "          ...,\n",
       "          [ 0.4714,  0.5239,  0.5747,  ...,  0.6737,  0.6145,  0.5503],\n",
       "          [ 0.6527,  0.6699,  0.7331,  ...,  0.9995,  0.8773,  0.7728],\n",
       "          [ 0.8333,  0.9046,  0.9397,  ...,  0.3787,  0.3476,  0.3253]],\n",
       "\n",
       "         [[ 0.3675,  0.3271,  0.2799,  ...,  0.3872,  0.3079,  0.2668],\n",
       "          [ 0.0984,  0.0200, -0.0284,  ..., -1.2012, -1.2199, -1.0412],\n",
       "          [-0.5715, -0.5714, -0.5397,  ...,  0.1490,  0.2129,  0.2271],\n",
       "          ...,\n",
       "          [ 0.3375,  0.3105,  0.2718,  ...,  1.2191,  1.3026,  1.2668],\n",
       "          [ 1.3589,  1.3350,  1.2164,  ...,  0.5928,  0.3878,  0.2589],\n",
       "          [ 0.4982,  0.6000,  0.7148,  ...,  0.5737,  0.4856,  0.4117]]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# auto_model(test_data.to('mps')).prediction_logits\n",
    "auto_model(dset_test.__getitem__(0)['context_values'].reshape(1,seq_len,n_features)).prediction_logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
