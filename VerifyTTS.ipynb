{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 TTS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- `checkpoint_utils.py`: comment out a bunch of the loading stuff because of missing keys in the checkpoint\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `xvectors.zip` from https://drive.google.com/uc?export=download&id=16QOUURZBrW7-GYbVG_gXt3mTMlZmQoH0\n",
    "\n",
    "- everything from https://huggingface.co/mechanicalsea/speecht5-tts\n",
    "\n",
    "- https://github.com/kan-bayashi/ParallelWaveGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60213c02",
   "metadata": {},
   "source": [
    "## Tokenize input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e74ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the original tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b145f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run our own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768371be",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = torch.tensor([[\n",
    "    4, 35,  5, 22, 23,  4, 20, 11,  7,  6, 31, 12,  4, 16, 24, 23,  4, 35,\n",
    "         16, 21, 21, 10,  9, 21,  4, 55,  7, 17,  5, 41,  4, 32, 11, 10, 12,  4,\n",
    "         10, 12,  4, 34, 24,  5,  5, 17, 11,  4,  6,  5,  5,  4, 19, 10, 27,  5,\n",
    "          4,  6,  7, 15, 28, 10,  9, 21,  4,  6,  8,  4, 22,  8, 16,  4, 19,  8,\n",
    "         13,  4, 13,  5,  7, 15, 26\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3f9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": src_tokens,\n",
    "#    \"attention_mask\": torch.ones_like(src_tokens),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20cde14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test with attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9bfaeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389f72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint from https://huggingface.co/mechanicalsea/speecht5-tts\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task t2s \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_tts.pt\n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForConditionalGeneration, \n",
    "    SpeechT5ForCTC, \n",
    "    SpeechT5ForTTS,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = SpeechT5Config()\n",
    "# hf_model = SpeechT5Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_tts.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SpeechT5ForTTS.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForTTS(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5EncoderWithTextPrenet(\n",
       "      (prenet): SpeechT5TextEncoderPrenet(\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (encode_positions): SpeechT5ScaledPositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5DecoderWithSpeechPrenet(\n",
       "      (prenet): SpeechT5SpeechDecoderPrenet(\n",
       "        (layer1): Linear(in_features=80, out_features=256, bias=True)\n",
       "        (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (layer3): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (encode_positions): SpeechT5ScaledPositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (speaker_embeds_layer): Linear(in_features=1280, out_features=768, bias=True)\n",
       "      )\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (speech_decoder_postnet): SpeechT5SpeechDecoderPostnet(\n",
       "    (feat_out): Linear(in_features=768, out_features=160, bias=True)\n",
       "    (prob_out): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): SpeechT5BatchNormConvLayer(\n",
       "        (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): Tanh()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): SpeechT5BatchNormConvLayer(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): Tanh()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): SpeechT5BatchNormConvLayer(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): Tanh()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (3): SpeechT5BatchNormConvLayer(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): Tanh()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (4): SpeechT5BatchNormConvLayer(\n",
       "        (conv): Conv1d(256, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (batch_norm): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09c1c4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_tts.pt were not used when initializing SpeechT5Model: ['speecht5.encoder.prenet.encode_positions.alpha', 'speecht5.encoder.prenet.encode_positions.pe', 'speecht5.encoder.prenet.embed_tokens.weight']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading should work OK for class `SpeechT5Model` too:\n",
    "hf_model_naked = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046bda1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "See the script `generate_speech.py` from the SpeechT5 repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24e0f251",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea4b4380",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"t2s\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a79fbfdf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9b2818e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/speecht5-tts/speecht5_tts.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda7a201",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.speecht5.T5TransformerModel"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64d53b18",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.modules.encoder.TransformerEncoder"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bad44136",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c1faddb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.data.encoders.sentencepiece_bpe.SentencepieceBPE"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.data import encoders\n",
    "from argparse import Namespace\n",
    "tokenizer = encoders.build_bpe(\n",
    "    Namespace(\n",
    "        bpe='sentencepiece', \n",
    "        sentencepiece_model='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/MODEL_DIR/spm_char.model'\n",
    "    )\n",
    ")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd5619",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# orig_model.decoder.layers[0].encoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Verify text encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `text_encoder_prenet` converts the input tokens into embeddings of shape `(batch, sequence_length, 768)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e6cce50",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_input, encoder_padding_mask = orig_model.text_encoder_prenet(src_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26c4a65d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 79, 768]), torch.Size([1, 79]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "058de538",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0383,  0.0826,  0.0607,  ..., -0.0333, -0.1770,  0.0203],\n",
       "         [ 0.0857,  0.0239,  0.2346,  ...,  0.1588,  0.0084,  0.0664],\n",
       "         [ 0.0644,  0.0361,  0.2011,  ..., -0.0519, -0.1996,  0.0747],\n",
       "         ...,\n",
       "         [ 0.1361,  0.1569, -0.1022,  ...,  0.1605,  0.0147,  0.0804],\n",
       "         [ 0.2444, -0.0554, -0.0303,  ...,  0.0666,  0.0143, -0.0211],\n",
       "         [ 0.0408, -0.0834,  0.1128,  ...,  0.1641, -0.0404,  0.0649]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c07884f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e992b36",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Run Hugging Face model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1877608f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.prenet(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4fd3c9d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([79, 768])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in hf_outputs if hasattr(x, \"shape\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "addc344c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0383,  0.0826,  0.0607,  ..., -0.0333, -0.1770,  0.0203],\n",
       "        [ 0.0857,  0.0239,  0.2346,  ...,  0.1588,  0.0084,  0.0664],\n",
       "        [ 0.0644,  0.0361,  0.2011,  ..., -0.0519, -0.1996,  0.0747],\n",
       "        ...,\n",
       "        [ 0.1361,  0.1569, -0.1022,  ...,  0.1605,  0.0147,  0.0804],\n",
       "        [ 0.2444, -0.0554, -0.0303,  ...,  0.0666,  0.0143, -0.0211],\n",
       "        [ 0.0408, -0.0834,  0.1128,  ...,  0.1641, -0.0404,  0.0649]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adcd7a32",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hf_encoder_input = hf_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b099ad3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_input - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5533cb26",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc87193ec40>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.abs(encoder_input - hf_outputs[0]).numpy()[0] > 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47c86f72",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run it with the original's speech prenet input:\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b8b4df6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Run it with our input, which is slightly different (see above)\n",
    "# with torch.no_grad():\n",
    "#     encoder_output = orig_model.encoder(hf_encoder_input, ~hf_encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41ff76cc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([79, 1, 768])\n",
      "encoder_padding_mask shape torch.Size([1, 79])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([79, 1, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a38979d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9b9dda4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0323,  0.0410,  0.0620,  ...,  0.0133,  0.0218, -0.0187],\n",
       "         [ 0.0523, -0.0873, -0.0304,  ..., -0.0184, -0.0473,  0.0077],\n",
       "         [ 0.0937, -0.0637, -0.0121,  ..., -0.0029, -0.0065, -0.0139],\n",
       "         ...,\n",
       "         [-0.1147,  0.0520,  0.0286,  ...,  0.0440,  0.0449,  0.1282],\n",
       "         [-0.1129,  0.0273, -0.0115,  ...,  0.0530,  0.0472,  0.1208],\n",
       "         [ 0.0075, -0.0139,  0.0873,  ...,  0.0442, -0.0180, -0.0587]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cde6dbda",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use exact same inputs as the original model:\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speecht5(\n",
    "#          inputs_embeds=encoder_input,\n",
    "#          attention_mask=(~encoder_padding_mask),\n",
    "#      )\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fefcf62e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28f529a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.wrapped_encoder(\n",
    "         hidden_states=hf_encoder_input,\n",
    "         attention_mask=torch.ones_like(src_tokens),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb12e71a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06fb675b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8fe9914e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69eda6f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a747e547",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c950849",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Full encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffdcbec5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputs = src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52844106",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test batch\n",
    "# inputs = torch.tile(src_tokens, (2, 1))\n",
    "# inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f038415",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_out = orig_model.forward_text_encoder(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28782edd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "865dbb0e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aea13537",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO: what happened to my batch here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33848c76",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder(\n",
    "         input_values=src_tokens,\n",
    "         attention_mask=torch.ones_like(src_tokens),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40060f6d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3486fd24",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba01bb7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Speaker embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f7c06",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#         if spkembs is not None and self.spk_embed_integration_type != \"pre\":\n",
    "#             encoder_out[\"encoder_out\"] = [self._integrate_with_spk_embed(\n",
    "#                 encoder_out[\"encoder_out\"][0].transpose(0, 1), spkembs\n",
    "#             ).transpose(0, 1)]\n",
    "#             spkembs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877ecf8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e143d69",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bed91",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#         threshold = kwargs.get(\"threshold\", 0.5)\n",
    "#         minlenratio = kwargs.get(\"threshold\", 0.0)\n",
    "\n",
    "#         if source is None:\n",
    "#             assert src_tokens.size(0) == 1\n",
    "#             encoder_out = self.forward_text_encoder(src_tokens)\n",
    "#             maxlenratio = kwargs.get(\"threshold\", 20.0)\n",
    "#         else:\n",
    "#             assert source.size(0) == 1\n",
    "#             encoder_out = self.forward_encoder(source, padding_mask=kwargs[\"padding_mask\"])\n",
    "#             maxlenratio = kwargs.get(\"threshold\", 10.0)\n",
    "\n",
    "#         if spkembs is not None and self.spk_embed_integration_type != \"pre\":\n",
    "#             encoder_out[\"encoder_out\"] = [self._integrate_with_spk_embed(\n",
    "#                 encoder_out[\"encoder_out\"][0].transpose(0, 1), spkembs\n",
    "#             ).transpose(0, 1)]\n",
    "#             spkembs = None\n",
    "\n",
    "#         maxlen = int(encoder_out[\"encoder_out\"][0].size(0) * maxlenratio / self.reduction_factor)\n",
    "#         minlen = int(encoder_out[\"encoder_out\"][0].size(0) * minlenratio / self.reduction_factor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1932bcf1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spkembs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Verify speech decoder prenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ef95457",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#idx = 0\n",
    "#outs, probs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0950265",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 80])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = encoder_out[\"encoder_out\"][0].new_zeros(1, 1, orig_model.speech_decoder_postnet.odim)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "279a02a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO: just for testing\n",
    "# ys = torch.randn(7, 3, 80)\n",
    "# spkembs = torch.randn(7, 512)\n",
    "\n",
    "ys = torch.randn(1, 1, 80)\n",
    "spkembs = torch.randn(1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc99b511",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_in, _ = orig_model.speech_decoder_prenet(ys, spkembs=spkembs)\n",
    "    decoder_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "86cf9ecd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 1.3234e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          8.8437e-02, 0.0000e+00, 4.6974e-02, 0.0000e+00, 0.0000e+00,\n",
       "          3.9721e-01, 0.0000e+00, 0.0000e+00, 2.7208e-01, 1.8816e-01,\n",
       "          0.0000e+00, 1.3635e-02, 0.0000e+00, 0.0000e+00, 9.8454e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.8131e-01, 0.0000e+00, 2.0238e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4409e-02, 0.0000e+00,\n",
       "          0.0000e+00, 1.5188e+00, 0.0000e+00, 2.3420e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 7.9868e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2511e-01, 2.1177e-01,\n",
       "          0.0000e+00, 9.6839e-02, 0.0000e+00, 0.0000e+00, 1.5289e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1648e-01, 0.0000e+00,\n",
       "          0.0000e+00, 4.0492e-01, 0.0000e+00, 5.6434e-01, 1.8852e-02,\n",
       "          9.6049e-02, 4.0857e-02, 0.0000e+00, 5.8210e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.7559e-01, 2.2903e-01, 8.8451e-03,\n",
       "          9.9987e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          5.7909e-02, 4.9711e-01, 0.0000e+00, 4.8095e-01, 1.0536e-01,\n",
       "          2.0112e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2584e-02,\n",
       "          0.0000e+00, 1.0763e-01, 0.0000e+00, 8.4339e-02, 4.8436e-02,\n",
       "          7.7591e-01, 1.9546e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.0702e-01, 0.0000e+00, 3.5751e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.2148e-01, 2.6020e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 3.6313e-01, 3.1732e-01, 0.0000e+00,\n",
       "          0.0000e+00, 9.7902e-02, 2.3956e-01, 2.2280e-01, 0.0000e+00,\n",
       "          0.0000e+00, 7.4291e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.1354e+00, 0.0000e+00, 0.0000e+00, 1.1253e-01, 0.0000e+00,\n",
       "          1.7945e-01, 8.2373e-01, 3.0497e-01, 1.9662e-01, 0.0000e+00,\n",
       "          0.0000e+00, 8.4477e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5047e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6884e-01,\n",
       "          2.8889e-01, 1.2474e-01, 2.7413e-01, 4.2818e-01, 0.0000e+00,\n",
       "          1.4809e-01, 1.1371e-01, 0.0000e+00, 3.4869e-01, 1.7958e-01,\n",
       "          0.0000e+00, 0.0000e+00, 3.4874e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          6.6061e-01, 0.0000e+00, 0.0000e+00, 2.7317e-01, 0.0000e+00,\n",
       "          3.2944e-01, 6.8583e-01, 0.0000e+00, 0.0000e+00, 1.0607e+00,\n",
       "          0.0000e+00, 1.4825e+00, 5.5358e-01, 1.8391e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.9706e-01, 3.0256e-01, 0.0000e+00, 0.0000e+00, 2.1250e-01,\n",
       "          1.8277e-01, 1.7997e-01, 2.7930e-01, 9.6937e-02, 3.8584e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7455e-02,\n",
       "          0.0000e+00, 1.8308e-01, 0.0000e+00, 0.0000e+00, 1.6233e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.3055e-01, 7.4128e-02, 2.2989e-01,\n",
       "          0.0000e+00, 9.5620e-01, 0.0000e+00, 2.6867e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6912e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.3574e-01, 4.3083e-01, 0.0000e+00, 6.9622e-02, 2.6928e-01,\n",
       "          0.0000e+00, 6.4011e-02, 0.0000e+00, 2.4209e-01, 3.4207e-01,\n",
       "          0.0000e+00, 5.6814e-01, 0.0000e+00, 0.0000e+00, 2.2371e-01,\n",
       "          4.8289e-02, 1.5147e-01, 6.1988e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 3.0262e-01, 0.0000e+00, 0.0000e+00, 7.3088e-01,\n",
       "          3.1118e-01, 3.3174e-01, 0.0000e+00, 3.8982e-01, 6.3264e-01,\n",
       "          2.3055e-01, 0.0000e+00, 3.0390e-01, 8.7524e-01, 1.4918e-02,\n",
       "          0.0000e+00, 3.8652e-01, 0.0000e+00, 1.1625e-01, 0.0000e+00,\n",
       "          1.3414e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.1856e+00, 1.5038e-01, 5.3634e-01, 1.9511e-03, 0.0000e+00,\n",
       "          8.0927e-01, 2.2451e+00, 0.0000e+00, 2.0692e-01, 1.2439e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.6421e-01, 5.8453e-01, 2.0095e-01,\n",
       "          0.0000e+00, 0.0000e+00, 4.4920e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.7348e-01, 0.0000e+00, 1.7426e-01,\n",
       "          0.0000e+00, 5.5965e-01, 0.0000e+00, 2.2199e-01, 2.8098e-01,\n",
       "          2.9397e-01, 0.0000e+00, 7.9966e-04, 5.1662e-01, 0.0000e+00,\n",
       "          5.7304e-01, 0.0000e+00, 1.5161e-01, 0.0000e+00, 2.1593e-01,\n",
       "          0.0000e+00, 3.5741e-01, 0.0000e+00, 0.0000e+00, 4.9213e-01,\n",
       "          5.2289e-01, 2.8582e-01, 0.0000e+00, 7.5205e-01, 0.0000e+00,\n",
       "          2.6234e-01, 3.2730e-02, 0.0000e+00, 0.0000e+00, 2.1259e-01,\n",
       "          3.7000e-01, 0.0000e+00, 3.8825e-01, 0.0000e+00, 3.6954e-01,\n",
       "          7.3771e-01, 0.0000e+00, 2.9649e-01, 0.0000e+00, 1.3259e-01,\n",
       "          1.1771e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1485e-02,\n",
       "          0.0000e+00, 5.4283e-02, 0.0000e+00, 1.1009e-01, 5.8892e-02,\n",
       "          5.0014e-02, 3.3234e-01, 2.8860e-01, 2.0349e-01, 8.6731e-02,\n",
       "          2.6361e-01, 1.2976e+00, 9.2180e-02, 4.3307e-01, 0.0000e+00,\n",
       "          1.4599e-01, 3.4386e-01, 0.0000e+00, 0.0000e+00, 1.1230e+00,\n",
       "          4.7649e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.0422e-01, 6.3824e-01, 3.6737e-01,\n",
       "          0.0000e+00, 0.0000e+00, 1.5880e-01, 4.5185e-01, 0.0000e+00,\n",
       "          0.0000e+00, 6.1155e-02, 6.9762e-01, 1.2726e-01, 1.2660e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3474e-01, 2.1599e-01,\n",
       "          5.0533e-01, 1.9885e-01, 8.6599e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 7.9508e-01, 1.6213e-01, 4.3493e-01,\n",
       "          4.6281e-01, 0.0000e+00, 0.0000e+00, 7.6681e-01, 8.6590e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 5.4461e-02, 2.8014e-02, 0.0000e+00, 3.7728e-01,\n",
       "          3.9451e-01, 0.0000e+00, 2.7770e-02, 0.0000e+00, 2.4735e-01,\n",
       "          1.0729e+00, 0.0000e+00, 2.6310e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1343e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.3103e-01, 1.0005e+00, 0.0000e+00, 0.0000e+00, 9.6120e-02,\n",
       "          0.0000e+00, 5.3098e-01, 0.0000e+00, 8.1382e-02, 0.0000e+00,\n",
       "          1.9583e-01, 0.0000e+00, 0.0000e+00, 2.0303e-01, 0.0000e+00,\n",
       "          3.5989e-01, 0.0000e+00, 1.5074e-01, 0.0000e+00, 0.0000e+00,\n",
       "          1.2172e-01, 0.0000e+00, 2.1042e-01, 0.0000e+00, 0.0000e+00,\n",
       "          2.4478e-01, 2.5500e-01, 2.1650e-01, 3.4263e-01, 0.0000e+00,\n",
       "          1.1337e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.3923e-01, 1.1501e+00, 2.6337e-01, 0.0000e+00,\n",
       "          3.9570e-01, 3.5339e-02, 5.1617e-01, 4.8422e-01, 0.0000e+00,\n",
       "          2.5432e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.1141e-01, 0.0000e+00, 0.0000e+00,\n",
       "          1.7068e-01, 1.2932e-01, 2.5020e-01, 0.0000e+00, 0.0000e+00,\n",
       "          2.7098e-01, 0.0000e+00, 3.7763e-01, 0.0000e+00, 2.3777e-01,\n",
       "          0.0000e+00, 2.9605e-01, 3.7125e-01, 1.0003e+00, 0.0000e+00,\n",
       "          4.0371e-01, 0.0000e+00, 0.0000e+00, 4.5872e-01, 0.0000e+00,\n",
       "          0.0000e+00, 1.4038e-01, 2.0300e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.7197e-01, 0.0000e+00, 0.0000e+00,\n",
       "          1.1434e+00, 0.0000e+00, 0.0000e+00, 4.7824e-01, 6.5272e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.4512e-01, 4.0559e-01, 3.8296e-02, 0.0000e+00,\n",
       "          1.5695e-01, 4.0476e-01, 0.0000e+00, 0.0000e+00, 2.5209e-01,\n",
       "          0.0000e+00, 1.4155e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.6174e-01, 0.0000e+00, 5.5387e-01,\n",
       "          1.8834e-01, 2.0587e-01, 0.0000e+00, 0.0000e+00, 9.5900e-02,\n",
       "          0.0000e+00, 0.0000e+00, 4.5855e-02, 0.0000e+00, 3.2964e-01,\n",
       "          0.0000e+00, 2.5684e-01, 0.0000e+00, 2.0980e-01, 2.5320e-01,\n",
       "          6.6466e-02, 0.0000e+00, 1.6998e-01, 0.0000e+00, 0.0000e+00,\n",
       "          2.3225e-01, 0.0000e+00, 1.0586e-01, 0.0000e+00, 0.0000e+00,\n",
       "          3.6043e-01, 0.0000e+00, 3.8906e-01, 2.3169e+00, 0.0000e+00,\n",
       "          1.8726e+00, 0.0000e+00, 4.4794e-01, 0.0000e+00, 2.5777e-01,\n",
       "          0.0000e+00, 7.1327e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.9275e-01, 2.0665e-01, 0.0000e+00, 0.0000e+00,\n",
       "          4.2312e-02, 0.0000e+00, 1.3061e+00, 1.3863e-01, 0.0000e+00,\n",
       "          5.6037e-01, 1.0371e-02, 0.0000e+00, 0.0000e+00, 2.6787e-01,\n",
       "          0.0000e+00, 0.0000e+00, 9.9167e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 8.0805e-01, 1.4907e+00, 1.1924e-01, 4.7766e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4687e-01, 0.0000e+00,\n",
       "          3.1811e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.3462e-01, 1.7323e-01, 0.0000e+00, 4.7619e-01, 1.3953e-01,\n",
       "          3.5195e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 5.9150e-01, 0.0000e+00, 0.0000e+00,\n",
       "          3.9763e-01, 0.0000e+00, 2.6312e-01, 0.0000e+00, 3.6270e-01,\n",
       "          1.7879e-01, 1.2513e+00, 7.4481e-01, 3.1830e-01, 1.9935e-02,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4600e-01,\n",
       "          0.0000e+00, 0.0000e+00, 1.3866e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 7.8526e-01, 2.6868e-01, 0.0000e+00,\n",
       "          9.6304e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7987e-01, 0.0000e+00,\n",
       "          0.0000e+00, 2.2697e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.7788e-01, 0.0000e+00, 4.4953e-02, 2.3777e-01,\n",
       "          0.0000e+00, 0.0000e+00, 2.1367e-02, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.0760e+00, 0.0000e+00, 2.1853e+00,\n",
       "          0.0000e+00, 3.0946e-01, 0.0000e+00, 4.7445e-02, 0.0000e+00,\n",
       "          2.9511e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.0561e-01, 1.2143e-01, 0.0000e+00, 2.2401e-01, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7617e-01, 2.5877e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.6750e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5784e-01, 0.0000e+00,\n",
       "          7.7727e-02, 1.3016e-01, 4.5540e-03, 0.0000e+00, 3.2100e-01,\n",
       "          6.5368e-01, 0.0000e+00, 1.4655e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5949e-01, 1.5312e+00,\n",
       "          0.0000e+00, 4.6614e-01, 6.6836e-01, 3.4921e-01, 5.8708e-01,\n",
       "          0.0000e+00, 1.7660e-01, 2.8416e-01, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1cc023",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77134734",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.prenet(\n",
    "         input_values=ys,\n",
    "         speaker_embeddings=spkembs,\n",
    "     )\n",
    "\n",
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f7fc65dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_in - hf_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d7a7e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "559eb4d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output_tokens = decoder_in[:, -1:]\n",
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d516c43e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tgt_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05ae78d7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "incremental_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6229b43c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_out,\n",
    "        incremental_state=incremental_state,\n",
    "        alignment_layer=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a190958",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fea0351e",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05117956",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b593692b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d486f4b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.wrapped_decoder(\n",
    "         hidden_states=prev_output_tokens,\n",
    "         #attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_out[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         #encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea512cc2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'past_key_values']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d82c369a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb6282bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3e254cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_output - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d1fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceec7dd9",
   "metadata": {},
   "source": [
    "## Full decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1700a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_out = orig_model.forward_text_encoder(src_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4146fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.randn(1, 3, 80)\n",
    "spkembs = torch.randn(1, 512)\n",
    "incremental_states = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bd610771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 768]), torch.Size([1, 1, 768]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_in.shape, decoder_in[:,-1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36c4c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_in, _ = orig_model.speech_decoder_prenet(ys, spkembs=spkembs)\n",
    "    z, extra = orig_model.decoder(\n",
    "        decoder_in[:,-1:], None, encoder_out, incremental_states, alignment_layer=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9d93b58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6a048e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder(\n",
    "         input_values=ys,\n",
    "         #attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_out[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         #encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         speaker_embeddings=spkembs,\n",
    "         return_dict=True,         \n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c43d8c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c119b515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(z - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify speech decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9997d2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 768]), torch.Size([768]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape, z[0, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ca697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             outs += [self.speech_decoder_postnet.feat_out(z[0, -1]).view(self.reduction_factor, self.speech_decoder_postnet.odim)]  # [(r, odim), ...]\n",
    "#             probs += [torch.sigmoid(self.speech_decoder_postnet.prob_out(z[0, -1]))]  # [(r), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "adaabbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#orig_model.speech_decoder_postnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b54ff725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model.speech_decoder_postnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    before_outs, after_outs, logits = orig_model.speech_decoder_postnet(z)  # z[:, -1])\n",
    "    \n",
    "    # before_outs = speech_decoder_postnet.feat_out(...)\n",
    "    # after_outs = final output\n",
    "    # logits = speech_decoder_postnet.prob_out(...)  for stop token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 80]), torch.Size([1, 2, 80]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_outs.shape, after_outs.shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7d6de14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-13.0990,  -5.9124])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5b5609cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ba7571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    before_outs_hf, after_outs_hf, logits_hf = hf_model.speech_decoder_postnet(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5623a791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 80]), torch.Size([1, 2, 80]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_outs_hf.shape, after_outs_hf.shape, logits_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77004a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6eeff463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(before_outs - before_outs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "428e5908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(after_outs - after_outs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "65805d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(logits - logits_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4e9dc",
   "metadata": {},
   "source": [
    "Run the full model to make sure this doesn't give any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         decoder_input_ids=torch.tensor([[3, 4, 5]]),\n",
    "         #decoder_input_ids=torch.tensor([[3, 4, 5], [2, 2, 2]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677718c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf0244b",
   "metadata": {},
   "source": [
    "Also calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[2,  4, 18, 10, 12,  6,  5]]),\n",
    "         labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13]]),\n",
    "         #labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13], [4, 18, 10, 12,  6,  5, 13]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34dd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195961eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2ce14a",
   "metadata": {},
   "source": [
    "Generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ccdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = hf_model.generate(inputs.input_values, max_length=100)\n",
    "# hf_outputs = hf_model.generate(inputs.input_values, num_beams=5, max_length=100) #, bos_token_id=2)\n",
    "# hf_outputs = hf_model.generate(torch.rand(1, 10000), num_beams=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c80b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fce9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hf_outputs.shape[0]):\n",
    "    print(tgt_dict.string(hf_outputs[i]))\n",
    "    print(tokenizer.decode(tgt_dict.string(hf_outputs[i])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tgt_dict[x] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a116f698",
   "metadata": {},
   "source": [
    "For comparison, Speech2Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00306c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "s2t_model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "s2t_processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = s2t_processor(\n",
    "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n",
    ")\n",
    "input_features = inputs.input_features\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ef5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batch\n",
    "# input_features = torch.tile(input_features, dims=(2, 1, 1))\n",
    "# input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = s2t_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = s2t_processor.batch_decode(generated_ids)[0]\n",
    "transcription\n",
    "\n",
    "#'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a21206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Speech2TextModel\n",
    "\n",
    "s2t_model = Speech2TextModel.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50884f2",
   "metadata": {},
   "source": [
    "Test other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd38a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART, Speech2Text, Wav2Vec2 don't have pruning\n",
    "#hf_model.prune_heads({1: [0, 2], 2: [2,3 ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f954bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.resize_token_embeddings(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e8369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b945156",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_ctc.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c092a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_naked.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
