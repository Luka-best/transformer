{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cd0a9",
   "metadata": {},
   "source": [
    "# Verifying the SpeechT5 TTS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5342da",
   "metadata": {},
   "source": [
    "I needed to do the following to be able to load the original model:\n",
    "\n",
    "- Clone the https://github.com/microsoft/SpeechT5 repo\n",
    "\n",
    "Install stuff:\n",
    "\n",
    "```\n",
    "pip install editdistance\n",
    "pip install -U sacrebleu==1.5.1\n",
    "\n",
    "git submodule update --init SpeechT5/fairseq\n",
    "cd SpeechT5\n",
    "pip install --editable fairseq/\n",
    "pip install espnet\n",
    "```\n",
    "\n",
    "Hack the code:\n",
    "\n",
    "- Copy `speecht5/tasks/speecht5.py` into `fairseq/fairseq/tasks`\n",
    "\n",
    "- `checkpoint_utils.py`: comment out a bunch of the loading stuff because of missing keys in the checkpoint\n",
    "\n",
    "Additional stuff to download:\n",
    "\n",
    "- `dict.txt` from https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK\n",
    "\n",
    "- `tokenizer` from https://drive.google.com/uc?export=download&id=1wClgQjXXoU2lmpbaEa1v2SqMbg7cAutq\n",
    "\n",
    "- `xvectors.zip` from https://drive.google.com/uc?export=download&id=16QOUURZBrW7-GYbVG_gXt3mTMlZmQoH0\n",
    "\n",
    "- everything from https://huggingface.co/mechanicalsea/speecht5-tts\n",
    "\n",
    "- https://github.com/kan-bayashi/ParallelWaveGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65661728",
   "metadata": {},
   "source": [
    "Set Python path so it can find the `speecht5` and `fairseq` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5\")\n",
    "sys.path.insert(0, \"../SpeechT5/SpeechT5/fairseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60213c02",
   "metadata": {},
   "source": [
    "## Tokenize input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "75fa539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from fairseq.data import encoders\n",
    "\n",
    "bpe_tokenizer_path = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/MODEL_DIR/spm_char.model\"\n",
    "tokenizer = encoders.build_bpe(Namespace(**{\"bpe\": \"sentencepiece\", \"sentencepiece_model\": bpe_tokenizer_path}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f8409c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "src_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "src_dict.add_symbol(\"<mask>\")\n",
    "src_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(src_dict):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5d11bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.tasks.hubert_pretraining import LabelEncoder\n",
    "label_processor = LabelEncoder(src_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2df06465",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hey, what's up, Hugging Face? This is Speech tee five talking to you for real.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5b2842f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"▁ H e y , ▁ w h a t ' s ▁ u p , ▁ H u g g i n g ▁ F a c e ? ▁ T h i s ▁ i s ▁ S p e e c h ▁ t e e ▁ f i v e ▁ t a l k i n g ▁ t o ▁ y o u ▁ f o r ▁ r e a l .\""
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = tokenizer.encode(text)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "767a1346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 35,  5, 22, 23,  4, 20, 11,  7,  6, 31, 12,  4, 16, 24, 23,  4, 35,\n",
       "        16, 21, 21, 10,  9, 21,  4, 55,  7, 17,  5, 41,  4, 32, 11, 10, 12,  4,\n",
       "        10, 12,  4, 34, 24,  5,  5, 17, 11,  4,  6,  5,  5,  4, 19, 10, 27,  5,\n",
       "         4,  6,  7, 15, 28, 10,  9, 21,  4,  6,  8,  4, 22,  8, 16,  4, 19,  8,\n",
       "        13,  4, 13,  5,  7, 15, 26], dtype=torch.int32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = label_processor(label)\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768371be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_tokens = torch.tensor([[\n",
    "#     4, 35,  5, 22, 23,  4, 20, 11,  7,  6, 31, 12,  4, 16, 24, 23,  4, 35,\n",
    "#          16, 21, 21, 10,  9, 21,  4, 55,  7, 17,  5, 41,  4, 32, 11, 10, 12,  4,\n",
    "#          10, 12,  4, 34, 24,  5,  5, 17, 11,  4,  6,  5,  5,  4, 19, 10, 27,  5,\n",
    "#           4,  6,  7, 15, 28, 10,  9, 21,  4,  6,  8,  4, 22,  8, 16,  4, 19,  8,\n",
    "#          13,  4, 13,  5,  7, 15, 26\n",
    "# ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e74ef32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b145f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run our own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3f9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": src_tokens,\n",
    "#    \"attention_mask\": torch.ones_like(src_tokens),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20cde14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test with attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9bfaeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389f72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2205900b",
   "metadata": {},
   "source": [
    "## Load the Transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c0f8",
   "metadata": {},
   "source": [
    "To convert the original checkpoint weights to Transformers:\n",
    "\n",
    "First download the checkpoint from https://huggingface.co/mechanicalsea/speecht5-tts\n",
    "\n",
    "Then run the following, using your own `--checkpoint_path` and `--pytorch_dump_folder_path`:\n",
    "\n",
    "```nohighlight\n",
    "cd transformers/src/transformers/models/speecht5\n",
    "\n",
    "python convert_speecht5_original_pytorch_checkpoint_to_pytorch.py \\\n",
    "  --task t2s \\\n",
    "  --checkpoint_path /path/to/SpeechT5/speecht5_tts.pt\n",
    "  --pytorch_dump_folder_path /some/other/path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "552fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SpeechT5Config, \n",
    "    SpeechT5CTCTokenizer,\n",
    "    SpeechT5Processor,\n",
    "    SpeechT5Model, \n",
    "    SpeechT5ForConditionalGeneration, \n",
    "    SpeechT5ForCTC, \n",
    "    SpeechT5ForTTS,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aa6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = SpeechT5Config()\n",
    "# hf_model = SpeechT5Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1ba8f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_tts.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5df4bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SpeechT5ForTTS.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b7dd7606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5ForTTS(\n",
       "  (speecht5): SpeechT5Model(\n",
       "    (encoder): SpeechT5EncoderWithTextPrenet(\n",
       "      (prenet): SpeechT5TextEncoderPrenet(\n",
       "        (embed_tokens): Embedding(81, 768, padding_idx=1)\n",
       "        (encode_positions): SpeechT5ScaledPositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (wrapped_encoder): SpeechT5Encoder(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): SpeechT5EncoderLayer(\n",
       "            (attention): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (embed_positions): SpeechT5RelativePositionalEncoding(\n",
       "          (pe_k): Embedding(320, 64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): SpeechT5DecoderWithSpeechPrenet(\n",
       "      (prenet): SpeechT5SpeechDecoderPrenet(\n",
       "        (layer1): Linear(in_features=80, out_features=256, bias=True)\n",
       "        (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (layer3): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (encode_positions): SpeechT5ScaledPositionalEncoding(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (speaker_embeds_layer): Linear(in_features=1280, out_features=768, bias=True)\n",
       "      )\n",
       "      (wrapped_decoder): SpeechT5Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): SpeechT5DecoderLayer(\n",
       "            (self_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): SpeechT5Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): SpeechT5FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (speech_decoder_postnet): SpeechT5SpeechDecoderPostnet(\n",
       "    (feat_out): Linear(in_features=768, out_features=160, bias=True)\n",
       "    (prob_out): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): SpeechT5BatchNormConvLayer(\n",
       "        (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): Tanh()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (1): SpeechT5BatchNormConvLayer(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): Tanh()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (2): SpeechT5BatchNormConvLayer(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): Tanh()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (3): SpeechT5BatchNormConvLayer(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): Tanh()\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "      (4): SpeechT5BatchNormConvLayer(\n",
       "        (conv): Conv1d(256, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (batch_norm): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "09c1c4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5_tts.pt were not used when initializing SpeechT5Model: ['speecht5.decoder.prenet.speaker_embeds_layer.bias', 'speecht5.decoder.prenet.layer3.weight', 'speecht5.decoder.prenet.encode_positions.pe', 'speecht5.decoder.prenet.layer3.bias', 'speech_decoder_postnet.layers.2.batch_norm.num_batches_tracked', 'speech_decoder_postnet.layers.4.batch_norm.bias', 'speecht5.encoder.prenet.encode_positions.alpha', 'speech_decoder_postnet.layers.1.batch_norm.running_var', 'speech_decoder_postnet.layers.2.batch_norm.weight', 'speech_decoder_postnet.layers.1.batch_norm.num_batches_tracked', 'speech_decoder_postnet.layers.4.batch_norm.num_batches_tracked', 'speech_decoder_postnet.layers.2.batch_norm.running_var', 'speech_decoder_postnet.layers.2.batch_norm.bias', 'speech_decoder_postnet.feat_out.weight', 'speech_decoder_postnet.layers.4.batch_norm.running_mean', 'speech_decoder_postnet.layers.3.batch_norm.running_var', 'speech_decoder_postnet.layers.4.conv.weight', 'speech_decoder_postnet.layers.0.batch_norm.running_mean', 'speech_decoder_postnet.layers.1.batch_norm.running_mean', 'speech_decoder_postnet.layers.3.batch_norm.num_batches_tracked', 'speech_decoder_postnet.layers.4.batch_norm.weight', 'speecht5.encoder.prenet.encode_positions.pe', 'speech_decoder_postnet.feat_out.bias', 'speech_decoder_postnet.layers.3.conv.weight', 'speech_decoder_postnet.layers.2.batch_norm.running_mean', 'speech_decoder_postnet.layers.4.batch_norm.running_var', 'speecht5.decoder.prenet.layer1.weight', 'speecht5.decoder.prenet.encode_positions.alpha', 'speech_decoder_postnet.layers.1.conv.weight', 'speech_decoder_postnet.layers.0.conv.weight', 'speech_decoder_postnet.layers.0.batch_norm.bias', 'speech_decoder_postnet.layers.2.conv.weight', 'speech_decoder_postnet.layers.1.batch_norm.bias', 'speecht5.decoder.prenet.layer2.bias', 'speecht5.decoder.prenet.layer2.weight', 'speech_decoder_postnet.layers.3.batch_norm.weight', 'speech_decoder_postnet.layers.0.batch_norm.weight', 'speech_decoder_postnet.prob_out.bias', 'speech_decoder_postnet.layers.1.batch_norm.weight', 'speech_decoder_postnet.layers.3.batch_norm.bias', 'speech_decoder_postnet.layers.3.batch_norm.running_mean', 'speech_decoder_postnet.layers.0.batch_norm.running_var', 'speech_decoder_postnet.prob_out.weight', 'speech_decoder_postnet.layers.0.batch_norm.num_batches_tracked', 'speecht5.decoder.prenet.layer1.bias', 'speecht5.encoder.prenet.embed_tokens.weight', 'speecht5.decoder.prenet.speaker_embeds_layer.weight']\n",
      "- This IS expected if you are initializing SpeechT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading should work OK for class `SpeechT5Model` too:\n",
    "hf_model_naked = SpeechT5Model.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64823660",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046bda1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "See the script `generate_speech.py` from the SpeechT5 repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1697ab0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load the dictionary. This adds `<s>, <pad>, </s>, <unk>` tokens to the front and `<mask>` and `<ctc_blank>` to the end. **dict.txt** was [downloaded from here](https://drive.google.com/uc?export=download&id=19hcQ58RHZ6CssxF8Qp6yEF1NW_AXxObK). This is the Vocabulary link from the main SpeechT5 README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24e0f251",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 81\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import Dictionary\n",
    "tgt_dict = Dictionary.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/DATA_ROOT/dict.txt\")\n",
    "tgt_dict.add_symbol(\"<mask>\")\n",
    "tgt_dict.add_symbol(\"<ctc_blank>\")\n",
    "print(f\"dictionary size: \" f\"{len(tgt_dict):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85935250",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To load the model we need the `SpeechT5Task` object but constructing it is annoying. Fortunately, `build_model` only reads two properties from the task object, so we can fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea4b4380",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FakeTask:\n",
    "    def __init__(self):\n",
    "        self.dicts = { \"text\": tgt_dict }\n",
    "        self.t5_task = \"t2s\"\n",
    "        \n",
    "task = FakeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc6c7a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load the fine-tuned ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a79fbfdf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from speecht5.models.speecht5 import T5TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9b2818e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../weights/speecht5-tts/speecht5_tts.pt\")\n",
    "\n",
    "orig_model = T5TransformerModel.build_model(checkpoint[\"cfg\"][\"model\"], task)\n",
    "\n",
    "orig_model.load_state_dict(checkpoint[\"model\"])\n",
    "orig_model = orig_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda7a201",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.speecht5.T5TransformerModel"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64d53b18",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speecht5.models.modules.encoder.TransformerEncoder"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orig_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bad44136",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#list(checkpoint[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c1faddb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.data.encoders.sentencepiece_bpe.SentencepieceBPE"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.data import encoders\n",
    "from argparse import Namespace\n",
    "tokenizer = encoders.build_bpe(\n",
    "    Namespace(\n",
    "        bpe='sentencepiece', \n",
    "        sentencepiece_model='/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/MODEL_DIR/spm_char.model'\n",
    "    )\n",
    ")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd5619",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# orig_model.decoder.layers[0].encoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56503f04",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Verify text encoder prenet output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f9ff",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `text_encoder_prenet` converts the input tokens into embeddings of shape `(batch, sequence_length, 768)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e6cce50",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_input, encoder_padding_mask = orig_model.text_encoder_prenet(src_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26c4a65d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 79, 768]), torch.Size([1, 79]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape, encoder_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "058de538",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0383,  0.0826,  0.0607,  ..., -0.0333, -0.1770,  0.0203],\n",
       "         [ 0.0857,  0.0239,  0.2346,  ...,  0.1588,  0.0084,  0.0664],\n",
       "         [ 0.0644,  0.0361,  0.2011,  ..., -0.0519, -0.1996,  0.0747],\n",
       "         ...,\n",
       "         [ 0.1361,  0.1569, -0.1022,  ...,  0.1605,  0.0147,  0.0804],\n",
       "         [ 0.2444, -0.0554, -0.0303,  ...,  0.0666,  0.0143, -0.0211],\n",
       "         [ 0.0408, -0.0834,  0.1128,  ...,  0.1641, -0.0404,  0.0649]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c07884f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#encoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e992b36",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Run Hugging Face model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1877608f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.prenet(**inputs)\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4fd3c9d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([79, 768])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in hf_outputs if hasattr(x, \"shape\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "addc344c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0383,  0.0826,  0.0607,  ..., -0.0333, -0.1770,  0.0203],\n",
       "        [ 0.0857,  0.0239,  0.2346,  ...,  0.1588,  0.0084,  0.0664],\n",
       "        [ 0.0644,  0.0361,  0.2011,  ..., -0.0519, -0.1996,  0.0747],\n",
       "        ...,\n",
       "        [ 0.1361,  0.1569, -0.1022,  ...,  0.1605,  0.0147,  0.0804],\n",
       "        [ 0.2444, -0.0554, -0.0303,  ...,  0.0666,  0.0143, -0.0211],\n",
       "        [ 0.0408, -0.0834,  0.1128,  ...,  0.1641, -0.0404,  0.0649]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adcd7a32",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hf_encoder_input = hf_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbc6dd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If the weights and model were converted correctly, this should report zero or a very small number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b099ad3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_input - hf_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5533cb26",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc87193ec40>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.abs(encoder_input - hf_outputs[0]).numpy()[0] > 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76dc23",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Verify Transformer encoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47c86f72",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run it with the original's speech prenet input:\n",
    "with torch.no_grad():\n",
    "    encoder_output = orig_model.encoder(encoder_input, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b8b4df6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Run it with our input, which is slightly different (see above)\n",
    "# with torch.no_grad():\n",
    "#     encoder_output = orig_model.encoder(hf_encoder_input, ~hf_encoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41ff76cc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out shape torch.Size([79, 1, 768])\n",
      "encoder_padding_mask shape torch.Size([1, 79])\n",
      "encoder_states []\n",
      "src_tokens []\n",
      "decoder_input [None]\n",
      "encoder_out_for_ctc shape torch.Size([79, 1, 81])\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_out shape\", encoder_output[\"encoder_out\"][0].shape)\n",
    "print(\"encoder_padding_mask shape\", encoder_output[\"encoder_padding_mask\"][0].shape)\n",
    "print(\"encoder_states\", encoder_output[\"encoder_states\"])  # []\n",
    "print(\"src_tokens\", encoder_output[\"src_tokens\"])  # []\n",
    "print(\"decoder_input\", encoder_output[\"decoder_input\"])  # [None]\n",
    "print(\"encoder_out_for_ctc shape\", encoder_output[\"encoder_out_for_ctc\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a38979d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9b9dda4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0323,  0.0410,  0.0620,  ...,  0.0133,  0.0218, -0.0187],\n",
       "         [ 0.0523, -0.0873, -0.0304,  ..., -0.0184, -0.0473,  0.0077],\n",
       "         [ 0.0937, -0.0637, -0.0121,  ..., -0.0029, -0.0065, -0.0139],\n",
       "         ...,\n",
       "         [-0.1147,  0.0520,  0.0286,  ...,  0.0440,  0.0449,  0.1282],\n",
       "         [-0.1129,  0.0273, -0.0115,  ...,  0.0530,  0.0472,  0.1208],\n",
       "         [ 0.0075, -0.0139,  0.0873,  ...,  0.0442, -0.0180, -0.0587]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out_for_ctc\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cde6dbda",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use exact same inputs as the original model:\n",
    "# with torch.no_grad():\n",
    "#      hf_outputs = hf_model.speecht5(\n",
    "#          inputs_embeds=encoder_input,\n",
    "#          attention_mask=(~encoder_padding_mask),\n",
    "#      )\n",
    "\n",
    "# type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fefcf62e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28f529a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder.wrapped_encoder(\n",
    "         hidden_states=hf_encoder_input,\n",
    "         attention_mask=torch.ones_like(src_tokens),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb12e71a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06fb675b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8fe9914e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69eda6f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a747e547",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c950849",
   "metadata": {},
   "source": [
    "## Full encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffdcbec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52844106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batch\n",
    "# inputs = torch.tile(src_tokens, (2, 1))\n",
    "# inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f038415",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_out = orig_model.forward_text_encoder(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28782edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1216, -0.1223, -0.0161,  ...,  0.1346,  0.0016, -0.0969],\n",
       "         [-0.4308,  0.2260,  0.2156,  ..., -0.1011,  0.0140, -0.4570],\n",
       "         [-0.5038,  0.2883,  0.1684,  ...,  0.0300, -0.0678, -0.4736],\n",
       "         ...,\n",
       "         [ 0.4783, -0.1476, -0.0570,  ..., -0.2642, -0.6181,  0.2321],\n",
       "         [ 0.4313, -0.2115, -0.1195,  ..., -0.4216, -0.4153,  0.2950],\n",
       "         [ 0.0910, -0.0379,  0.1903,  ...,  0.0070, -0.1593, -0.0444]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "865dbb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[\"encoder_out\"][0].permute((1, 0, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aea13537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what happened to my batch here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33848c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.encoder(\n",
    "         input_values=src_tokens,\n",
    "         attention_mask=torch.ones_like(src_tokens),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40060f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 79, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3486fd24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(encoder_output[\"encoder_out\"][0].permute((1, 0, 2)) - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bb840",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Verify speech decoder prenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ef95457",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#idx = 0\n",
    "#outs, probs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e0950265",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 80])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = encoder_out[\"encoder_out\"][0].new_zeros(1, 1, orig_model.speech_decoder_postnet.odim)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "279a02a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO: just for testing\n",
    "# ys = torch.randn(7, 3, 80)\n",
    "# spkembs = torch.randn(7, 512)\n",
    "\n",
    "ys = torch.randn(1, 1, 80)\n",
    "spkembs = torch.randn(1, 512)\n",
    "#spkembs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fc99b511",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_in, _ = orig_model.speech_decoder_prenet(ys, spkembs=spkembs)\n",
    "decoder_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "86cf9ecd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#decoder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1cc023",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "77134734",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.prenet(\n",
    "         input_values=ys,\n",
    "         speaker_embeddings=spkembs,\n",
    "     )\n",
    "\n",
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f7fc65dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_in - hf_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d7a7e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fec354d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Verify Transformer decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "559eb4d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output_tokens = decoder_in[:, -1:]\n",
    "prev_output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d516c43e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tgt_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05ae78d7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "incremental_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6229b43c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_output, extra = orig_model.decoder(\n",
    "        prev_output_tokens,\n",
    "        tgt_mask,\n",
    "        encoder_out=encoder_out,\n",
    "        incremental_state=incremental_state,\n",
    "        alignment_layer=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a190958",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fea0351e",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05117956",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"attn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b593692b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#[x.shape for x in extra[\"inner_states\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d486f4b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder.wrapped_decoder(\n",
    "         hidden_states=prev_output_tokens,\n",
    "         #attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_out[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         #encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea512cc2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'past_key_values']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d82c369a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb6282bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hf_outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3e254cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(decoder_output - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d1fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceec7dd9",
   "metadata": {},
   "source": [
    "## Full decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1700a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_out = orig_model.forward_text_encoder(src_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4146fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.randn(1, 3, 80)\n",
    "spkembs = torch.randn(1, 512)\n",
    "incremental_states = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bd610771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 768]), torch.Size([1, 1, 768]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_in.shape, decoder_in[:,-1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36c4c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder_in, _ = orig_model.speech_decoder_prenet(ys, spkembs=spkembs)\n",
    "    z, extra = orig_model.decoder(\n",
    "        decoder_in[:,-1:], None, encoder_out, incremental_states, alignment_layer=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c1f49894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6a048e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model.speecht5.decoder(\n",
    "         input_values=ys,\n",
    "         #attention_mask=decoder_attention_mask,\n",
    "         encoder_hidden_states=encoder_out[\"encoder_out\"][0].permute((1, 0, 2)),\n",
    "         #encoder_attention_mask=hf_encoder_attention_mask.repeat((1, beam_size)).view(beam_size * batch_size, -1),\n",
    "         speaker_embeddings=spkembs,\n",
    "         return_dict=True,         \n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6c4b72b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c119b515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(z - hf_outputs[\"last_hidden_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9113f7",
   "metadata": {},
   "source": [
    "## Verify speech decoder postnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "16c82e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 768]), torch.Size([768]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape, z[0, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ca697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             outs += [self.speech_decoder_postnet.feat_out(z[0, -1]).view(self.reduction_factor, self.speech_decoder_postnet.odim)]  # [(r, odim), ...]\n",
    "#             probs += [torch.sigmoid(self.speech_decoder_postnet.prob_out(z[0, -1]))]  # [(r), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "110cd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#orig_model.speech_decoder_postnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8e294ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model.speech_decoder_postnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a20be43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    before_outs, after_outs, logits = orig_model.speech_decoder_postnet(z)  # z[:, -1])\n",
    "    \n",
    "    # before_outs = speech_decoder_postnet.feat_out(...)\n",
    "    # after_outs = final output\n",
    "    # logits = speech_decoder_postnet.prob_out(...)  for stop token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a7a143bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 80]), torch.Size([1, 2, 80]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_outs.shape, after_outs.shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7d6de14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-13.0990,  -5.9124])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7ee9a8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ba7571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    before_outs_hf, after_outs_hf, logits_hf = hf_model.speech_decoder_postnet(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5623a791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 80]), torch.Size([1, 2, 80]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_outs_hf.shape, after_outs_hf.shape, logits_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230f4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6eeff463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(before_outs - before_outs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a0cb9d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(after_outs - after_outs_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e8055eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(logits - logits_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24926eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "394115d0",
   "metadata": {},
   "source": [
    "## Generating speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b949bb79",
   "metadata": {},
   "source": [
    "The original repo comes with a `generate_speech.py` script. This is used instead of the typical encoder-decoder generation loop.\n",
    "\n",
    "What this script does:\n",
    "\n",
    "- create SpeechT5 task\n",
    "- load the model checkpoint\n",
    "- load the dataset\n",
    "- create an iterator for the examples in the dataset\n",
    "- loop through the iterator and call `task.generate_speech()` on each example\n",
    "- save the predicted features to a numpy file\n",
    "- save some spectrogram plots\n",
    "\n",
    "The example that goes into `task.generate_speech()` is already tokenized. This is done by the dataset.\n",
    "\n",
    "`task.generate_speech()` calls `model.generate_speech()` on the T5TransformerModel.\n",
    "\n",
    "Dataset (`text_to_speech_dataset.py`):\n",
    "\n",
    "- `__getitem__`:\n",
    "    - calls `get_labels` which gets the text and tokenizes it\n",
    "    - calls `get_audio` to load the target audio file and converts it to log-mel spectrograms\n",
    "    - calls `get_features_or_waveform` to load the speaker embeddings from NumPy files (= 512-element vector)\n",
    "    \n",
    "- `collater`:\n",
    "    - thins out frames of the target spectrogram for reduction factor (= 2)\n",
    "    - creates `prev_output_tokens` by shifting the target spectrogram\n",
    "    - creates labels for stop token prediction (tensor with same size as spectrogram that is all zero except for final element, which is 1)\n",
    "    - pads everything to fit in a mini-batch\n",
    "\n",
    "We don't need the target audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ac6c3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hey, what's up, Hugging Face? This is Speech tee five talking to you for real.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "dc43c338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4, 35,  5, 22, 23,  4, 20, 11,  7,  6, 31, 12,  4, 16, 24, 23,  4, 35,\n",
       "         16, 21, 21, 10,  9, 21,  4, 55,  7, 17,  5, 41,  4, 32, 11, 10, 12,  4,\n",
       "         10, 12,  4, 34, 24,  5,  5, 17, 11,  4,  6,  5,  5,  4, 19, 10, 27,  5,\n",
       "          4,  6,  7, 15, 28, 10,  9, 21,  4,  6,  8,  4, 22,  8, 16,  4, 19,  8,\n",
       "         13,  4, 13,  5,  7, 15, 26]], dtype=torch.int32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = tokenizer.encode(text)\n",
    "src_tokens = label_processor(label)\n",
    "src_tokens = src_tokens.unsqueeze(0)\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f7b0a239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xvector = \"1272-128104.npy\"\n",
    "xvector = \"711-187464.npy\"\n",
    "spkembs_path = \"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/AUDIO_DIR/dev_clean/xvectors/\" + xvector\n",
    "\n",
    "spkembs = torch.from_numpy(np.load(spkembs_path)).float()\n",
    "spkembs = spkembs.unsqueeze(0)\n",
    "spkembs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "0d4e5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "minlenratio = 0.0\n",
    "maxlenratio = 20.0\n",
    "reduction_factor = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3cb030",
   "metadata": {},
   "source": [
    "### The loop from the original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "25d9872b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder_out',\n",
       " 'encoder_padding_mask',\n",
       " 'encoder_states',\n",
       " 'src_tokens',\n",
       " 'decoder_input',\n",
       " 'encoder_out_for_ctc']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_out = orig_model.forward_text_encoder(src_tokens)\n",
    "list(encoder_out.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a4a02",
   "metadata": {},
   "source": [
    "This part is skipped, since `spk_embed_integration_type == \"pre\"`:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fb4ad2a",
   "metadata": {},
   "source": [
    "if spkembs is not None and self.spk_embed_integration_type != \"pre\":\n",
    "    encoder_out[\"encoder_out\"] = [self._integrate_with_spk_embed(\n",
    "        encoder_out[\"encoder_out\"][0].transpose(0, 1), spkembs\n",
    "    ).transpose(0, 1)]\n",
    "    spkembs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "6221c52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(790, 0)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = int(encoder_out[\"encoder_out\"][0].size(0) * maxlenratio / reduction_factor)\n",
    "minlen = int(encoder_out[\"encoder_out\"][0].size(0) * minlenratio / reduction_factor)\n",
    "maxlen, minlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "73147aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 80])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = encoder_out[\"encoder_out\"][0].new_zeros(1, 1, orig_model.speech_decoder_postnet.odim)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "5e41c48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "6c4505aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs, probs = [], []\n",
    "\n",
    "incremental_states = {}\n",
    "attns = []\n",
    "\n",
    "idx = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    while True:\n",
    "        # update index\n",
    "        idx += 1\n",
    "\n",
    "        # calculate output and stop prob at idx-th step\n",
    "        decoder_in, _ = orig_model.speech_decoder_prenet(ys, spkembs=spkembs)\n",
    "\n",
    "        z, extra = orig_model.decoder(\n",
    "            decoder_in[:, -1:], None, encoder_out, incremental_states, alignment_layer=-1\n",
    "        )\n",
    "        \n",
    "        outs += [\n",
    "            orig_model.speech_decoder_postnet.feat_out(z[0, -1]).view(\n",
    "                reduction_factor, orig_model.speech_decoder_postnet.odim)\n",
    "        ]  # [(r, odim), ...]\n",
    "        \n",
    "        probs += [torch.sigmoid(orig_model.speech_decoder_postnet.prob_out(z[0, -1]))]  # [(r), ...]\n",
    "\n",
    "        # update next inputs\n",
    "        ys = torch.cat((ys, outs[-1][-1].view(1, 1, orig_model.speech_decoder_postnet.odim)), dim=1)  # (1, idx + 1, odim)\n",
    "        attns.append(torch.stack([att_l[0] for att_l in extra['attn'][0]], dim=0))\n",
    "        \n",
    "\n",
    "        if idx == 2: break #TODO\n",
    "\n",
    "\n",
    "        # check whether to finish generation\n",
    "        if int(sum(probs[-1] >= threshold)) > 0 or idx >= maxlen:\n",
    "            # check mininum length\n",
    "            if idx < minlen:\n",
    "                continue\n",
    "            outs = (torch.cat(outs, dim=0).unsqueeze(0).transpose(1, 2))  # (L, odim) -> (1, L, odim) -> (1, odim, L)\n",
    "            if orig_model.speech_decoder_postnet.postnet is not None:\n",
    "                outs = outs + orig_model.speech_decoder_postnet.postnet(outs)  # (1, odim, L)\n",
    "            outs = outs.transpose(2, 1).squeeze(0)  # (L, odim)\n",
    "            probs = torch.cat(probs, dim=0)\n",
    "            attn = torch.cat(attns, dim=2)\n",
    "            break\n",
    "\n",
    "    # if outs.size(0) == maxlen:\n",
    "    #     logging.warning(\"output length reaches maximum length\")\n",
    "    # return outs, probs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "01f5e049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "991c694b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([224, 80]), torch.Size([224]), torch.Size([6, 12, 112, 79]))"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape, probs.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "3e241d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4473, -2.5842, -2.6425,  ..., -3.4070, -3.4652, -3.6969],\n",
       "        [-2.3174, -2.4196, -2.4533,  ..., -3.2911, -3.3278, -3.6190],\n",
       "        [-2.0294, -2.1171, -2.1305,  ..., -3.1284, -3.1923, -3.4745],\n",
       "        ...,\n",
       "        [-1.6293, -1.6272, -1.5869,  ..., -2.5902, -2.6733, -2.9613],\n",
       "        [-1.6065, -1.5989, -1.5645,  ..., -2.5180, -2.6069, -2.8994],\n",
       "        [-1.6373, -1.6390, -1.5916,  ..., -2.5051, -2.5946, -2.8914]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "49019a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1997e-11, 1.3594e-04, 6.1334e-12, 3.5746e-06, 1.3660e-11, 1.8025e-06,\n",
       "        2.8043e-11, 1.5775e-06, 2.5754e-11, 1.4805e-06, 3.0223e-11, 5.5564e-06,\n",
       "        3.3931e-11, 1.3164e-05, 3.5434e-11, 2.1806e-05, 3.5025e-11, 2.5369e-05,\n",
       "        3.1782e-11, 2.1289e-05, 2.7731e-11, 1.5104e-05, 2.2610e-11, 1.0534e-05,\n",
       "        1.5802e-11, 6.3807e-06, 9.1175e-12, 3.2117e-06, 6.7534e-12, 1.2539e-06,\n",
       "        6.3691e-12, 6.0306e-06, 6.5031e-12, 1.1213e-05, 1.7064e-11, 3.1604e-06,\n",
       "        2.7723e-11, 1.2787e-06, 3.3940e-11, 2.3180e-06, 1.8359e-11, 2.0884e-06,\n",
       "        7.4981e-12, 2.3369e-06, 7.8722e-11, 1.2947e-05, 1.0444e-10, 9.5641e-06,\n",
       "        1.1066e-10, 5.5312e-06, 6.5378e-11, 5.0701e-06, 3.2989e-11, 4.6968e-06,\n",
       "        6.1659e-12, 3.5963e-06, 2.6113e-11, 1.6300e-05, 2.1985e-11, 2.6226e-06,\n",
       "        4.2833e-11, 5.9315e-06, 4.3508e-11, 2.8847e-06, 1.6733e-11, 1.4583e-06,\n",
       "        3.0768e-11, 7.7178e-05, 3.4352e-11, 9.8295e-06, 2.5548e-11, 7.4068e-06,\n",
       "        2.1749e-11, 1.4215e-05, 1.4443e-11, 1.7621e-05, 7.8930e-12, 1.6108e-05,\n",
       "        1.8213e-11, 2.5609e-06, 6.1150e-11, 2.9661e-06, 7.5695e-11, 4.6110e-06,\n",
       "        7.1215e-11, 2.2105e-05, 6.3380e-11, 2.5129e-05, 5.1645e-11, 1.7340e-05,\n",
       "        9.4494e-11, 1.9587e-05, 1.7654e-10, 1.6142e-05, 1.6173e-10, 4.8652e-05,\n",
       "        9.9948e-11, 1.3199e-04, 2.3435e-11, 4.3537e-05, 1.5542e-11, 6.4078e-06,\n",
       "        1.5430e-11, 4.0896e-06, 2.0288e-11, 8.3423e-05, 2.1500e-11, 2.2276e-04,\n",
       "        1.9320e-11, 2.1477e-04, 1.4784e-11, 9.4588e-05, 1.2360e-11, 3.4221e-05,\n",
       "        1.0315e-11, 5.4837e-05, 1.1109e-11, 9.9029e-06, 3.7348e-11, 4.1575e-06,\n",
       "        6.3583e-11, 2.8391e-06, 2.4100e-10, 2.5222e-05, 2.0619e-10, 2.2134e-05,\n",
       "        1.5560e-10, 1.1377e-05, 1.0180e-10, 4.1065e-06, 1.0361e-10, 4.6371e-06,\n",
       "        3.0561e-10, 3.0168e-05, 2.9267e-10, 9.1505e-05, 1.2719e-10, 3.2424e-05,\n",
       "        1.9318e-11, 4.1677e-05, 1.8034e-11, 2.8432e-06, 5.4803e-11, 2.6293e-06,\n",
       "        6.2209e-11, 1.9949e-06, 4.8921e-11, 5.9191e-06, 2.4068e-11, 1.4601e-05,\n",
       "        3.5854e-11, 9.0700e-06, 1.7690e-10, 5.8749e-05, 1.2688e-10, 5.2685e-06,\n",
       "        2.5560e-11, 1.2549e-06, 1.4403e-10, 6.7487e-06, 1.0871e-10, 9.1682e-06,\n",
       "        8.9457e-11, 1.3446e-05, 7.3993e-11, 1.0513e-05, 5.9846e-11, 2.5556e-05,\n",
       "        3.1538e-11, 7.9076e-05, 1.0406e-11, 4.7041e-05, 3.0260e-11, 5.1011e-06,\n",
       "        8.8584e-11, 1.8676e-06, 1.3006e-10, 7.1834e-06, 1.1928e-10, 1.8172e-05,\n",
       "        1.0923e-10, 2.0343e-05, 7.2265e-11, 1.4131e-05, 1.9068e-11, 2.1427e-05,\n",
       "        9.7718e-12, 6.4493e-06, 9.2019e-11, 4.6575e-06, 9.4216e-11, 2.0522e-06,\n",
       "        1.5639e-10, 1.6530e-05, 1.1298e-10, 1.6928e-05, 9.0452e-11, 1.7388e-05,\n",
       "        4.7673e-11, 1.9655e-05, 9.8920e-12, 4.4297e-05, 3.7931e-11, 1.4902e-04,\n",
       "        5.6766e-11, 1.1070e-04, 7.5548e-11, 6.9728e-05, 6.7738e-11, 2.2238e-04,\n",
       "        6.4364e-11, 9.0420e-04, 5.0805e-11, 4.4899e-03, 3.1349e-11, 3.4276e-02,\n",
       "        1.7009e-11, 2.1591e-02, 1.6378e-11, 4.0067e-03, 2.8223e-11, 1.9194e-02,\n",
       "        6.0582e-11, 7.2937e-01])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "11e81e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/RESULTS_PATH_TTS/lol-feats.npy\", outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2fcda858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#        np.save(op.join(cfg.common_eval.results_path, audio_name.replace(\".flac\", \"-feats.npy\")), outs)  #MIH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "ca9c058b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 80)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.load(\"/Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/RESULTS_PATH_TTS/1272-141231-0020-feats.npy\")\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "8ea516b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.447312 , -2.5842195, -2.6425114, ..., -3.4069824, -3.465168 ,\n",
       "        -3.6968606],\n",
       "       [-2.3173997, -2.4196224, -2.4533415, ..., -3.2910826, -3.3278248,\n",
       "        -3.619002 ],\n",
       "       [-2.0293949, -2.1170504, -2.1304567, ..., -3.1284175, -3.1922834,\n",
       "        -3.4744668],\n",
       "       ...,\n",
       "       [-1.6293311, -1.6271574, -1.5868783, ..., -2.5902255, -2.673273 ,\n",
       "        -2.96128  ],\n",
       "       [-1.6064708, -1.598937 , -1.5644566, ..., -2.518047 , -2.6068907,\n",
       "        -2.8994167],\n",
       "       [-1.6372557, -1.6390084, -1.5915557, ..., -2.5051334, -2.5946448,\n",
       "        -2.891439 ]], dtype=float32)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f5c169b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(x - outs.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130612e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a909eb1e",
   "metadata": {},
   "source": [
    "### The loop using the HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "22ec8803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "     encoder_out_hf = hf_model.speecht5.encoder(\n",
    "         input_values=src_tokens,\n",
    "         attention_mask=torch.ones_like(src_tokens),\n",
    "         return_dict=True,\n",
    "     )\n",
    "\n",
    "type(encoder_out_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "ef9c2679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state']"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoder_out_hf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "779136f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(790, 0)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: size(1) instead of size(0) because permuted\n",
    "maxlen = int(encoder_out_hf[\"last_hidden_state\"].size(1) * maxlenratio / reduction_factor)\n",
    "minlen = int(encoder_out_hf[\"last_hidden_state\"].size(1) * minlenratio / reduction_factor)\n",
    "maxlen, minlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "b0d7866a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 80])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_hf = encoder_out_hf[\"last_hidden_state\"].new_zeros(1, 1, hf_model.config.num_mel_bins)\n",
    "ys_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "5103f42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "9a80bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_hf, probs_hf = [], []\n",
    "past_key_values = None\n",
    "idx = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    while True:\n",
    "        # update index\n",
    "        idx += 1\n",
    "\n",
    "        # TODO: combine this in SpeechT5ForTTS?\n",
    "        \n",
    "        decoder_out_hf = hf_model.speecht5.decoder(\n",
    "            input_values=ys_hf,\n",
    "            #attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_out_hf[\"last_hidden_state\"],\n",
    "            #encoder_attention_mask=???,\n",
    "            speaker_embeddings=spkembs,\n",
    "            return_dict=True,         \n",
    "        )\n",
    "        \n",
    "        z = decoder_out_hf.last_hidden_state\n",
    "\n",
    "        with torch.no_grad():\n",
    "            before_outs_hf, after_outs_hf, logits_hf = hf_model.speech_decoder_postnet(z)\n",
    "            \n",
    "        outs_hf += [ before_outs_hf.view(reduction_factor, hf_model.config.num_mel_bins) ]\n",
    "        probs_hf += [ torch.sigmoid(logits_hf) ]\n",
    "\n",
    "        # update next inputs\n",
    "        ys_hf = torch.cat((ys_hf, outs_hf[-1][-1].view(1, 1, hf_model.config.num_mel_bins)), dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # NOTE: there's a bug somewhere because the second iteration of the loop\n",
    "        # gives different results; the loop also never terminates\n",
    "        \n",
    "        \n",
    "        \n",
    "        if idx == 2: break #TODO\n",
    "\n",
    "            \n",
    "        # check whether to finish generation\n",
    "        if int(sum(probs[-1] >= threshold)) > 0 or idx >= maxlen:\n",
    "            # check mininum length\n",
    "            if idx < minlen:\n",
    "                continue\n",
    "                \n",
    "            outs_hf = (torch.cat(outs_hf, dim=0).unsqueeze(0).transpose(1, 2))\n",
    "            \n",
    "            # TODO: this is after_outs_hf from the model, but I need to run that again here\n",
    "            #outs_hf = outs_hf + hf_model.speech_decoder_postnet.postnet(outs_hf)\n",
    "            \n",
    "            outs_hf = outs_hf.transpose(2, 1).squeeze(0)  # (L, odim)\n",
    "            probs_hf = torch.cat(probs_hf, dim=0)\n",
    "            break\n",
    "\n",
    "    # if outs.size(0) == maxlen:\n",
    "    #     logging.warning(\"output length reaches maximum length\")\n",
    "    # return outs, probs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c10b7053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.1800)\n",
      "tensor(0.7811)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(outs)):\n",
    "    print(torch.max(torch.abs(outs[i] - outs_hf[i])))\n",
    "    print(torch.max(torch.abs(probs[i] - probs_hf[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "e12dd518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1800)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(ys - ys_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "061a807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 80]), torch.Size([1, 3, 80]))"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape, ys_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "181fc414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "8d4b9325",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [358]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mouts_hf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, probs_hf\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "outs_hf.shape, probs_hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a56c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bba5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1682df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use_cache = True!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac56e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26cfccbb",
   "metadata": {},
   "source": [
    "## Vocoder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90dac2ea",
   "metadata": {},
   "source": [
    "parallel-wavegan-decode \\\n",
    "    --checkpoint /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/weights/speecht5-tts/pretrained_vocoder/train_nodev_clean_libritts_hifigan.v1/hifigan-libritts-1930000steps.pkl \\\n",
    "    --dumpdir /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/RESULTS_PATH_TTS \\\n",
    "    --outdir /Users/matthijs/Documents/FILES/HuggingFace/SpeechT5/tryout/RESULTS_PATH_TTS/wav \\\n",
    "    --normalize-before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0685a8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f4277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6ddd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1612cc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f01770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e98799",
   "metadata": {},
   "source": [
    "## Use the `transformers` generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daecf9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6bc37bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: does this even make sense for the TTS model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aea16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55c4e9dc",
   "metadata": {},
   "source": [
    "Run the full model to make sure this doesn't give any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b889ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         decoder_input_ids=torch.tensor([[3, 4, 5]]),\n",
    "         #decoder_input_ids=torch.tensor([[3, 4, 5], [2, 2, 2]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677718c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf0244b",
   "metadata": {},
   "source": [
    "Also calculate loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     hf_outputs = hf_model(\n",
    "         input_values=inputs.input_values,\n",
    "         attention_mask=inputs.attention_mask,\n",
    "         #decoder_input_ids=torch.tensor([[2,  4, 18, 10, 12,  6,  5]]),\n",
    "         labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13]]),\n",
    "         #labels=torch.tensor([[4, 18, 10, 12,  6,  5, 13], [4, 18, 10, 12,  6,  5, 13]]),  # batch\n",
    "     )\n",
    "\n",
    "type(hf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34dd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195961eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2ce14a",
   "metadata": {},
   "source": [
    "Generator loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ccdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs = hf_model.generate(inputs.input_values, max_length=100)\n",
    "# hf_outputs = hf_model.generate(inputs.input_values, num_beams=5, max_length=100) #, bos_token_id=2)\n",
    "# hf_outputs = hf_model.generate(torch.rand(1, 10000), num_beams=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c80b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fce9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hf_outputs.shape[0]):\n",
    "    print(tgt_dict.string(hf_outputs[i]))\n",
    "    print(tokenizer.decode(tgt_dict.string(hf_outputs[i])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tgt_dict[x] for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a116f698",
   "metadata": {},
   "source": [
    "For comparison, Speech2Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00306c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "s2t_model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "s2t_processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = s2t_processor(\n",
    "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n",
    ")\n",
    "input_features = inputs.input_features\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ef5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batch\n",
    "# input_features = torch.tile(input_features, dims=(2, 1, 1))\n",
    "# input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf06a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = s2t_model.generate(inputs=input_features)\n",
    "\n",
    "transcription = s2t_processor.batch_decode(generated_ids)[0]\n",
    "transcription\n",
    "\n",
    "#'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a21206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Speech2TextModel\n",
    "\n",
    "s2t_model = Speech2TextModel.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d50884f2",
   "metadata": {},
   "source": [
    "Test other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd38a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART, Speech2Text, Wav2Vec2 don't have pruning\n",
    "#hf_model.prune_heads({1: [0, 2], 2: [2,3 ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f954bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.resize_token_embeddings(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e8369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b945156",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_ctc.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c092a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_naked.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
