{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5_utils import *\n",
    "\n",
    "eli5 = nlp.load_dataset(\"/home/yacine/Code/nlp/datasets/explainlikeimfive\", name=\"LFQA_reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Keep the defense honest, get a feel for the pass rush, open up the passing game. An offense that's too one dimensional will fail. And those rushes up the middle can be busted wide open sometimes for big yardage.\",\n",
       " 'As someone who uses quality Premium, I wish this was true.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[al['text'][0] for al in eli5['train_eli5'][:2]['answers']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ArgumentsQAR():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 512\n",
    "        self.max_length = 128\n",
    "        self.checkpoint_batch_size = 32\n",
    "        self.print_freq = 10\n",
    "        self.pretrained_model_name = \"google/bert_uncased_L-8_H-768_A-12\"\n",
    "        self.model_save_name = \"retriever_models/joint_retriever_model_l-8_h-768_b-512-512\"\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_epochs = 20\n",
    "\n",
    "qar_args = ArgumentsQAR()\n",
    "\n",
    "qar_train_dset_eli5 = ELI5DatasetQARetriver(eli5['train_eli5'], min_answer_length=64, training=True)\n",
    "qar_train_dset_asks = ELI5DatasetQARetriver(eli5['train_asks'], min_answer_length=64, training=True, n_samples=eli5['train_eli5'].num_rows)\n",
    "qar_train_dset_askh = ELI5DatasetQARetriver(eli5['train_askh'], min_answer_length=64, training=True, n_samples=eli5['train_eli5'].num_rows)\n",
    "\n",
    "qar_train_dset_list = [qar_train_dset_eli5, qar_train_dset_asks, qar_train_dset_askh]\n",
    "\n",
    "qar_valid_dset = ELI5DatasetQARetriver(eli5['validation_eli5'], min_answer_length=64, training=False)\n",
    "\n",
    "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
    "    model_name=qar_args.pretrained_model_name,\n",
    "    from_file=None,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "qar_optimizer = AdamW(qar_model.parameters(), lr=qar_args.learning_rate, eps=1e-8)\n",
    "qar_scheduler = get_linear_schedule_with_warmup(\n",
    "        qar_optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=qar_args.num_epochs * math.ceil(len(qar_train_dset_eli5) / qar_args.batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 7.79 GiB total capacity; 6.04 GiB already allocated; 66.69 MiB free; 6.07 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-179aed1c1578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     train_qa_retriever_joint_epoch(\n\u001b[1;32m      3\u001b[0m         \u001b[0mqar_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqar_train_dset_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqar_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mqar_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqar_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqar_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m     m_save_dict = {\n",
      "\u001b[0;32m~/Code/transformers/examples/eli5/eli5_utils.py\u001b[0m in \u001b[0;36mtrain_qa_retriever_joint_epoch\u001b[0;34m(model, dataset_list, tokenizer, optimizer, scheduler, args, e)\u001b[0m\n\u001b[1;32m    670\u001b[0m             )\n\u001b[1;32m    671\u001b[0m             \u001b[0;31m# optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mdetached_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetach_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdetached_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/transformers/examples/eli5/eli5_utils.py\u001b[0m in \u001b[0;36mpartial_encode\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m                     \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m                 )\n\u001b[1;32m    536\u001b[0m                 \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/transformers/src/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/transformers/src/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/transformers/src/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 7.79 GiB total capacity; 6.04 GiB already allocated; 66.69 MiB free; 6.07 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for e in range(qar_args.num_epochs):\n",
    "    train_qa_retriever_joint_epoch(\n",
    "        qar_model, qar_train_dset_list, qar_tokenizer,\n",
    "        qar_optimizer, qar_scheduler, qar_args, e\n",
    "    )\n",
    "    m_save_dict = {\n",
    "        'model': qar_model.state_dict(),\n",
    "        'optimizer': qar_optimizer.state_dict(),\n",
    "        'scheduler': qar_scheduler.state_dict(),\n",
    "    }\n",
    "    print(\"Saving model {}\".format(qar_args.model_save_name))\n",
    "    torch.save(m_save_dict, '{}_{}.pth'.format(qar_args.model_save_name, e))\n",
    "    eval_loss = evaluate_qa_retriever(qar_model, qar_valid_dset, qar_tokenizer, qar_args)\n",
    "    print(\"Evaluation loss epoch {:4d}: {:.3f}\".format(e, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate recall@N for validation / test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used train rtriever to index Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5_utils import *\n",
    "\n",
    "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
    "    model_name=\"google/bert_uncased_L-8_H-512_A-8\",\n",
    "    from_file='{}_{}.pth'.format(\"retriever_models/eli5_retriever_model_512\", 9),\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "kilt_snippets_dbuilder = KiltSnippets(data_dir='kilt_snippets_100w')\n",
    "kilt_snippets_dbuilder.download_and_prepare()\n",
    "wiki_passages = kilt_snippets_dbuilder.as_dataset(split=nlp.splits.Split.TRAIN)\n",
    "\n",
    "make_qa_dense_index(qar_model, qar_tokenizer,\n",
    "                    wiki_passages,\n",
    "                    batch_size=512, max_length=96,\n",
    "                    index_name='kilt_passages_reps_16.dat',\n",
    "                    device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5_utils import *\n",
    "\n",
    "eli5_dbuilder = ELI5NLP(data_dir='eli5')\n",
    "eli5_dbuilder.download_and_prepare()\n",
    "\n",
    "eli5_train = eli5_dbuilder.as_dataset(split=nlp.splits.Split.TRAIN)\n",
    "eli5_valid = eli5_dbuilder.as_dataset(split=nlp.splits.Split.VALIDATION)\n",
    "\n",
    "eli5_train_docs = json.load(open('precomputed/eli5_train_precomputed_dense_docs.json'))\n",
    "eli5_valid_docs = json.load(open('precomputed/eli5_valid_precomputed_dense_docs.json'))\n",
    "\n",
    "s2s_train_dset = ELI5DatasetS2S(eli5_train, document_cache=dict([(k, d) for k, d, src_ls in eli5_train_docs]))\n",
    "s2s_valid_dset = ELI5DatasetS2S(eli5_valid, document_cache=dict([(k, d) for k, d, src_ls in eli5_valid_docs]), training=False)\n",
    "\n",
    "qa_s2s_tokenizer, qa_s2s_model = make_qa_s2s_model(\n",
    "    model_name=\"t5-small\",\n",
    "    from_file=None,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "class ArgumentsS2S():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 4\n",
    "        self.backward_freq = 4\n",
    "        self.max_length = 1024\n",
    "        self.print_freq = 4000\n",
    "        self.model_save_name = \"seq2seq_models/eli5_t5_model_1024\"\n",
    "        self.learning_rate = 1e-4\n",
    "        self.num_epochs = 10\n",
    "\n",
    "s2s_args = ArgumentsS2S()\n",
    "s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n",
    "s2s_scheduler = get_linear_schedule_with_warmup(\n",
    "        s2s_optimizer,\n",
    "        num_warmup_steps=400,\n",
    "        num_training_steps=s2s_args.num_epochs * math.ceil(len(s2s_train_dset) / s2s_args.batch_size)\n",
    ")\n",
    "\n",
    "for e in range(s2s_args.num_epochs):\n",
    "    train_qa_s2s_epoch(\n",
    "        qa_s2s_model,\n",
    "        s2s_train_dset, qa_s2s_tokenizer,\n",
    "        s2s_optimizer, s2s_scheduler,\n",
    "        s2s_args, e\n",
    "    )\n",
    "    m_save_dict = {\n",
    "        'model': qa_s2s_model.state_dict(),\n",
    "        'optimizer': s2s_optimizer.state_dict(),\n",
    "        'scheduler': s2s_scheduler.state_dict(),\n",
    "    }\n",
    "    print(\"Saving model {}\".format(s2s_args.model_save_name))\n",
    "    torch.save(m_save_dict, '{}_{}.pth'.format(s2s_args.model_save_name, e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(s2s_args.num_epochs):\n",
    "    train_qa_s2s_epoch(\n",
    "        qa_s2s_model,\n",
    "        s2s_train_dset, qa_s2s_tokenizer,\n",
    "        s2s_optimizer, s2s_scheduler,\n",
    "        s2s_args, e\n",
    "    )\n",
    "    m_save_dict = {\n",
    "        'model': qa_s2s_model.state_dict(),\n",
    "        'optimizer': s2s_optimizer.state_dict(),\n",
    "        'scheduler': s2s_scheduler.state_dict(),\n",
    "    }\n",
    "    print(\"Saving model {}\".format(s2s_args.model_save_name))\n",
    "    torch.save(m_save_dict, '{}_{}.pth'.format(s2s_args.model_save_name, e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5_utils import *\n",
    "\n",
    "eli5_dbuilder = ELI5NLP(data_dir='eli5')\n",
    "eli5_dbuilder.download_and_prepare()\n",
    "\n",
    "eli5_train = eli5_dbuilder.as_dataset(split=nlp.splits.Split.TRAIN)\n",
    "eli5_valid = eli5_dbuilder.as_dataset(split=nlp.splits.Split.VALIDATION)\n",
    "\n",
    "eli5_train_docs = json.load(open('precomputed/eli5_train_precomputed_dense_docs.json'))\n",
    "eli5_valid_docs = json.load(open('precomputed/eli5_valid_precomputed_dense_docs.json'))\n",
    "\n",
    "s2s_train_dset = ELI5DatasetS2S(eli5_train, document_cache=dict([(k, d) for k, d, src_ls in eli5_train_docs]))\n",
    "s2s_valid_dset = ELI5DatasetS2S(eli5_valid, document_cache=dict([(k, d) for k, d, src_ls in eli5_valid_docs]), training=False)\n",
    "\n",
    "qa_s2s_tokenizer, qa_s2s_model = make_qa_s2s_model(\n",
    "    model_name=\"t5-small\",\n",
    "    from_file=None,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "class ArgumentsS2S():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 1\n",
    "        self.backward_freq = 16\n",
    "        self.max_length = 512\n",
    "        self.print_freq = 1000\n",
    "        self.model_save_name = \"seq2seq_models/eli5_t5_model_512\"\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_epochs = 10\n",
    "\n",
    "s2s_args = ArgumentsS2S()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: why is google fibre taking so long to roll out?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd, a = s2s_valid_dset[123]\n",
    "qd.split(' context: ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Because Google has to build a lot of cables, and they have to lay a lot more cables than you think. They also have to make sure that the cables are strong enough to handle the load, and make sure they don't break down. They have to build the cables in a way that they can make sure the cables don't get damaged.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s2s_generate(qd, qa_s2s_model, qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    num_beams=2,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Fiber has a ton of infrastructure to build and maintain in addition to the fiber itself. A lot of that infrastructure is already in place. Google is just waiting for you to pay for it. Google Fiber is a lot like a business. You have to buy a lot of equipment, and you have to pay a lot.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s2s_generate(qd, qa_s2s_model, qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    do_sample=True,\n",
    "    temp=0.7,\n",
    "    top_p=0.95,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"question: the hubble telescope was launched in 1990. since our technology has advanced tremendously since then, wouldn't it be advantageous to send a more advanced telescope up there?\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd, a = s2s_valid_dset[0]\n",
    "qd.split(' context: ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Hubble was launched in 1990. The Hubble telescope is still in orbit around the Sun. It's not going to get any more advanced than the Hubble, and it's not even close to the size of the Hubble. It was launched for science, not for fun. It is still a very small telescope.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s2s_generate(qd, qa_s2s_model, qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    num_beams=2,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Hubble was launched because people needed more information about the universe and Hubble was the best instrument available. The Hubble is still in use today for science purposes, as it's used to study a lot of the light that comes from distant stars. If you want to send a telescope up to any star that's far away, you'd need to send it to a star that has a much bigger size and brighter light, and you'd have to send another telescope to observe the star. Since telescopes are only a few meters across and the Hubble is only a couple of meters across, you don't really need to do that.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s2s_generate(qd, qa_s2s_model, qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    do_sample=True,\n",
    "    temp=0.7,\n",
    "    top_p=0.95,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: could a computer be built out of electromagnetic relays instead of semiconductors?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd, a = s2s_valid_dset[1]\n",
    "qd.split(' context: ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, because they would be very expensive and very heavy, and they would need to be able to withstand a lot of heat and radiation to be useful. It would be a lot easier to just use a magnetic field instead of semiconductors. But it would be much easier to make a computer out of electromagnetic relays than a semiconductor one, and it would probably be cheaper to do so. So it's not really worth the cost to do it, but it's a good way to get a computer that can handle a lot more heat and heat without having to worry about it getting too hot or getting too cold. But if you want to build a computer with a lot less power, you'd need to make it a lot smaller, and you'd have to make sure that it could handle the amount of heat that would be required to run the computer.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s2s_generate(qd, qa_s2s_model, qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    num_beams=2,\n",
    "    min_len=128,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No. Electromagnetics are far too reactive to be used in a computer. Not to mention the inherent resistance. It's also not practical. It would be a lot more energy than you can ever hope to gain. Also, it's a very bad idea to build a computer out of magnetic relays.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s2s_generate(qd, qa_s2s_model, qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    do_sample=True,\n",
    "    temp=0.7,\n",
    "    top_p=0.95,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"question: in trading places (1983, akroyd/murphy) how does the scheme at the end of the movie work? why would buying a lot of oj at a high price ruin the duke brothers? i have a vague understanding, but i'm hoping someone can explain it better to me. and maybe throw in some knowledge about the stock market in general while you're at it? thank you! edit: hey everyone, thanks for all the great answers! i think i actually really understand futures commodities now. who ever said reddit was a waste of time, eh? so yes, this question has been answered, and yes, i've seen/heard that great npr clip about this topic. thanks again to everyone!\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd, a = s2s_valid_dset[2]\n",
    "qd.split(' context: ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Duke brothers have a lot of money, and they're trying to sell it to the Duke brothers. They're also trying to buy OJ at a high price. They don't want to buy it at a low price, because then they'd have to pay a lot more for it, and that would ruin them. So they buy a ton of OJ, and then sell it for a lot less.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s2s_generate(qd, qa_s2s_model, qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    num_beams=2,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"URL_0  I think this explains it quite well. And you might like this too. It's a great episode of The Simpsons. Edit: I've heard it described in other comments as a stock market scam. I find this to be hilarious. edit2: Edit3: I find it hilarious. I've seen this described as a crime syndicate or a scam artist.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s2s_generate(qd, qa_s2s_model, qa_s2s_tokenizer,\n",
    "    num_answers=1,\n",
    "    do_sample=True,\n",
    "    temp=0.7,\n",
    "    top_p=0.95,\n",
    "    max_input_length=512,\n",
    "    device=\"cuda:0\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.744018793106079\n",
      "10 28.753477811813354\n",
      "20 53.937344551086426\n",
      "30 78.60568451881409\n",
      "40 104.10094237327576\n",
      "50 128.5769989490509\n",
      "60 154.27992796897888\n",
      "70 179.06538081169128\n",
      "80 203.8809735774994\n",
      "90 230.13178277015686\n",
      "100 256.45514965057373\n",
      "110 282.3637926578522\n",
      "120 307.88806414604187\n",
      "130 333.8857891559601\n",
      "140 360.0709044933319\n",
      "150 386.02467918395996\n",
      "160 414.57378482818604\n",
      "170 442.61042857170105\n",
      "180 469.20493245124817\n",
      "190 494.8593752384186\n",
      "200 520.1254770755768\n",
      "210 545.6333200931549\n",
      "220 572.047559261322\n",
      "230 597.3021047115326\n",
      "240 622.8987302780151\n",
      "250 648.7013621330261\n",
      "260 674.3510477542877\n",
      "270 700.575044631958\n",
      "280 728.2109053134918\n",
      "290 754.1497228145599\n",
      "300 782.2213447093964\n",
      "310 808.7190420627594\n",
      "320 835.6998612880707\n",
      "330 863.1649961471558\n",
      "340 889.878288269043\n",
      "350 916.4322776794434\n",
      "360 942.4431409835815\n",
      "370 969.9250016212463\n",
      "380 996.5509266853333\n",
      "390 1022.6301748752594\n",
      "400 1048.5962445735931\n",
      "410 1074.9585995674133\n",
      "420 1101.2033276557922\n",
      "430 1128.582790851593\n",
      "440 1155.4053695201874\n",
      "450 1181.374934911728\n",
      "460 1209.5674030780792\n",
      "470 1239.436999320984\n",
      "480 1266.2141463756561\n",
      "490 1293.4241681098938\n",
      "500 1323.0100865364075\n",
      "510 1350.4561829566956\n",
      "520 1377.4305746555328\n",
      "530 1404.9353828430176\n",
      "540 1433.87153840065\n",
      "550 1460.307555437088\n",
      "560 1487.960082769394\n",
      "570 1515.521205663681\n",
      "580 1543.5963864326477\n",
      "590 1573.0112266540527\n",
      "600 1600.4035971164703\n",
      "610 1627.6757376194\n",
      "620 1654.3252892494202\n",
      "630 1682.4718887805939\n",
      "640 1709.9222922325134\n",
      "650 1737.107579946518\n",
      "660 1764.1454610824585\n",
      "670 1791.2165014743805\n",
      "680 1817.7742636203766\n",
      "690 1845.635470867157\n",
      "700 1873.0992829799652\n",
      "710 1901.7115211486816\n",
      "720 1927.130925655365\n",
      "730 1951.9790225028992\n",
      "740 1977.2427294254303\n",
      "750 2002.2097487449646\n",
      "760 2027.4377732276917\n",
      "770 2052.4029853343964\n",
      "780 2077.7944264411926\n",
      "790 2102.801197052002\n",
      "800 2127.6833171844482\n",
      "810 2152.740152835846\n",
      "820 2178.2457463741302\n",
      "830 2204.2031724452972\n",
      "840 2232.528069972992\n",
      "850 2258.7690773010254\n",
      "860 2285.871170759201\n",
      "870 2315.1624064445496\n",
      "880 2340.7701206207275\n",
      "890 2366.4515142440796\n",
      "900 2393.0948173999786\n",
      "910 2420.472536802292\n",
      "920 2447.071256875992\n",
      "930 2473.613833665848\n",
      "940 2499.4607396125793\n",
      "950 2526.311693906784\n",
      "960 2554.4685385227203\n",
      "970 2579.7506012916565\n",
      "980 2605.2748894691467\n",
      "990 2631.0144391059875\n",
      "1000 2656.9201147556305\n",
      "1010 2683.0055282115936\n",
      "1020 2709.3055386543274\n",
      "1030 2737.6859900951385\n",
      "1040 2763.7142944335938\n",
      "1050 2789.0459439754486\n",
      "1060 2814.8332393169403\n",
      "1070 2840.2304253578186\n",
      "1080 2866.625965356827\n",
      "1090 2892.9246814250946\n",
      "1100 2919.089020729065\n",
      "1110 2945.8848555088043\n",
      "1120 2972.2632100582123\n",
      "1130 2997.30664563179\n",
      "1140 3023.3492093086243\n",
      "1150 3049.4997234344482\n",
      "1160 3075.5515332221985\n",
      "1170 3102.460287332535\n",
      "1180 3128.5416400432587\n",
      "1190 3154.9566304683685\n",
      "1200 3182.275478363037\n",
      "1210 3210.170147418976\n",
      "1220 3237.19616150856\n",
      "1230 3265.23552775383\n",
      "1240 3292.8912405967712\n",
      "1250 3319.3299787044525\n",
      "1260 3346.78577876091\n",
      "1270 3373.1819043159485\n",
      "1280 3399.9515645504\n",
      "1290 3427.249433994293\n",
      "1300 3453.518622636795\n",
      "1310 3480.9237117767334\n",
      "1320 3508.3716559410095\n",
      "1330 3535.7298963069916\n",
      "1340 3562.5147728919983\n",
      "1350 3588.7741491794586\n",
      "1360 3614.5221784114838\n",
      "1370 3640.129417657852\n",
      "1380 3669.498962879181\n",
      "1390 3696.5501189231873\n",
      "1400 3722.4800505638123\n",
      "1410 3748.4080562591553\n",
      "1420 3776.459785223007\n",
      "1430 3803.3505222797394\n",
      "1440 3829.5545308589935\n",
      "1450 3855.114069700241\n",
      "1460 3880.5141694545746\n",
      "1470 3906.3400807380676\n",
      "1480 3932.078344345093\n",
      "1490 3957.9697120189667\n",
      "1500 3985.017519712448\n",
      "1510 4011.2201488018036\n",
      "1520 4036.657493829727\n",
      "1530 4064.183616399765\n",
      "1540 4089.862156152725\n",
      "1550 4116.76051568985\n",
      "1560 4142.292782783508\n",
      "1570 4168.367073535919\n",
      "1580 4194.065651655197\n",
      "1590 4219.453572750092\n",
      "1600 4247.7000732421875\n",
      "1610 4273.099565982819\n",
      "1620 4298.799532651901\n",
      "1630 4325.582900285721\n",
      "1640 4351.650647640228\n",
      "1650 4376.849317550659\n",
      "1660 4402.234132528305\n",
      "1670 4428.097621917725\n",
      "1680 4453.97546005249\n",
      "1690 4479.820170164108\n",
      "1700 4505.252448558807\n",
      "1710 4531.0626809597015\n",
      "1720 4557.19069314003\n",
      "1730 4583.0620086193085\n",
      "1740 4608.326864242554\n",
      "1750 4634.507502555847\n",
      "1760 4660.427661895752\n",
      "1770 4687.156256914139\n",
      "1780 4714.643468141556\n",
      "1790 4740.312047719955\n",
      "1800 4765.948339939117\n",
      "1810 4791.701061964035\n",
      "1820 4816.815726518631\n",
      "1830 4843.062593221664\n",
      "1840 4868.415763616562\n",
      "1850 4894.474339723587\n",
      "1860 4920.188796520233\n",
      "1870 4944.919880151749\n",
      "1880 4970.805044412613\n",
      "1890 4998.5492470264435\n",
      "1900 5024.655067443848\n",
      "1910 5050.2032334804535\n",
      "1920 5076.066184043884\n",
      "1930 5102.129430532455\n",
      "1940 5127.503154039383\n",
      "1950 5153.307322978973\n",
      "1960 5181.128698587418\n",
      "1970 5207.469444274902\n",
      "1980 5232.658756494522\n",
      "1990 5260.044815063477\n"
     ]
    }
   ],
   "source": [
    "st_time = time()\n",
    "examples_with_generations = []\n",
    "for i in range(2000):\n",
    "    qd, a = s2s_valid_dset[i]\n",
    "    beam = qa_s2s_generate(\n",
    "        qd, qa_s2s_model, qa_s2s_tokenizer,\n",
    "        num_answers=1,\n",
    "        num_beams=2,\n",
    "        min_len=128,\n",
    "        max_input_length=512,\n",
    "        device=\"cuda:0\"\n",
    "    )[0]\n",
    "    examples_with_generations += [(qd.split(' context: ')[0], a, beam)]\n",
    "    if i % 10 == 0:\n",
    "        print(i, time() - st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"question: why we can't transplant intestines it seems like we can transplant anything these days from hearts, livers, penises and even faces! i'm an ulcerative colitis patient and always wondered why it isn't possible.\",\n",
       " \"We can transplant intestines. However, ulcerative colitis is poor candidate because it is thought to be mostly caused by your own immune system attacking your intestines. They will probably attack someone else's intestines just as badly, so the disease will just reoccur. When you get a transplant, you usually have to be on powerful immunosuppressive drugs for the rest of your life to prevent rejection of the organ. But, immunosuppressives are also a treatment for ulcerative colitis itself. If the immunosuppressives were effective for you and the dangers of them seemed warranted, you would just take them in the first place and skip the transplant. If they don't work for you, then the transplant probably won't either.\",\n",
       " \"It's a lot easier to transplant a heart than an intestine. It's easier to get a heart from a liver, liver, kidney, pancreas, etc. than it is to get an intestine from an intestines. The intestines are a lot more complicated than a heart or liver, so it's much harder to transplant them. It also takes a lot of time and money to do it, and it's not really a good idea to do that. It would be like trying to transplant an organ from a heart, liver or kidney. You'd have to wait a long time to get it done, and then you'd need to wait even longer to get the organ back.\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice(examples_with_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"question: why do we have to build earth re-entry capsules for future space missions? - why can't we use the iss? why do we have to build earth re-entry capsules for future space missions? (all the future mars and asteroid ones) the capsules are heavy and difficult. why can't the mars trips for example just rendezvous with the international space station and then just come back to earth on a soyuz? wouldn't this be easier/cheaper/lighter? *edit:* thanks for all the great explanations.\",\n",
       " 'Because stopping in space isn\\'t easy. For every bit of outward acceleration you build up as you head out, you have to turn around and stop it or slow it down enough to land or orbit. The same holds true on the way back. Depending on your flight plan, decelerating and matching orbits or docking chews up half or more of your fuel budget. It is much easier and consumes much less \"reaction mass\" if you take a man-containing craft and plummet it through the earth\\'s atmosphere to allow it to lose a lot of its velocity via aerobraking friction. And the number of directions (vectors) that you can come in at and have a successful landing is pretty high. If instead you were to try and dock at a space station, you\\'d have to slow down from your interplanetary flight at just the right angle and vector in order to match it and dock, potentially requiring a LOT of fuel, which means either a novel fuel source that is super-high in energy-to-weight and energy-to-volume ratios, or a really big ship with really big tanks.',\n",
       " \"The ISS is too big and expensive to use. It's a lot easier to just send a Soyuz or Soyuz-class capsule to the ISS and then come back to Earth. It would be a lot cheaper to just use the ISS, but it would be much more expensive to send a capsule to Mars, which is a lot more expensive. Also, the ISS has a lot of stuff on it, and the ISS is a pretty big place. The ISS isn't going to be able to be used for a long period of time, so it's not worth the cost to use it for a few years. The Mars trip is going to take a long time, and it's going to have a lot less stuff on board.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice(examples_with_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "from rouge import Rouge\n",
    "from spacy.lang.en import English\n",
    "from time import time\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "rouge = Rouge()\n",
    "tokenizer = English().Defaults.create_tokenizer()\n",
    "rouge_metric = nlp.load_metric('rouge')\n",
    "\n",
    "def compute_rouge_eli5(compare_list):\n",
    "    preds = [\" \".join([stemmer.stem(str(w))\n",
    "                       for w in tokenizer(pred)])\n",
    "             for gold, pred in compare_list]\n",
    "    golds = [\" \".join([stemmer.stem(str(w))\n",
    "                       for w in tokenizer(gold)])\n",
    "             for gold, pred in compare_list]\n",
    "    scores = rouge.get_scores(preds, golds, avg=True)\n",
    "    return scores\n",
    "\n",
    "def compute_rouge_nlp(compare_list):\n",
    "    refs = [\" \".join([stemmer.stem(str(w))\n",
    "                      for w in tokenizer(a.replace('\\n', ''))]) \n",
    "            for a, b in compare_list]\n",
    "    preds = [\" \".join([stemmer.stem(str(w))\n",
    "                       for w in tokenizer(b.replace('\\n', ''))]) \n",
    "             for a, b in compare_list]\n",
    "    scores = rouge_metric.compute(preds, refs, rouge_types = ['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-1 \t 0.267\n",
      "R-2 \t 0.056\n",
      "R-l \t 0.267\n"
     ]
    }
   ],
   "source": [
    "rouge_res = compute_rouge_eli5([(a, b) for q, a, b in examples_with_generations])\n",
    "for t in ['1', '2', 'l']:\n",
    "    print('R-{} \\t {:.3f}'.format(t, rouge_res['rouge-{}'.format(t)]['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-1 \t 0.264\n",
      "R-2 \t 0.056\n",
      "R-L \t 0.152\n"
     ]
    }
   ],
   "source": [
    "rouge_res = rouge_res = compute_rouge_nlp([(a, b) for q, a, b in examples_with_generations])\n",
    "for t in ['1', '2', 'L']:\n",
    "    print('R-{} \\t {:.3f}'.format(t, rouge_res['rouge{}'.format(t)].mid.fmeasure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the retriever discriminate between generated and gold answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = qa_s2s_model.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
    "    model_name=\"google/bert_uncased_L-8_H-512_A-8\",\n",
    "    from_file='{}_{}.pth'.format(\"retriever_models/eli5_retriever_model_512\", 9),\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    q_reps = embed_question_for_retrieval(\n",
    "        [q for q, a, b in examples_with_generations],\n",
    "        qar_tokenizer, qar_model, device='cuda:0'\n",
    "    )\n",
    "    a_reps = embed_passages_for_retrieval(\n",
    "        {'passage_text': [a for q, a, b in examples_with_generations]},\n",
    "        qar_tokenizer, qar_model, device='cuda:0'\n",
    "    )\n",
    "    b_reps = embed_passages_for_retrieval(\n",
    "        {'passage_text': [b for q, a, b in examples_with_generations]},\n",
    "        qar_tokenizer, qar_model, device='cuda:0'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.275"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((q_reps * a_reps).sum(axis=-1) - (q_reps * b_reps).sum(axis=-1)) > 0).sum() / len(examples_with_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
