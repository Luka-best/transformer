{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5_utils import *\n",
    "\n",
    "eli5_dbuilder = ELI5NLP(data_dir='eli5')\n",
    "eli5_dbuilder.download_and_prepare()\n",
    "\n",
    "eli5_train = eli5_dbuilder.as_dataset(split=nlp.splits.Split.TRAIN)\n",
    "eli5_valid = eli5_dbuilder.as_dataset(split=nlp.splits.Split.VALIDATION)\n",
    "eli5_test = eli5_dbuilder.as_dataset(split=nlp.splits.Split.TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0     0 of   539 \t L: 6.342 \t -- 9.093\n",
      " 0    10 of   539 \t L: 6.347 \t -- 95.173\n",
      " 0    20 of   539 \t L: 6.308 \t -- 182.368\n",
      " 0    30 of   539 \t L: 6.243 \t -- 271.287\n",
      " 0    40 of   539 \t L: 6.161 \t -- 362.347\n",
      " 0    50 of   539 \t L: 6.010 \t -- 454.841\n",
      " 0    60 of   539 \t L: 5.733 \t -- 545.507\n",
      " 0    70 of   539 \t L: 5.391 \t -- 638.503\n",
      " 0    80 of   539 \t L: 4.973 \t -- 729.353\n",
      " 0    90 of   539 \t L: 4.670 \t -- 818.165\n",
      " 0   100 of   539 \t L: 4.343 \t -- 908.974\n",
      " 0   110 of   539 \t L: 4.084 \t -- 997.924\n",
      " 0   120 of   539 \t L: 3.877 \t -- 1086.849\n",
      " 0   130 of   539 \t L: 3.733 \t -- 1176.005\n",
      " 0   140 of   539 \t L: 3.645 \t -- 1264.747\n",
      " 0   150 of   539 \t L: 3.508 \t -- 1353.476\n",
      " 0   160 of   539 \t L: 3.459 \t -- 1443.157\n",
      " 0   170 of   539 \t L: 3.384 \t -- 1530.991\n",
      " 0   180 of   539 \t L: 3.237 \t -- 1619.006\n",
      " 0   190 of   539 \t L: 3.172 \t -- 1706.880\n",
      " 0   200 of   539 \t L: 3.150 \t -- 1794.816\n",
      " 0   210 of   539 \t L: 3.081 \t -- 1883.141\n",
      " 0   220 of   539 \t L: 3.028 \t -- 1972.121\n",
      " 0   230 of   539 \t L: 2.967 \t -- 2062.746\n",
      " 0   240 of   539 \t L: 2.911 \t -- 2155.731\n",
      " 0   250 of   539 \t L: 2.875 \t -- 2246.224\n",
      " 0   260 of   539 \t L: 2.822 \t -- 2337.107\n"
     ]
    }
   ],
   "source": [
    "class ArgumentsQAR():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 2048\n",
    "        self.max_length = 128\n",
    "        self.checkpoint_batch_size = 256\n",
    "        self.print_freq = 10\n",
    "        self.pretrained_model_name = \"google/bert_uncased_L-8_H-512_A-8\"\n",
    "        self.model_save_name = \"retriever_models/eli5_retriever_model_2048\"\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_epochs = 20\n",
    "\n",
    "qar_args = ArgumentsQAR()\n",
    "\n",
    "qar_train_dset = ELI5DatasetQARetriver(eli5_train, min_answer_length=64, training=True)\n",
    "qar_valid_dset = ELI5DatasetQARetriver(eli5_valid, min_answer_length=64, training=False)\n",
    "\n",
    "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
    "    model_name=qar_args.pretrained_model_name,\n",
    "    from_file=None,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "qar_optimizer = AdamW(qar_model.parameters(), lr=qar_args.learning_rate, eps=1e-8)\n",
    "qar_scheduler = get_linear_schedule_with_warmup(\n",
    "        qar_optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=qar_args.num_epochs * math.ceil(len(qar_train_dset) / qar_args.batch_size)\n",
    ")\n",
    "\n",
    "for e in range(qar_args.num_epochs):\n",
    "    train_qa_retriever_epoch(\n",
    "        qar_model, qar_train_dset, qar_tokenizer,\n",
    "        qar_optimizer, qar_scheduler, qar_args, e\n",
    "    )\n",
    "    m_save_dict = {\n",
    "        'model': qar_model.state_dict(),\n",
    "        'optimizer': qar_optimizer.state_dict(),\n",
    "        'scheduler': qar_scheduler.state_dict(),\n",
    "    }\n",
    "    print(\"Saving model {}\".format(qar_args.model_save_name))\n",
    "    torch.save(m_save_dict, '{}_{}.pth'.format(qar_args.model_save_name, e))\n",
    "    eval_loss = evaluate_qa_retriever(qar_model, qar_valid_dset, qar_tokenizer, qar_args)\n",
    "    print(\"Evaluation loss epoch {:4d}: {:.3f}\".format(e, eval_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate recall@N for validation / test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used train rtriever to index Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5_utils import *\n",
    "\n",
    "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
    "    model_name=\"google/bert_uncased_L-8_H-512_A-8\",\n",
    "    from_file='{}_{}.pth'.format(\"retriever_models/eli5_retriever_model_512\", 9),\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "kilt_snippets_dbuilder = KiltSnippets(data_dir='kilt_snippets_100w')\n",
    "kilt_snippets_dbuilder.download_and_prepare()\n",
    "wiki_passages = kilt_snippets_dbuilder.as_dataset(split=nlp.splits.Split.TRAIN)\n",
    "\n",
    "make_qa_dense_index(qar_model, qar_tokenizer,\n",
    "                    wiki_passages,\n",
    "                    batch_size=512, max_length=96,\n",
    "                    index_name='kilt_passages_reps_16.dat',\n",
    "                    device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5_utils import *\n",
    "\n",
    "eli5_dbuilder = ELI5NLP(data_dir='eli5')\n",
    "eli5_dbuilder.download_and_prepare()\n",
    "\n",
    "eli5_train = eli5_dbuilder.as_dataset(split=nlp.splits.Split.TRAIN)\n",
    "eli5_valid = eli5_dbuilder.as_dataset(split=nlp.splits.Split.VALIDATION)\n",
    "eli5_test = eli5_dbuilder.as_dataset(split=nlp.splits.Split.TEST)\n",
    "\n",
    "eli5_train_docs = json.load(open('eli5_train_precomputed_dense_docs.json'))\n",
    "eli5_valid_docs = json.load(open('eli5_valid_precomputed_dense_docs.json'))\n",
    "eli5_test_docs = json.load(open('eli5_test_precomputed_dense_docs.json'))\n",
    "\n",
    "s2s_train_dset = ELI5DatasetS2S(eli5_train, document_cache=dict([(k, d) for k, d, src_ls in eli5_train_docs]))\n",
    "s2s_valid_dset = ELI5DatasetS2S(eli5_valid, document_cache=dict([(k, d) for k, d, src_ls in eli5_valid_docs]), training=False)\n",
    "\n",
    "qa_s2s_tokenizer, qa_s2s_model = make_qa_s2s_model(\n",
    "    model_name=\"bart-large\",\n",
    "    from_file=None,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "class ArgumentsS2S():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 1\n",
    "        self.backward_freq = 16\n",
    "        self.max_length = 1024\n",
    "        self.print_freq = 1000\n",
    "        self.model_save_name = \"seq2seq_models/eli5_bart_model_512\"\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_epochs = 10\n",
    "\n",
    "s2s_args = ArgumentsS2S()\n",
    "s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n",
    "s2s_scheduler = get_linear_schedule_with_warmup(\n",
    "        s2s_optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=s2s_args.num_epochs * math.ceil(len(s2s_train_dset) / s2s_args.batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(s2s_args.num_epochs):\n",
    "    train_qa_s2s_epoch(\n",
    "        qa_s2s_model,\n",
    "        s2s_train_dset, qa_s2s_tokenizer,\n",
    "        s2s_optimizer, s2s_scheduler,\n",
    "        s2s_args, e\n",
    "    )\n",
    "    m_save_dict = {\n",
    "        'model': qa_s2s_model.state_dict(),\n",
    "        'optimizer': s2s_optimizer.state_dict(),\n",
    "        'scheduler': s2s_scheduler.state_dict(),\n",
    "    }\n",
    "    print(\"Saving model {}\".format(s2s_args.model_save_name))\n",
    "    torch.save(m_save_dict, '{}_{}.pth'.format(s2s_args.model_save_name, e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
