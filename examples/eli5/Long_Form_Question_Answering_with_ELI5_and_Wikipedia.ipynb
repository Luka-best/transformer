{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Form Question Answering with ELI5 and Wikipedia  \n",
    "\n",
    "---  \n",
    "\n",
    "### Table of Contents  \n",
    "\n",
    "1. [Introduction](#intro)  \n",
    "2. [Task and Data Description](#task_description)  \n",
    "3. [Retrieving Support Documents](#retrieval)  \n",
    "    a. [Sparse Retrieval with ElasticSearch](#elasticsearch)  \n",
    "    b. [Using a Trained Dense Retriever](#dense_train)  \n",
    "    c. [Dense Retriever Evaluation](#dense_eval)  \n",
    "4. [Answer Generation Model](#generation)  \n",
    "    a. [Conditional Generation with Seq2seq Models](#seq2seq_presentation)  \n",
    "    b. [Fine-Tuning Seq2seq Models](#seq2seq_train)  \n",
    "5. [Conclusion](#conclusion)  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"images/choco_bis.svg\" width=\"900\" align=\"center\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<a id='intro'></a>\n",
    "\n",
    "Imagine that you are taken with a sudden desire to understand **how the fruit of a tropical tree gets transformed into chocolate bars**, or want to understand **the role of fever in the human body's immune response**: how would you go about finding that information?\n",
    "\n",
    "If your specific question has already been asked and provided a clear and succint answer on one of the many question answering platforms answering on the Internet (such as [**Quora**](https://www.quora.com/How-is-chocolate-made), [**Reddit**](https://www.reddit.com/user/ex_5_libris/comments/9c8gb1/chocolate_how_chocolate_is_made/), or [**Yahoo Answers**](https://answers.yahoo.com/question/index?qid=20070615082202AArsYN1)), you're in luck: modern search engine will probably take you to that pre-existing answer pretty reliably. Otherwise, the process will be a little more involved. You will likely have to collect relevant information from a variety of sources, figure out how these pieces of knowledge fit together in relation to your query, and synthetize a narrative that answers your initial question.\n",
    "\n",
    "Now, wouldn't it be great if your computer could do all of that for you: **gather** the right sources, **synthetize** the information, and **write up** an easy-to-read summary of the relevant points? The bad news is: such a system isn't quite available yet, at least not one that can provide *reliable* information in its summary. The good news on the other hand: a number of recent advances in natural language understanding and generation have made working toward solving this task much easier. These advances include progress in the pre-training (e.g. [BART](https://arxiv.org/abs/1910.13461), [T5](https://arxiv.org/abs/1910.10683)) and evaluation (e.g. for [factuality](https://arxiv.org/abs/2004.04228)) of sequence-to-sequence models used for conditional text generation, new ways to use these models to find information in Wikipedia (e.g. [REALM](https://kentonl.com/pub/gltpc.2020.pdf), [DPR](https://arxiv.org/abs/2004.04906)), and new [training datasets](https://arxiv.org/abs/1907.09190).\n",
    "\n",
    "**In this notebook,** we show how we can take advantage of some of these recent works to train a **long form question answering** system which takes in a question, fetches 10 relevant passages from a [Wikipedia snapshot](https://www.aclweb.org/anthology/2020.lrec-1.297/), and writes a multi-sentence answer based on the question and retrieved passages. Follow along to learn about the steps involved and read some background on the state of the art for some related tasks, or go straight to the:  \n",
    "## [**Live Demo!**](http://35.226.96.115:8080/)  \n",
    "(And don't forget to scroll down on the left sidebar to show all of the generation options!)\n",
    "\n",
    "#### Preliminaries  \n",
    "\n",
    "The implementation presented here relies on the [HuggingFace](https://huggingface.co/) [ðŸ¤—transformers](https://github.com/huggingface/transformers) and [ðŸ¤—nlp](https://github.com/huggingface/nlp) libraries. Wikipedia indexing relies on [ElasticSearch](https://www.elastic.co/elasticsearch) with its [python bindings](https://github.com/elastic/elasticsearch-py) for the sparse version, and [faiss](https://github.com/facebookresearch/faiss/) for the dense version. You can get all of these by running:\n",
    "> pip install elasticsearch  \n",
    "> pip install faiss_gpu  \n",
    "> pip install nlp  \n",
    "> pip install transformers  \n",
    ">  \n",
    "> wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.7.1-linux-x86_64.tar.gz  \n",
    "> tar -xzvf elasticsearch-7.7.1-linux-x86_64.tar.gz  \n",
    "\n",
    "The training relies on two datasets: [ELI5](https://arxiv.org/abs/1907.09190), a processed version of the [r/explainlikeimfive](https://www.reddit.com/r/explainlikeimfive/) subreddit, and the [Wiki40b](https://www.aclweb.org/anthology/2020.lrec-1.297/) Wikipedia image. Downloading these and splitting Wikipedia into snippets for indexing can take a long time (up to 72 hours for the ELI5 dataset creation, which has to filter through all of the Reddit dumps). We suggest that you start by downloading and pre-processing these as follows before doing anything else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "\n",
    "wiki40b_snippets = nlp.load_dataset('wiki_snippets', name='wiki40b_en_100_0', experimental=True)['train']\n",
    "eli5 = nlp.load_dataset('explainlikeimfive', name='LFQA_reddit', experimental=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task and Data Description\n",
    "<a id='task_description'></a>\n",
    "\n",
    "The task of Long Form Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '37a8or',\n",
       " 'title': 'Why is Google Fibre taking so long to roll out?',\n",
       " 'selftext': '',\n",
       " 'document': '',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers': {'a_id': ['crkzpne',\n",
       "   'crl0n7a',\n",
       "   'crkzcxh',\n",
       "   'crkyyph',\n",
       "   'crl3fkq',\n",
       "   'crl8oid'],\n",
       "  'text': [\"One does not simply lay down a large fiber network. First, you have to have the money. That's not really an issue for Google. Then, you have to convince municipal governments to let you build a network, and you have to get past the incumbent ISP, who wants to keep their monopoly intact. You have to find enough subscribers, you have to find people to build the network, you have to do customer service and installation, and you have to not be hated by the public. Throwing money at those problems is ineffective.\",\n",
       "   \"I'm in Georgia so hopefully I'll see fiber within my lifetime.  \\n\\nConsidering that we've paid for fiber to the home twice over.  Please see the $300 billion broadband scandal.  \\n\\nThe fiber is actually the cheap part.. the labor is probably the most expensive part to it. \\n\\nThe slowest part is all the paper work and fighting the counties and att and the cable providers.  I was alSo told that getting across a railroad takes 5 months of paperwork and $40k\\n\\nBasically because the local monopolies have so much money,  they pay people on the board to vote against public interest.   \\n\\nSo they tie Google up in legal paperwork and local monopolies get it blocked.  \\n\\nI really wish Google would've hired me for the Atlanta fiber team.  I would've been keeping everyone updated to what's taking so long.   \\n\\nI'm pretty sure I  could beast through all this political garbage. \\n\\nThe fastest way through red tape is to steamroll through it. :)\",\n",
       "   \"One of two possibilities, firstly Google doesn't really want to be an ISP but wants to coax other ISP's into rolling out substantial faster internet services.  \\n\\nSecondly Google can't roll out fiber everywhere it wants by throwing money at it. Who gets Google fiber and when might not be in Googles control.\",\n",
       "   'Putting stuff in the ground takes time.\\n\\nOn top of that there\\'s a lot of paperwork, city regulations and so on. That\\'s part of the reason why Google Fibre isn\\'t going to be everywhere, Google are only picking cities which are \"easy\" for it to roll out in.',\n",
       "   'Bureaucracy, Politics, and money are the reason. Google has the money, but most local governments have some sort of contracts with 1 or more ISPs that essentially grant them a monopoly(or a near monopoly) over certain areas.\\n\\nThis is the reason why a lot of ISPs have failed and companies like Comcast refuse to increase bandwidth because they know they have a monopoly and they have enough money to get the local city officials on their side and shut down competitors.',\n",
       "   'There are a few factors:\\n\\n * Installing the fibre is expensive, so google will only roll it out where there is enough market to support it (or where there will be enough market during the lifetime).  Add to this that as soon as Google moves to put in fibre, the existing providers all drop their prices and try to lock in customers contractually to prevent google from succeeding.\\n\\n * Planning can be hard to get.  Some local governments already granted monopolies to other providers, others don\\'t want the hassle, still others are getting kickbacks from existing suppliers to \"stand on the hose\".\\n\\n * The idea for google is not to actually roll out it\\'s fibre nationwide.  That\\'s not the business they want to be in.  The idea is to force existing providers to up their game and do their actual jobs nationwide.  It\\'s sort of like how google are not in the bus business and don\\'t want to be.  But they run their own buses because otherwise there is no way for the workers to get to work on time.  Google does not want to be your ISP or an infrastructure company.  But since your ISP and Infra companies are so bad, it\\'s having to do their job for them just to get their product to you.'],\n",
       "  'score': [45, 16, 13, 11, 5, 3]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': []},\n",
       " 'answers_urls': {'url': []}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5['validation_eli5'][123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Support Documents\n",
    "<a id='retrieval'></a>\n",
    "\n",
    "The first question is...\n",
    "\n",
    "### Sparse Retrieval with ElasticSearch\n",
    "<a id='elasticsearch'></a>\n",
    "\n",
    "The traditional approach until...  \n",
    "\n",
    "First, let's create a dense index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5_utils import *\n",
    "\n",
    "es_client = Elasticsearch([{'host': 'localhost', 'port': '9200'}])\n",
    "if not es_client.indices.exists('wiki40b_snippets_100w'):\n",
    "    make_es_index_snippets(es_client, wiki40b_snippets, index_name='wiki40b_snippets_100w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test for one of the ELI5 questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is Google Fibre taking so long to roll out?\n",
      "-----\n",
      "\n",
      "Internet in New Zealand: \n",
      "  Local loop unbundling and the structural separation of Telecom & Recent developments\n",
      "\n",
      "Bharat Broadband Network: \n",
      "  BharatNet Phase-II (Dec 2018)\n",
      "\n",
      "Google Voice Search: \n",
      "  Google Voice Search on Google.com & History\n",
      "\n",
      "Internet in New Zealand: \n",
      "  DSL & Fibre\n",
      "\n",
      "Roving: \n",
      "  Roving\n",
      "\n",
      "Rolag: \n",
      "  Rolag\n",
      "\n",
      "NBN Co: \n",
      "  National Broadband Network\n",
      "\n",
      "Digital loop carrier: \n",
      "  Configuration\n",
      "\n",
      "After School Club: \n",
      "  Premise and format\n",
      "\n",
      "MNSi Telecom: \n",
      "  Acquisitions & Introduction to fibre\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = eli5['validation_eli5'][123]['title']\n",
    "doc, res_list = query_es_index(question, es_client, index_name='wiki40b_snippets_100w', n_results=10)\n",
    "\n",
    "print(question)\n",
    "print('-----\\n')\n",
    "for res in res_list:\n",
    "    print(\"{}: \\n  {}\\n\".format(\n",
    "        res['article_title'],\n",
    "        res['section_title'] if res['section_title'].strip() != '' else res['article_title']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Trained Dense Retriever\n",
    "<a id='dense_train'></a>\n",
    "\n",
    "Can we take advantage of our data to do better?\n",
    "\n",
    "### Dense Retriever Evaluation\n",
    "<a id='dense_eval'></a>\n",
    "\n",
    "How can we evaluate the embedding model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Generation Model\n",
    "<a id='generation'></a>\n",
    "\n",
    "Once we have a question and a document containing\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
