{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Form Question Answering with ELI5 and Wikipedia  \n",
    "\n",
    "---  \n",
    "\n",
    "### Table of Contents  \n",
    "\n",
    "1. [Introduction](#intro)  \n",
    "2. [Task and Data Description](#task_description)  \n",
    "3. [Retrieving Support Documents](#retrieval)  \n",
    "    a. [Sparse Retrieval with ElasticSearch](#elasticsearch)  \n",
    "    b. [Training a Dense Retriever with ELI5 and in-batch Negatives](#dense_train)  \n",
    "    c. [Using a Trained Dense Retriever](#dense_use)  \n",
    "    d. [Retriever Evaluation](#dense_eval)  \n",
    "4. [Answer Generation Model](#generation)  \n",
    "    a. [Conditional Generation with Seq2seq Models](#seq2seq_presentation)  \n",
    "    b. [Fine-Tuning Seq2seq Models](#seq2seq_train)  \n",
    "5. [Conclusion](#conclusion)  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"images/choco_bis.svg\" width=\"900\" align=\"center\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<a id='intro'></a>\n",
    "\n",
    "Imagine that you are taken with a sudden desire to understand **how the fruit of a tropical tree gets transformed into chocolate bars**, or want to understand **the role of fever in the human body's immune response**: how would you go about finding that information?\n",
    "\n",
    "If your specific question has already been asked and provided a clear and succint answer on one of the many question answering platforms answering on the Internet (such as [**Quora**](https://www.quora.com/How-is-chocolate-made), [**Reddit**](https://www.reddit.com/user/ex_5_libris/comments/9c8gb1/chocolate_how_chocolate_is_made/), or [**Yahoo Answers**](https://answers.yahoo.com/question/index?qid=20070615082202AArsYN1)), you're in luck: modern search engine will probably take you to that pre-existing answer pretty reliably. Otherwise, the process will be a little more involved. You will likely have to collect relevant information from a variety of sources, figure out how these pieces of knowledge fit together in relation to your query, and synthetize a narrative that answers your initial question.\n",
    "\n",
    "Now, wouldn't it be great if your computer could do all of that for you: **gather** the right sources, **synthetize** the information, and **write up** an easy-to-read summary of the relevant points? The bad news is: such a system isn't quite available yet, at least not one that can provide *reliable* information in its summary. The good news on the other hand: a number of recent advances in natural language understanding and generation have made working toward solving this task much easier. These advances include progress in the pre-training (e.g. [BART](https://arxiv.org/abs/1910.13461), [T5](https://arxiv.org/abs/1910.10683)) and evaluation (e.g. for [factuality](https://arxiv.org/abs/2004.04228)) of sequence-to-sequence models used for conditional text generation, new ways to use these models to find information in Wikipedia (e.g. [REALM](https://kentonl.com/pub/gltpc.2020.pdf), [DPR](https://arxiv.org/abs/2004.04906)), and new [training datasets](https://arxiv.org/abs/1907.09190).\n",
    "\n",
    "**In this notebook,** we show how we can take advantage of some of these recent works to train a **long form question answering** system which takes in a question, fetches 10 relevant passages from a [Wikipedia snapshot](https://www.aclweb.org/anthology/2020.lrec-1.297/), and writes a multi-sentence answer based on the question and retrieved passages. Follow along to learn about the steps involved and read some background on the state of the art for some related tasks, or go straight to the:  \n",
    "## [**Live Demo!**](http://35.226.96.115:8080/)  \n",
    "(And don't forget to scroll down on the left sidebar to show all of the generation options!)\n",
    "\n",
    "#### Preliminaries  \n",
    "\n",
    "The implementation presented here relies on the [HuggingFace](https://huggingface.co/) [🤗transformers](https://github.com/huggingface/transformers) and [🤗nlp](https://github.com/huggingface/nlp) libraries. Wikipedia indexing relies on [ElasticSearch](https://www.elastic.co/elasticsearch) with its [python bindings](https://github.com/elastic/elasticsearch-py) for the sparse version, and [faiss](https://github.com/facebookresearch/faiss/) for the dense version. You can get all of these by running:\n",
    "> pip install elasticsearch  \n",
    "> pip install faiss_gpu  \n",
    "> pip install nlp  \n",
    "> pip install transformers  \n",
    ">  \n",
    "> wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.7.1-linux-x86_64.tar.gz  \n",
    "> tar -xzvf elasticsearch-7.7.1-linux-x86_64.tar.gz  \n",
    "\n",
    "The training relies on two datasets: [ELI5](https://arxiv.org/abs/1907.09190), a processed version of the [r/explainlikeimfive](https://www.reddit.com/r/explainlikeimfive/) subreddit, and the [Wiki40b](https://www.aclweb.org/anthology/2020.lrec-1.297/) Wikipedia image. Downloading these and splitting Wikipedia into snippets for indexing can take a long time (up to 72 hours for the ELI5 dataset creation, which has to filter through all of the Reddit dumps). We suggest that you start by downloading and pre-processing these as follows before doing anything else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "from eli5_utils import *\n",
    "\n",
    "wiki40b_snippets = nlp.load_dataset('wiki_snippets', name='wiki40b_en_100_0', experimental=True)['train']\n",
    "eli5 = nlp.load_dataset('explainlikeimfive', name='LFQA_reddit', experimental=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task and Data Description\n",
    "<a id='task_description'></a>\n",
    "\n",
    "The task of Long Form Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '8houtx',\n",
       " 'title': 'Why does water heated to room temperature feel colder than the air around it?',\n",
       " 'selftext': '',\n",
       " 'document': '',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers': {'a_id': ['dylcnfk', 'dylcj49'],\n",
       "  'text': [\"Water transfers heat more efficiently than air. When something feels cold it's because heat is being transferred from your skin to whatever you're touching. Since water absorbs the heat more readily than air, it feels colder.\",\n",
       "   \"Air isn't as good at transferring heat compared to something like water or steel (sit on a room temperature steel bench vs. a room temperature wooden bench, and the steel one will feel more cold).\\n\\nWhen you feel cold, what you're feeling is heat being transferred out of you.  If there is no breeze, you feel a certain way.  If there's a breeze, you will get colder faster (because the moving air is pulling the heat away from you), and if you get into water, its quite good at pulling heat from you.   Get out of the water and have a breeze blow on you while you're wet, all of the water starts evaporating, pulling even more heat from you.\"],\n",
       "  'score': [5, 2]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': []},\n",
       " 'answers_urls': {'url': []}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5['test_eli5'][12345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Support Documents\n",
    "<a id='retrieval'></a>\n",
    "\n",
    "The first question is...\n",
    "\n",
    "### Sparse Retrieval with ElasticSearch\n",
    "<a id='elasticsearch'></a>\n",
    "\n",
    "The traditional approach until...  \n",
    "\n",
    "First, let's create a dense index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5_utils import *\n",
    "\n",
    "es_client = Elasticsearch([{'host': 'localhost', 'port': '9200'}])\n",
    "if not es_client.indices.exists('wiki40b_snippets_100w'):\n",
    "    make_es_index_snippets(es_client, wiki40b_snippets, index_name='wiki40b_snippets_100w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test for one of the ELI5 questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why does water heated to room temperature feel colder than the air around it?\n",
      "-----\n",
      "\n",
      "Salt fingering: \n",
      "  Salt fingering\n",
      "\n",
      "Solar water heating: \n",
      "  Flat plate & Evacuated tube\n",
      "\n",
      "Humidifier: \n",
      "  Fixed-installation humidifiers & Problems\n",
      "\n",
      "Drake Landing Solar Community: \n",
      "  How it works & Energy centre\n",
      "\n",
      "Diamond dust: \n",
      "  Characteristics & Formation\n",
      "\n",
      "Effects of global warming on oceans: \n",
      "  Ocean currents\n",
      "\n",
      "Mesoscale convective system: \n",
      "  Lake-effect snow\n",
      "\n",
      "Thermal comfort: \n",
      "  Interplay of temperature and humidity\n",
      "\n",
      "Honyaki: \n",
      "  Traditional process\n",
      "\n",
      "Greywell Tunnel: \n",
      "  SSSI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = eli5['test_eli5'][12345]['title']\n",
    "doc, res_list = query_es_index(question, es_client, index_name='wiki40b_snippets_100w', n_results=10)\n",
    "\n",
    "print(question)\n",
    "print('-----\\n')\n",
    "for res in res_list:\n",
    "    print(\"{}: \\n  {}\\n\".format(\n",
    "        res['article_title'],\n",
    "        res['section_title'] if res['section_title'].strip() != '' else res['article_title']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Dense Retriever with ELI5 and in-batch Negatives\n",
    "<a id='dense_train'></a>\n",
    "\n",
    "Can we take advantage of our data to do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
    "    model_name=\"google/bert_uncased_L-8_H-768_A-12\",\n",
    "    from_file=None,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Trained Dense Retriever\n",
    "<a id='dense_use'></a>\n",
    "\n",
    "Can we take advantage of our data to do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
    "    model_name=\"google/bert_uncased_L-8_H-768_A-12\",\n",
    "    from_file=\"retriever_models/eli5_retriever_model_l-8_h-768_b-512-512_9.pth\",\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_res = faiss.StandardGpuResources()\n",
    "wiki40b_passage_reps = np.memmap(\n",
    "            'wiki40b_passages_reps_32_l-8_h-768_b-512-512.dat',\n",
    "            dtype='float32', mode='r',\n",
    "            shape=(wiki40b_snippets.num_rows, 128)\n",
    ")\n",
    "\n",
    "wiki40b_index_flat = faiss.IndexFlatIP(128)\n",
    "wiki40b_gpu_index = faiss.index_cpu_to_gpu(faiss_res, 1, wiki40b_index_flat)\n",
    "wiki40b_gpu_index.add(wiki40b_passage_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why does water heated to room temperature feel colder than the air around it?\n",
      "-----\n",
      "\n",
      "Fugacity: \n",
      "  History\n",
      "\n",
      "Heat transfer: \n",
      "  Heat transfer in the human body & Evaporative cooling\n",
      "\n",
      "Johan Sandström: \n",
      "  Sandström  Theorem\n",
      "\n",
      "Thermal equilibrium: \n",
      "  Bodies prepared with separately uniform temperatures, then put into purely thermal communication with each other\n",
      "\n",
      "Evaporative cooler: \n",
      "  Physical principles\n",
      "\n",
      "Thermal contact conductance: \n",
      "  Factors influencing contact conductance & Contact pressure\n",
      "\n",
      "Thermodynamic temperature: \n",
      "  The heat of phase changes\n",
      "\n",
      "Temperature: \n",
      "  Local thermodynamic equilibrium & Bodies in thermodynamic equilibrium\n",
      "\n",
      "Tail flick test: \n",
      "  Limitations\n",
      "\n",
      "Latent heat: \n",
      "  Usage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = eli5['test_eli5'][12345]['title']\n",
    "doc, res_list = query_qa_dense_index(\n",
    "    question,\n",
    "    qar_model, qar_tokenizer,\n",
    "    wiki40b_snippets, wiki40b_gpu_index,\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "print(question)\n",
    "print('-----\\n')\n",
    "for res in res_list:\n",
    "    print(\"{}: \\n  {}\\n\".format(\n",
    "        res['article_title'],\n",
    "        res['section_title'] if res['section_title'].strip() != '' else res['article_title']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever Evaluation\n",
    "<a id='dense_eval'></a>\n",
    "\n",
    "How can we evaluate the embedding model? Let's start by grabbing a couple of useful metrics from the `nlp` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010 Sparse: RG-0.2652 BS-0.8053 | Dense: RG-0.2521 BS-0.8141 \t 103.34\n",
      "020 Sparse: RG-0.2657 BS-0.8068 | Dense: RG-0.2647 BS-0.8193 \t 174.52\n",
      "030 Sparse: RG-0.2631 BS-0.8052 | Dense: RG-0.2591 BS-0.8156 \t 261.19\n",
      "040 Sparse: RG-0.2623 BS-0.8049 | Dense: RG-0.2594 BS-0.8149 \t 352.42\n",
      "050 Sparse: RG-0.2660 BS-0.8060 | Dense: RG-0.2639 BS-0.8176 \t 457.43\n",
      "060 Sparse: RG-0.2698 BS-0.8053 | Dense: RG-0.2649 BS-0.8172 \t 540.86\n",
      "070 Sparse: RG-0.2684 BS-0.8058 | Dense: RG-0.2630 BS-0.8187 \t 602.46\n",
      "080 Sparse: RG-0.2671 BS-0.8062 | Dense: RG-0.2640 BS-0.8185 \t 694.04\n",
      "090 Sparse: RG-0.2646 BS-0.8063 | Dense: RG-0.2622 BS-0.8182 \t 763.54\n",
      "100 Sparse: RG-0.2627 BS-0.8058 | Dense: RG-0.2619 BS-0.8190 \t 822.09\n",
      "110 Sparse: RG-0.2646 BS-0.8056 | Dense: RG-0.2626 BS-0.8186 \t 900.97\n",
      "120 Sparse: RG-0.2673 BS-0.8055 | Dense: RG-0.2661 BS-0.8177 \t 1013.28\n",
      "130 Sparse: RG-0.2685 BS-0.8053 | Dense: RG-0.2678 BS-0.8175 \t 1080.84\n",
      "140 Sparse: RG-0.2660 BS-0.8050 | Dense: RG-0.2654 BS-0.8180 \t 1380.19\n",
      "150 Sparse: RG-0.2646 BS-0.8049 | Dense: RG-0.2639 BS-0.8172 \t 1466.71\n",
      "160 Sparse: RG-0.2639 BS-0.8051 | Dense: RG-0.2648 BS-0.8178 \t 1607.58\n",
      "170 Sparse: RG-0.2612 BS-0.8053 | Dense: RG-0.2618 BS-0.8174 \t 1677.37\n",
      "180 Sparse: RG-0.2616 BS-0.8056 | Dense: RG-0.2627 BS-0.8168 \t 1779.52\n",
      "190 Sparse: RG-0.2614 BS-0.8055 | Dense: RG-0.2626 BS-0.8166 \t 1850.76\n",
      "200 Sparse: RG-0.2610 BS-0.8056 | Dense: RG-0.2641 BS-0.8177 \t 1971.58\n",
      "210 Sparse: RG-0.2620 BS-0.8057 | Dense: RG-0.2649 BS-0.8175 \t 2082.52\n",
      "220 Sparse: RG-0.2620 BS-0.8058 | Dense: RG-0.2638 BS-0.8169 \t 2129.78\n",
      "230 Sparse: RG-0.2626 BS-0.8060 | Dense: RG-0.2639 BS-0.8168 \t 2187.62\n",
      "240 Sparse: RG-0.2625 BS-0.8063 | Dense: RG-0.2638 BS-0.8169 \t 2264.72\n",
      "250 Sparse: RG-0.2628 BS-0.8066 | Dense: RG-0.2634 BS-0.8167 \t 2366.96\n",
      "260 Sparse: RG-0.2632 BS-0.8064 | Dense: RG-0.2633 BS-0.8166 \t 2441.70\n",
      "270 Sparse: RG-0.2635 BS-0.8063 | Dense: RG-0.2635 BS-0.8164 \t 2536.18\n",
      "280 Sparse: RG-0.2648 BS-0.8064 | Dense: RG-0.2641 BS-0.8172 \t 2623.53\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-63b330ce8393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mes_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wiki40b_snippets_100w'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mn_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     48\u001b[0m     \u001b[0msparse_passages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'passage_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msparse_res_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/transformers/examples/eli5/eli5_utils.py\u001b[0m in \u001b[0;36mquery_es_index\u001b[0;34m(question, es_client, index_name, n_results)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 }\n\u001b[1;32m     80\u001b[0m             },\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;34m\"size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         }\n\u001b[1;32m     83\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/elasticsearch/client/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, body, index, doc_type, params, headers)\u001b[0m\n\u001b[1;32m   1621\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m         )\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    360\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m                 )\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             response = self.pool.urlopen(\n\u001b[0;32m--> 229\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRetry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             )\n\u001b[1;32m    231\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    419\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# load the ROUGE and BERTscore metrics from the nlp library\n",
    "nlp_rouge = nlp.load_metric('rouge')\n",
    "nlp_bertscore = nlp.load_metric('bertscore')\n",
    "\n",
    "# takes a list of retrieved documents and a list of possible answers\n",
    "# for a question and returns a measure of the lexical overlap between the\n",
    "# passages and answer\n",
    "def get_aggregate_rouge(res_list, answers):\n",
    "    res = np.zeros((len(res_list), len(answers), 3))\n",
    "    for i, hit in enumerate(res_list):\n",
    "        for j, a in enumerate(answers):\n",
    "            if len(hit.strip()) > 0 and len(a.strip()) > 0:\n",
    "                # get Rouge-1 P/R/F for each passage/answer pair\n",
    "                score = nlp_rouge.compute([hit], [a], rouge_types=['rouge1'])['rouge1'].mid\n",
    "                res[i,j] = np.array([score.precision, score.recall, score.fmeasure])\n",
    "    # average P/R/F rouge scores, then find best passage-answer match\n",
    "    return res.mean(axis=2).max()\n",
    "\n",
    "# Same with BERTscore metri which aligns contextual word embedings\n",
    "def get_aggregate_bertscore(res_list, answers):\n",
    "    res = np.zeros((len(res_list), len(answers), 3))\n",
    "    for i, hit in enumerate(res_list):\n",
    "        for j, a in enumerate(answers):\n",
    "            if len(hit.strip()) > 0 and len(a.strip()) > 0:\n",
    "                # get Rouge-1 P/R/F for each passage/answer pair\n",
    "                score = nlp_bertscore.compute([hit], [a], lang='en')\n",
    "                res[i,j] = np.array([score['precision'].item(), score['recall'].item(), score['f1'].item()])\n",
    "    # average P/R/F rouge scores, then find best passage-answer match\n",
    "    return res.mean(axis=2).max()\n",
    "\n",
    "# Compare which retriever finds passages that have the most\n",
    "# lexical overlap with the ELI5 answers\n",
    "st_time = time()\n",
    "tot_rg_sparse = 0.\n",
    "tot_bs_sparse = 0.\n",
    "tot_rg_dense = 0.\n",
    "tot_bs_dense = 0.\n",
    "# ROUGE and the sparse retriver take time to compute, so we only compare\n",
    "# on a small slice: we'll see that the difference is already apparent\n",
    "valid_slice = eli5['validation_eli5'][:1000]\n",
    "for i, (question, answers) in enumerate(zip(valid_slice['title'], valid_slice['answers'])):\n",
    "    # get documents with sparse retriever\n",
    "    _, sparse_res_list = query_es_index(\n",
    "        question,\n",
    "        es_client, index_name='wiki40b_snippets_100w',\n",
    "        n_results=5\n",
    "    )\n",
    "    sparse_passages = [res['passage_text'] for res in sparse_res_list]\n",
    "    if len(sparse_passages) == 0:\n",
    "        sparse_passages = [question]\n",
    "    tot_rg_sparse += get_aggregate_rouge(sparse_passages, answers['text'])\n",
    "    tot_bs_sparse += get_aggregate_bertscore(sparse_passages, answers['text'])\n",
    "    # get documents with dense retriever\n",
    "    _, dense_res_list = query_qa_dense_index(\n",
    "        question,\n",
    "        qar_model, qar_tokenizer,\n",
    "        wiki40b_snippets, wiki40b_gpu_index,\n",
    "        n_results=5\n",
    "    )\n",
    "    dense_passages = [res['passage_text'] for res in dense_res_list]\n",
    "    tot_rg_dense += get_aggregate_rouge(dense_passages, answers['text'])\n",
    "    tot_bs_dense += get_aggregate_bertscore(dense_passages, answers['text'])\n",
    "    # show average scores side by side\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(\"{:03d} Sparse: RG-{:.4f} BS-{:.4f} | Dense: RG-{:.4f} BS-{:.4f} \\t {:.2f}\".format(\n",
    "            i+1,\n",
    "            tot_rg_sparse / (i+1), tot_bs_sparse / (i+1),\n",
    "            tot_rg_dense / (i+1), tot_bs_dense / (i+1),\n",
    "            time() - st_time\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 Sparse: RG-0.2609 BS-0.8014 | Dense: RG-0.2634 BS-0.8135 \t 1923.04\n"
     ]
    }
   ],
   "source": [
    "print(\"{:03d} Sparse: RG-{:.4f} BS-{:.4f} | Dense: RG-{:.4f} BS-{:.4f} \\t {:.2f}\".format(\n",
    "            i+1,\n",
    "            tot_rg_sparse / (i+1), tot_bs_sparse / (i+1),\n",
    "            tot_rg_dense / (i+1), tot_bs_dense / (i+1),\n",
    "            time() - st_time\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Generation Model\n",
    "<a id='generation'></a>\n",
    "\n",
    "Once we have a question and a document containing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartConfig, BartForConditionalGeneration, BartTokenizer, AdamW\n",
    "from torch import nn\n",
    "from typing import List\n",
    "layers_to_copy = {  # maps # layers in student -> which teacher layers to copy\n",
    "    6: [0, 2, 4, 7, 9, 11],\n",
    "    1: [11],\n",
    "    3: [0, 6, 11],\n",
    "    2: [0, 11],\n",
    "    4: [0, 4, 8, 11],\n",
    "    9: [0, 1, 2, 4, 5, 7, 9, 10, 11],\n",
    "    12: list(range(12)),\n",
    "}\n",
    "def init_student(student, teacher):\n",
    "    \"\"\"Copy everything\"\"\"\n",
    "    teacher_state_dict = teacher.state_dict()\n",
    "    info = student.load_state_dict(teacher_state_dict, strict=False)\n",
    "    assert info.missing_keys == [], info.missing_keys\n",
    "    return student, info\n",
    "  \n",
    "def copy_layers(teacher_layers, student_layers, l2copy: List):\n",
    "    layers_to_copy = nn.ModuleList([l for i, l in enumerate(teacher_layers) if i in l2copy])\n",
    "    assert len(student_layers) == len(l2copy), f\"{len(student_layers)} != {len(l2copy)}\"\n",
    "    student_layers.load_state_dict(layers_to_copy.state_dict())\n",
    "    \n",
    "def make_student(teacher, student_updates):   \n",
    "    d_layers_to_copy = layers_to_copy[student_updates[\"decoder_layers\"]]\n",
    "    e_layers_to_copy = layers_to_copy[student_updates[\"encoder_layers\"]]\n",
    "    kw = teacher.config.to_diff_dict()\n",
    "    kw.update(student_updates)\n",
    "    # Copy weights\n",
    "    student_cfg = BartConfig(**kw)\n",
    "    student = BartForConditionalGeneration(student_cfg)\n",
    "    student, _ = init_student(student, teacher)\n",
    "    copy_layers(teacher.model.encoder.layers, student.model.encoder.layers, e_layers_to_copy)\n",
    "    copy_layers(teacher.model.decoder.layers, student.model.decoder.layers, d_layers_to_copy)\n",
    "    return student\n",
    "teacher = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "student_updates = {\n",
    "    \"decoder_layers\": 6,\n",
    "    \"encoder_layers\": 6,\n",
    "  }\n",
    "student = make_student(teacher, student_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_s2s_tokenizer = BartTokenizer.from_pretrained(\"bart-large\")\n",
    "qa_s2s_model = nn.DataParallel(student.to('cuda:0'))\n",
    "\n",
    "eli5_train_docs = json.load(open('precomputed/eli5_train_precomputed_dense_docs.json'))\n",
    "eli5_valid_docs = json.load(open('precomputed/eli5_valid_precomputed_dense_docs.json'))\n",
    "\n",
    "s2s_train_dset = ELI5DatasetS2S(eli5['train_eli5'], document_cache=dict([(k, d) for k, d, src_ls in eli5_train_docs]))\n",
    "s2s_valid_dset = ELI5DatasetS2S(eli5['validation_eli5'], document_cache=dict([(k, d) for k, d, src_ls in eli5_valid_docs]), training=False)\n",
    "\n",
    "class ArgumentsS2S():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 4\n",
    "        self.backward_freq = 16\n",
    "        self.max_length = 1024\n",
    "        self.print_freq = 100\n",
    "        self.model_save_name = \"seq2seq_models/joint_bart_student\"\n",
    "        self.learning_rate = 2e-4\n",
    "        self.num_epochs = 20\n",
    "\n",
    "s2s_args = ArgumentsS2S()\n",
    "s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n",
    "s2s_scheduler = get_linear_schedule_with_warmup(\n",
    "        s2s_optimizer,\n",
    "        num_warmup_steps=400,\n",
    "        num_training_steps=s2s_args.num_epochs * math.ceil(len(s2s_train_dset) / s2s_args.batch_size)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qa_s2s_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0):\n",
    "    model.train()\n",
    "    # make iterator\n",
    "    train_sampler = RandomSampler(dataset)\n",
    "    model_collate_fn = functools.partial(\n",
    "        make_qa_s2s_batch,\n",
    "        tokenizer=tokenizer, max_len=args.max_length, device='cuda:0'\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=args.batch_size,\n",
    "        sampler=train_sampler, collate_fn=model_collate_fn\n",
    "    )\n",
    "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
    "    # accumulate loss since last print\n",
    "    loc_steps = 0\n",
    "    loc_loss = 0.0\n",
    "    st_time = time()\n",
    "    for step, batch_inputs in enumerate(epoch_iterator):\n",
    "        pre_loss = model(**batch_inputs)[0]\n",
    "        loss = pre_loss.sum() / pre_loss.shape[0]\n",
    "        loss.backward()\n",
    "        # optimizer\n",
    "        if step % args.backward_freq == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "        # some printing within the epoch\n",
    "        loc_loss += loss.item()\n",
    "        loc_steps += 1\n",
    "        if step % args.print_freq == 0:\n",
    "            print(\n",
    "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
    "                    e, step,\n",
    "                    len(dataset) // args.batch_size,\n",
    "                    loc_loss / loc_steps,\n",
    "                    time() - st_time,\n",
    "                )\n",
    "            )\n",
    "            loc_loss = 0\n",
    "            loc_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s_args.batch_size = 4\n",
    "s2s_args.print_freq = 1000\n",
    "\n",
    "for e in range(s2s_args.num_epochs):\n",
    "    train_qa_s2s_epoch(\n",
    "        qa_s2s_model,\n",
    "        s2s_train_dset, qa_s2s_tokenizer,\n",
    "        s2s_optimizer, s2s_scheduler,\n",
    "        s2s_args, e\n",
    "    )\n",
    "    m_save_dict = {\n",
    "        'model': qa_s2s_model.state_dict(),\n",
    "        'optimizer': s2s_optimizer.state_dict(),\n",
    "        'scheduler': s2s_scheduler.state_dict(),\n",
    "    }\n",
    "    print(\"Saving model {}\".format(s2s_args.model_save_name))\n",
    "    torch.save(m_save_dict, '{}_{}.pth'.format(s2s_args.model_save_name, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qa_s2s_epoch(model, dataset, tokenizer, args):\n",
    "    model.train()\n",
    "    # make iterator\n",
    "    train_sampler = SequentialSampler(dataset)\n",
    "    model_collate_fn = functools.partial(\n",
    "        make_qa_s2s_batch,\n",
    "        tokenizer=tokenizer, max_len=args.max_length, device='cuda:0'\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=args.batch_size,\n",
    "        sampler=train_sampler, collate_fn=model_collate_fn\n",
    "    )\n",
    "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
    "    # accumulate loss since last print\n",
    "    loc_steps = 0\n",
    "    loc_loss = 0.0\n",
    "    st_time = time()\n",
    "    with torch.no_grad():\n",
    "        for step, batch_inputs in enumerate(epoch_iterator):\n",
    "            pre_loss = model(**batch_inputs)[0]\n",
    "            loss = pre_loss.sum() / pre_loss.shape[0]\n",
    "            loc_loss += loss.item()\n",
    "            loc_steps += 1\n",
    "            if step % args.print_freq == 0:\n",
    "                print(\n",
    "                    \"{:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
    "                        step,\n",
    "                        len(dataset) // args.batch_size,\n",
    "                        loc_loss / loc_steps,\n",
    "                        time() - st_time,\n",
    "                    )\n",
    "                )\n",
    "    print(\n",
    "        \"Total \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
    "            loc_loss / loc_steps,\n",
    "            time() - st_time,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 of  2453 \t L: 3.521 \t -- 0.315\n",
      " 1000 of  2453 \t L: 3.260 \t -- 319.746\n",
      " 2000 of  2453 \t L: 3.264 \t -- 638.111\n",
      "Total \t L: 3.265 \t -- 782.534\n"
     ]
    }
   ],
   "source": [
    "_ = qa_s2s_model.eval()\n",
    "s2s_args.print_freq = 100\n",
    "eval_qa_s2s_epoch(\n",
    "        qa_s2s_model,\n",
    "        s2s_valid_dset, qa_s2s_tokenizer,\n",
    "        s2s_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('question: why is google fibre taking so long to roll out? context: <p> 2009. in march 2009, arbor worked with 100 isps to create a new network monitoring system, called atlas 2.0. in october 2009, the company estimated that google paid almost nothing for youtube\\'s bandwidth, noting that google probably used dark fibre instead to run the website.\\n on august 31, 2010, tektronix communications announced that it has completed its acquisition of arbor networks. upon completion of the acquisition, arbor networks joins danaher corporation\\'s portfolio of communications and enterprise companies, which includes tektronix communications.\\n on september 3, 2013, arbor networks announced that it had acquired privately held packetloop, a leader in security analytics <p> in january 2017, construction was halted pending concerns about the placement of google fiber huts in city parks. mayor ivy taylor expressed commitment to working with google to address community concerns and allow the project to continue.\\n as of may 9, 2019 google fiber micro-trenched 600 miles of fiber in san antonio neighborhoods. city staff say the majority is on the far northwest and northeast sides, including the pilot area in the westover hills neighborhood. after closing service in louisville, ky the company said it learned from its challenges and refined its micro-trenching program to go deeper. according to the <p> the west side depicted gorai and mandapeshwar areas.\\n platforms. new services at borivali station.\\n google is in continuation with its project to provide high-speed public wifi service at 100 railway stations across india by end of the year.\\n google has been deploying the service over the extensive fibre network of indian railway\\'s arm railtel which provides internet services as railwire.\\n with these launches, the service is now live and available to users at 15 stations across the country.\\n backed by the “phenomenal” response to the service in mumbai central, google will also extend the project to cover key suburban mumbai stations <p> the benefit of a fully managed network backbone with minimal downtime, when a fault is detected the location is known to within a few metres, as well as showing the health of the cables and its segments, allowing for preemptive maintenance where cable is showing signs of distress.\\n the building of the network.\\n visionstream commenced building of the network on 15 june 2001 with the first soil being turned and commencement of laying the fibre 40\\xa0km west of young, new south wales at milvale, after many months of project management planning which included sourcing of materials, negotiating access along the <p> olathe, kansas – on march 19, 2013, google announced that the project would be expanded to olathe.\\n - north kansas city, missouri – on april 19, 2013, google announced that they were to begin a 20-year lease on dark fiber in the existing \"linkcity\" fiber network in north kansas city. the original news article was incomplete and later articles clarified the lease. independent of google\\'s network the system in north kansas city will also be upgraded to gigabit capacity and managed by a local company based out of north kansas city.\\n - shawnee, kansas – may 2, 2013\\n - raytown, <p> to be awarded the contract.\\n there were two primary components to the earthlink/google proposal: earthlink would install and maintain the network and provide a higher speed (1 mbit/s), paid connection. google would be an anchor tenant on earthlink\\'s network, offering a free basic service (300 kbit/s). earthlink\\'s plan would have offered a two-tiered pricing scheme, with a discount for qualifying low-income residents.\\n proposal. criticisms of proposal.\\n the san francisco budget analyst\\'s office has criticized the proposal, claiming that the department of technology and information services (dtis) engaged in a biased and secretive process while crafting the proposal. dtis\\'s critics state that <p> new development areas being a high priority.\\n initially, the plan was to have fibre within reach of 75% of the population by 2019 with an investment of $1.35 billion. in august 2017, the government announced the target was to be expanded to 87% of the population and to be completed by 2022.\\n partner companies.\\n crown fibre holdings limited (cfh) is a crown-owned company set up to manage the project. cfh has contracted four companies to deploy fibre network.\\n the main partner is chorus, which won 69% of the roll out area. chorus was part of incumbent telco spark but was <p> services in indian cities of delhi, mumbai, indore, bhopal, lucknow and kolkata. as of 2013, the maximum speed provided is up to 50\\xa0mbit/s. it uses cisco systems\\'s docsis 3 technology in three cities, where the speeds are 50 mbit/s.\\n it has around 11 million subscribers, out of which around 1.77 million have the availability of broadband. out of this, about 430,000 subscribers are using hathway\\'s broadband services.\\n hathway is set to release its digital video recorder (dvr) service soon, powered by nds xtv.\\n acquisition by reliance industries.\\n on 17 october 2018, reliance industries announced that it had acquired a 51.34% <p> end of may 2010, however in mid-june they acknowledged that a number of customers were still without access, and it would be up to six more weeks before they would be connected. eastlink declined to provide how many customers were still without access. seaside communications had stated its work would be completed by the end of summer (september) 2010; as of december 31, 2010 this pledge has still not been fulfilled.\\n initial case studies.\\n in its early phases the program was described as a success by motorola itself and other sources. seaside and omniglobe were generally successful at their objectives <p> broadband infrastructure are high enough to deter even the larger providers. sprint claims it spent \"tens of millions of dollars\" in their checking for compliance with nepa, a set of environmental impact regulations, that found \"no significant impact\" by the conclusion and ultimately delayed their entrance in that particular geography. \\n to remedy this anti-competitive climate, governments have worked to minimize costs entrants may incur. the telecommunicatons act of 1996 expanded access rights to pole attachments for isps with federal subsidies in an aim to encourage provider participation. \\n in 2015, the federal communications commission granted a preemption petition requested',\n",
       " \"One does not simply lay down a large fiber network. First, you have to have the money. That's not really an issue for Google. Then, you have to convince municipal governments to let you build a network, and you have to get past the incumbent ISP, who wants to keep their monopoly intact. You have to find enough subscribers, you have to find people to build the network, you have to do customer service and installation, and you have to not be hated by the public. Throwing money at those problems is ineffective.\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2s_valid_dset[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeast, yeast, and water are all part of the process of fermentation. Yeast is one of the few organisms that can make bread, beer, and wine.\n",
      "\n",
      "When you mix yeast and water, the yeast will ferment and turn into alcohol. This is why beer is fermented and wine is fermented.\n"
     ]
    }
   ],
   "source": [
    "print(qa_s2s_generate(\n",
    "        s2s_valid_dset[124][0], qa_s2s_model.module, qa_s2s_tokenizer,\n",
    "        num_answers=1,\n",
    "        num_beams=8,\n",
    "        min_len=64,\n",
    "        max_len=256,\n",
    "        max_input_length=1024,\n",
    "        device=\"cuda:0\"\n",
    "    )[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: how do apps like soundhound and shazam know what song is playing? context: <p> kind of applications is mainly used for finding a song that the user does not already know. searching by sound is not limited to just identifying songs, but also for identifying melodies, tunes or advertisements, sound library management and video files.\\n acoustic fingerprinting.\\n the way these apps search by sound is through generating an acoustic fingerprint; a digital summary of the sound. a microphone is used to pick up an audio sample, which is then broken down into a simple numeric signature, a code unique to each track. using the same method of fingerprinting sounds, when shazam picks up a <p> such as midomi and soundhound allow users to add to that library of music in order to expand the chances to match a sound sample with its corresponding sound.\\n query by humming.\\n midomi and soundhound both utilize query by humming, or qbh. this is a branch off of acoustic fingerprints, but is still a musical retrieval system. after receiving a user generated hummed melody, which is the input query, and returns a ranked list of songs that is closest to the user query.\\n see also.\\n - amplifind\\n - automatic content recognition\\n - list of online music databases\\n - music information <p> and by direct input of the name of a song or singer. users can share their search results on facebook, twitter or other sns website and get similar music recommendations. the app also gives information about the song title, its singer and allows you to purchase the song.\\n history.\\n the music radar team got 1st place in the query by singing/humming (qbsh) task at the music information retrieval evaluation exchange (mirex) in 2012 and 2013. the app was launched at the end of january 2013, supporting query by singing/humming and audio fingerprinting. the app reached its first one million user <p> audio continuously listens and analyzes music while most other services can only listen for a fixed amount of time. bing research developed a \"fingerprinting\" algorithm to identify songs.\\n on march 30, 2016 microsoft announced they will create bots based on bing features in skype, which bing music was one such feature.\\n availability.\\n availability. as part of cortana (where supported).\\n - australia\\n - canada\\n - france\\n - germany\\n - india\\n - italy\\n - spain\\n - united kingdom\\n - united states\\n availability. as part of bing mobile.\\n - argentina\\n - austria\\n - belgium\\n - brazil\\n - denmark\\n - finland\\n - ireland\\n - mexico\\n - <p> at launch, being expanded to the android app in april 2017.\\n in may 2017, spotify introduced spotify codes for its mobile apps, a way for users to share specific artists, tracks, playlists or albums with other people. users find the relevant content to share and press a \"soundwave-like barcode\" on the display. a camera icon in the apps\\' search fields lets other users point their device\\'s camera at the code, which takes them to the same content.\\n in january 2019, spotify introduced car view for android, allowing devices running android to have a compact now playing screen when the device <p> can identify music being played from any source, provided that the background noise level is not high enough to prevent an acoustic fingerprint being taken, and that the song is present in the software\\'s database.\\n as well as the free app, the company has released a paid app called shazam encore. in september 2012, the service was expanded to enable tv users in the us to identify featured music, access cast information, and get links to show information online, as well as added social networking capabilities.\\n shazam redesigned their app in 2014, and added additional features.\\n compatible devices.\\n shazam runs <p> smartphone or computer\\'s built-in microphone to gather a brief sample of audio being played. shazam stores a catalogue of audio fingerprints in a database. the user tags a song for 10 seconds and the application creates an audio fingerprint. shazam works by analyzing the captured sound and seeking a match based on an acoustic fingerprint in a database of millions of songs. if it finds a match, it sends information such as the artist, song title, and album back to the user. some implementations of shazam incorporate relevant links to services such as itunes, spotify, youtube, or groove music.\\n shazam <p> shazam (application)\\n shazam is an application owned and developed by apple inc. the application can identify music, movies, advertising, and television shows, based on a short sample played and using the microphone on the device. the software is available for android, macos, ios, watchos, tvos, and windows. \\n shazam entertainment limited was founded in 1999 by chris barton, philip inghelbrecht, avery wang, and dhiraj mukherjee. on 24 september 2018, the company was acquired by apple for a reported $400 million.\\n overview.\\n shazam identifies songs based on an audio fingerprint based on a time-frequency graph called a spectrogram. it uses a <p> features.\\n tunewiki provides real-time scrolling lyrics that display as a song plays, for songs saved on the phone as well as for songs played through streaming radio, provided that the user first synchronizes the initial line of textual lyrics with the correct verse of the playing song.\\n a geolocation feature locates songs playing in the area of the user, and includes social media functionality for posting to popular social media websites.\\n supported devices.\\n tunewiki had applications released for iphone and ipod touch, android, windows phone 7, blackberry, symbian s60v5, nokia series 40 (java based) and maemo, with a plugin for <p> on android, ios (including apple watch), blackberry os, and windows phone systems. shazam is also available for mac, as a desktop application that when enabled, runs in the background and automatically recognises any song played on or near the computer. apple\\'s launch of ios 8 in september 2014 came with the integration of shazam into apple\\'s siri function.\\n history.\\n the company was founded in 1999 by barton and inghelbrecht, who were students at university of california, berkeley and mukherjee, who worked at a london-based internet consulting firm called viant. in need of a digital signal processing specialist, the founding team'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2s_valid_dset[11][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
