2020-04-07 14:55:06,898 INFO     Running with arguments: Namespace(amp=False, average_over=30, batch_sizes=[1], csv_memory_filename='memory_1586264107.csv', csv_time_filename='time_1586264107.csv', fp16=False, keras_predict=False, log_filename='log_1586264107.txt', log_print=True, models=['distilgpt2'], no_memory=False, no_speed=False, save_to_csv=True, slice_sizes=[16], tensorflow=False, torch=True, torch_cuda=False, torchscript=False, verbose=True, xla=False)
2020-04-07 14:55:06,898 INFO     1 / 1
2020-04-07 14:55:06,900 DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443
2020-04-07 14:55:07,804 DEBUG    https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/distilgpt2-config.json HTTP/1.1" 200 0
2020-04-07 14:55:07,809 INFO     loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-config.json from cache at /home/patrick/.cache/torch/transformers/eb0f77b3f095880586731f57e2fe19060d71d1036ef8daf727bd97a17fb66a43.a41f80bd12c111d611dcd5546611b7e47c16a0a995f83df2f7b437a20b6849b5
2020-04-07 14:55:07,810 INFO     Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bad_words_ids": null,
  "bos_token_id": 50256,
  "decoder_start_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "finetuning_task": null,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "min_length": 0,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "no_repeat_ngram_size": 0,
  "num_beams": 1,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "prefix": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": null,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 50257
}

2020-04-07 14:55:07,811 DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443
2020-04-07 14:55:08,623 DEBUG    https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/distilgpt2-pytorch_model.bin HTTP/1.1" 200 0
2020-04-07 14:55:08,627 INFO     loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-pytorch_model.bin from cache at /home/patrick/.cache/torch/transformers/a2212aabe89bddaac8786d5937151284ae0a36a1cf96d2c0b9eb8c3dfa94fff5.ffe4c53a2a410b15148cf4170cc408d2d2f98adeecdde146ef8e71843039ff3c
2020-04-07 14:55:09,960 DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443
2020-04-07 14:55:10,618 DEBUG    https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/distilgpt2-config.json HTTP/1.1" 200 0
2020-04-07 14:55:10,622 INFO     loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-config.json from cache at /home/patrick/.cache/torch/transformers/eb0f77b3f095880586731f57e2fe19060d71d1036ef8daf727bd97a17fb66a43.a41f80bd12c111d611dcd5546611b7e47c16a0a995f83df2f7b437a20b6849b5
2020-04-07 14:55:10,623 INFO     Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bad_words_ids": null,
  "bos_token_id": 50256,
  "decoder_start_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "finetuning_task": null,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "min_length": 0,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "no_repeat_ngram_size": 0,
  "num_beams": 1,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "prefix": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": null,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 50257
}

2020-04-07 14:55:10,627 DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443
2020-04-07 14:55:11,184 DEBUG    https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/distilgpt2-vocab.json HTTP/1.1" 200 0
2020-04-07 14:55:11,189 DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443
2020-04-07 14:55:11,618 DEBUG    https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/distilgpt2-merges.txt HTTP/1.1" 200 0
2020-04-07 14:55:11,622 INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-vocab.json from cache at /home/patrick/.cache/torch/transformers/71cc2431cf0b5bbe7a23601a808ed322c90251c8261b46f04970140a3c2c1cb4.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
2020-04-07 14:55:11,623 INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-merges.txt from cache at /home/patrick/.cache/torch/transformers/4faf7afb02a1ea7d2944e9ba7a175c7b8de4957cdbae75cd5ddffc7c7643ebbc.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
2020-04-07 14:55:11,761 WARNING  Token indices sequence length is longer than the specified maximum sequence length for this model (2707 > 1024). Running this sequence through the model will result in indexing errors
2020-04-07 14:55:11,761 INFO     Using model GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
2020-04-07 14:55:11,762 INFO     Number of all parameters 81912576
2020-04-07 14:55:11,852 WARNING  Error while initializing comunication with GPU. We won't perform GPU memory tracing.
2020-04-07 14:55:11,917 INFO     
Lines by line memory consumption:
/home/patrick/python_bin/transformers/modeling_gpt2.py:400: mem 0.000B:         if input_ids is not None and inputs_embeds is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:402: mem 0.000B:         elif input_ids is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:403: mem 0.000B:             input_shape = input_ids.size()
/home/patrick/python_bin/transformers/modeling_gpt2.py:404: mem 0.000B:             input_ids = input_ids.view(-1, input_shape[-1])
/home/patrick/python_bin/transformers/modeling_gpt2.py:405: mem 0.000B:             batch_size = input_ids.shape[0]
/home/patrick/python_bin/transformers/modeling_gpt2.py:412: mem 0.000B:         if token_type_ids is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:414: mem 0.000B:         if position_ids is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:417: mem 0.000B:         if past is None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:418: mem 0.000B:             past_length = 0
/home/patrick/python_bin/transformers/modeling_gpt2.py:419: mem 0.000B:             past = [None] * len(self.h)
/home/patrick/python_bin/transformers/modeling_gpt2.py:422: mem 0.000B:         if position_ids is None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:423: mem 0.000B:             device = input_ids.device if input_ids is not None else inputs_embeds.device
/home/patrick/python_bin/transformers/modeling_gpt2.py:424: mem 0.000B:             position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
/home/patrick/python_bin/transformers/modeling_gpt2.py:425: mem 0.000B:             position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
/home/patrick/python_bin/transformers/modeling_gpt2.py:428: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:450: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:462: mem 0.000B:             head_mask = [None] * self.config.n_layer
/home/patrick/python_bin/transformers/modeling_gpt2.py:464: mem 0.000B:         if inputs_embeds is None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:465: mem 0.000B:             inputs_embeds = self.wte(input_ids)
/home/patrick/python_bin/transformers/modeling_gpt2.py:466: mem 0.000B:         position_embeds = self.wpe(position_ids)
/home/patrick/python_bin/transformers/modeling_gpt2.py:467: mem 0.000B:         if token_type_ids is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:470: mem 0.000B:             token_type_embeds = 0
/home/patrick/python_bin/transformers/modeling_gpt2.py:471: mem 0.000B:         hidden_states = inputs_embeds + position_embeds + token_type_embeds
/home/patrick/python_bin/transformers/modeling_gpt2.py:472: mem 0.000B:         hidden_states = self.drop(hidden_states)
/home/patrick/python_bin/transformers/modeling_gpt2.py:474: mem 0.000B:         output_shape = input_shape + (hidden_states.size(-1),)
/home/patrick/python_bin/transformers/modeling_gpt2.py:476: mem 0.000B:         presents = ()
/home/patrick/python_bin/transformers/modeling_gpt2.py:477: mem 0.000B:         all_attentions = []
/home/patrick/python_bin/transformers/modeling_gpt2.py:478: mem 0.000B:         all_hidden_states = ()
/home/patrick/python_bin/transformers/modeling_gpt2.py:479: mem 0.000B:         for i, (block, layer_past) in enumerate(zip(self.h, past)):
/home/patrick/python_bin/transformers/modeling_gpt2.py:480: mem 0.000B:             if self.output_hidden_states:
/home/patrick/python_bin/transformers/modeling_gpt2.py:483: mem 0.000B:             outputs = block(
/home/patrick/python_bin/transformers/modeling_gpt2.py:484: mem 0.000B:                 hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]
/home/patrick/python_bin/transformers/modeling_gpt2.py:225: mem 0.000B:         output_attn = self.attn(
/home/patrick/python_bin/transformers/modeling_gpt2.py:226: mem 0.000B:             self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask
/home/patrick/python_bin/transformers/modeling_gpt2.py:178: mem 256.000KB:         x = self.c_attn(x)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:179: mem 0.000B:         query, key, value = x.split(self.split_size, dim=2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:180: mem 0.000B:         query = self.split_heads(query)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:181: mem 0.000B:         key = self.split_heads(key, k=True)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:173: mem 0.000B:             return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)
/home/patrick/python_bin/transformers/modeling_gpt2.py:182: mem 0.000B:         value = self.split_heads(value)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:183: mem 0.000B:         if layer_past is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:187: mem 0.000B:         present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking
/home/patrick/python_bin/transformers/modeling_gpt2.py:189: mem 0.000B:         attn_outputs = self._attn(query, key, value, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_gpt2.py:141: mem 0.000B:         w = torch.matmul(q, k)
/home/patrick/python_bin/transformers/modeling_gpt2.py:142: mem 0.000B:         if self.scale:
/home/patrick/python_bin/transformers/modeling_gpt2.py:143: mem 0.000B:             w = w / math.sqrt(v.size(-1))
/home/patrick/python_bin/transformers/modeling_gpt2.py:144: mem 0.000B:         nd, ns = w.size(-2), w.size(-1)
/home/patrick/python_bin/transformers/modeling_gpt2.py:145: mem 0.000B:         b = self.bias[:, :, ns - nd : ns, :ns]
/home/patrick/python_bin/transformers/modeling_gpt2.py:146: mem 0.000B:         w = w * b - 1e4 * (1 - b)
/home/patrick/python_bin/transformers/modeling_gpt2.py:148: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:152: mem 0.000B:         w = nn.Softmax(dim=-1)(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:153: mem 0.000B:         w = self.attn_dropout(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:156: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:159: mem 0.000B:         outputs = [torch.matmul(w, v)]
/home/patrick/python_bin/transformers/modeling_gpt2.py:160: mem 0.000B:         if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:162: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_gpt2.py:190: mem 0.000B:         a = attn_outputs[0]
/home/patrick/python_bin/transformers/modeling_gpt2.py:192: mem 0.000B:         a = self.merge_heads(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:165: mem 0.000B:         x = x.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_gpt2.py:166: mem 0.000B:         new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)
/home/patrick/python_bin/transformers/modeling_gpt2.py:167: mem 0.000B:         return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:193: mem 0.000B:         a = self.c_proj(a)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:194: mem 0.000B:         a = self.resid_dropout(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:196: mem 0.000B:         outputs = [a, present] + attn_outputs[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:197: mem 0.000B:         return outputs  # a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:228: mem 0.000B:         a = output_attn[0]  # output_attn: a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:230: mem 0.000B:         x = x + a
/home/patrick/python_bin/transformers/modeling_gpt2.py:231: mem 0.000B:         m = self.mlp(self.ln_2(x))
/home/patrick/python_bin/transformers/modeling_gpt2.py:210: mem 0.000B:         h = self.act(self.c_fc(x))
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:211: mem 0.000B:         h2 = self.c_proj(h)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:212: mem 0.000B:         return self.dropout(h2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:232: mem 0.000B:         x = x + m
/home/patrick/python_bin/transformers/modeling_gpt2.py:234: mem 0.000B:         outputs = [x] + output_attn[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:235: mem 0.000B:         return outputs  # x, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:487: mem 0.000B:             hidden_states, present = outputs[:2]
/home/patrick/python_bin/transformers/modeling_gpt2.py:488: mem 0.000B:             if self.output_past:
/home/patrick/python_bin/transformers/modeling_gpt2.py:489: mem 0.000B:                 presents = presents + (present,)
/home/patrick/python_bin/transformers/modeling_gpt2.py:491: mem 0.000B:             if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:479: mem 0.000B:         for i, (block, layer_past) in enumerate(zip(self.h, past)):
/home/patrick/python_bin/transformers/modeling_gpt2.py:480: mem 0.000B:             if self.output_hidden_states:
/home/patrick/python_bin/transformers/modeling_gpt2.py:483: mem 0.000B:             outputs = block(
/home/patrick/python_bin/transformers/modeling_gpt2.py:484: mem 0.000B:                 hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]
/home/patrick/python_bin/transformers/modeling_gpt2.py:225: mem 0.000B:         output_attn = self.attn(
/home/patrick/python_bin/transformers/modeling_gpt2.py:226: mem 0.000B:             self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask
/home/patrick/python_bin/transformers/modeling_gpt2.py:178: mem 0.000B:         x = self.c_attn(x)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:179: mem 0.000B:         query, key, value = x.split(self.split_size, dim=2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:180: mem 0.000B:         query = self.split_heads(query)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:181: mem 0.000B:         key = self.split_heads(key, k=True)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:173: mem 0.000B:             return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)
/home/patrick/python_bin/transformers/modeling_gpt2.py:182: mem 0.000B:         value = self.split_heads(value)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:183: mem 0.000B:         if layer_past is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:187: mem 0.000B:         present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking
/home/patrick/python_bin/transformers/modeling_gpt2.py:189: mem 0.000B:         attn_outputs = self._attn(query, key, value, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_gpt2.py:141: mem 0.000B:         w = torch.matmul(q, k)
/home/patrick/python_bin/transformers/modeling_gpt2.py:142: mem 0.000B:         if self.scale:
/home/patrick/python_bin/transformers/modeling_gpt2.py:143: mem 0.000B:             w = w / math.sqrt(v.size(-1))
/home/patrick/python_bin/transformers/modeling_gpt2.py:144: mem 0.000B:         nd, ns = w.size(-2), w.size(-1)
/home/patrick/python_bin/transformers/modeling_gpt2.py:145: mem 0.000B:         b = self.bias[:, :, ns - nd : ns, :ns]
/home/patrick/python_bin/transformers/modeling_gpt2.py:146: mem 0.000B:         w = w * b - 1e4 * (1 - b)
/home/patrick/python_bin/transformers/modeling_gpt2.py:148: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:152: mem 0.000B:         w = nn.Softmax(dim=-1)(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:153: mem 0.000B:         w = self.attn_dropout(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:156: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:159: mem 0.000B:         outputs = [torch.matmul(w, v)]
/home/patrick/python_bin/transformers/modeling_gpt2.py:160: mem 0.000B:         if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:162: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_gpt2.py:190: mem 0.000B:         a = attn_outputs[0]
/home/patrick/python_bin/transformers/modeling_gpt2.py:192: mem 0.000B:         a = self.merge_heads(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:165: mem 0.000B:         x = x.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_gpt2.py:166: mem 0.000B:         new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)
/home/patrick/python_bin/transformers/modeling_gpt2.py:167: mem 0.000B:         return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:193: mem 0.000B:         a = self.c_proj(a)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:194: mem 0.000B:         a = self.resid_dropout(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:196: mem 0.000B:         outputs = [a, present] + attn_outputs[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:197: mem 0.000B:         return outputs  # a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:228: mem 0.000B:         a = output_attn[0]  # output_attn: a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:230: mem 0.000B:         x = x + a
/home/patrick/python_bin/transformers/modeling_gpt2.py:231: mem 0.000B:         m = self.mlp(self.ln_2(x))
/home/patrick/python_bin/transformers/modeling_gpt2.py:210: mem 0.000B:         h = self.act(self.c_fc(x))
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:211: mem 0.000B:         h2 = self.c_proj(h)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:212: mem 0.000B:         return self.dropout(h2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:232: mem 0.000B:         x = x + m
/home/patrick/python_bin/transformers/modeling_gpt2.py:234: mem 0.000B:         outputs = [x] + output_attn[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:235: mem 0.000B:         return outputs  # x, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:487: mem 0.000B:             hidden_states, present = outputs[:2]
/home/patrick/python_bin/transformers/modeling_gpt2.py:488: mem 0.000B:             if self.output_past:
/home/patrick/python_bin/transformers/modeling_gpt2.py:489: mem 0.000B:                 presents = presents + (present,)
/home/patrick/python_bin/transformers/modeling_gpt2.py:491: mem 0.000B:             if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:479: mem 0.000B:         for i, (block, layer_past) in enumerate(zip(self.h, past)):
/home/patrick/python_bin/transformers/modeling_gpt2.py:480: mem 0.000B:             if self.output_hidden_states:
/home/patrick/python_bin/transformers/modeling_gpt2.py:483: mem 0.000B:             outputs = block(
/home/patrick/python_bin/transformers/modeling_gpt2.py:484: mem 0.000B:                 hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]
/home/patrick/python_bin/transformers/modeling_gpt2.py:225: mem 0.000B:         output_attn = self.attn(
/home/patrick/python_bin/transformers/modeling_gpt2.py:226: mem 0.000B:             self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask
/home/patrick/python_bin/transformers/modeling_gpt2.py:178: mem 0.000B:         x = self.c_attn(x)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:179: mem 0.000B:         query, key, value = x.split(self.split_size, dim=2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:180: mem 0.000B:         query = self.split_heads(query)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:181: mem 0.000B:         key = self.split_heads(key, k=True)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:173: mem 0.000B:             return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)
/home/patrick/python_bin/transformers/modeling_gpt2.py:182: mem 0.000B:         value = self.split_heads(value)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:183: mem 0.000B:         if layer_past is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:187: mem 0.000B:         present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking
/home/patrick/python_bin/transformers/modeling_gpt2.py:189: mem 0.000B:         attn_outputs = self._attn(query, key, value, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_gpt2.py:141: mem 0.000B:         w = torch.matmul(q, k)
/home/patrick/python_bin/transformers/modeling_gpt2.py:142: mem 0.000B:         if self.scale:
/home/patrick/python_bin/transformers/modeling_gpt2.py:143: mem 0.000B:             w = w / math.sqrt(v.size(-1))
/home/patrick/python_bin/transformers/modeling_gpt2.py:144: mem 0.000B:         nd, ns = w.size(-2), w.size(-1)
/home/patrick/python_bin/transformers/modeling_gpt2.py:145: mem 0.000B:         b = self.bias[:, :, ns - nd : ns, :ns]
/home/patrick/python_bin/transformers/modeling_gpt2.py:146: mem 0.000B:         w = w * b - 1e4 * (1 - b)
/home/patrick/python_bin/transformers/modeling_gpt2.py:148: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:152: mem 0.000B:         w = nn.Softmax(dim=-1)(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:153: mem 0.000B:         w = self.attn_dropout(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:156: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:159: mem 0.000B:         outputs = [torch.matmul(w, v)]
/home/patrick/python_bin/transformers/modeling_gpt2.py:160: mem 0.000B:         if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:162: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_gpt2.py:190: mem 0.000B:         a = attn_outputs[0]
/home/patrick/python_bin/transformers/modeling_gpt2.py:192: mem 0.000B:         a = self.merge_heads(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:165: mem 0.000B:         x = x.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_gpt2.py:166: mem 0.000B:         new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)
/home/patrick/python_bin/transformers/modeling_gpt2.py:167: mem 0.000B:         return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:193: mem 0.000B:         a = self.c_proj(a)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:194: mem 0.000B:         a = self.resid_dropout(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:196: mem 0.000B:         outputs = [a, present] + attn_outputs[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:197: mem 0.000B:         return outputs  # a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:228: mem 0.000B:         a = output_attn[0]  # output_attn: a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:230: mem 0.000B:         x = x + a
/home/patrick/python_bin/transformers/modeling_gpt2.py:231: mem 0.000B:         m = self.mlp(self.ln_2(x))
/home/patrick/python_bin/transformers/modeling_gpt2.py:210: mem 0.000B:         h = self.act(self.c_fc(x))
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:211: mem 0.000B:         h2 = self.c_proj(h)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:212: mem 0.000B:         return self.dropout(h2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:232: mem 0.000B:         x = x + m
/home/patrick/python_bin/transformers/modeling_gpt2.py:234: mem 0.000B:         outputs = [x] + output_attn[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:235: mem 0.000B:         return outputs  # x, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:487: mem 0.000B:             hidden_states, present = outputs[:2]
/home/patrick/python_bin/transformers/modeling_gpt2.py:488: mem 0.000B:             if self.output_past:
/home/patrick/python_bin/transformers/modeling_gpt2.py:489: mem 0.000B:                 presents = presents + (present,)
/home/patrick/python_bin/transformers/modeling_gpt2.py:491: mem 0.000B:             if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:479: mem 0.000B:         for i, (block, layer_past) in enumerate(zip(self.h, past)):
/home/patrick/python_bin/transformers/modeling_gpt2.py:480: mem 0.000B:             if self.output_hidden_states:
/home/patrick/python_bin/transformers/modeling_gpt2.py:483: mem 0.000B:             outputs = block(
/home/patrick/python_bin/transformers/modeling_gpt2.py:484: mem 0.000B:                 hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]
/home/patrick/python_bin/transformers/modeling_gpt2.py:225: mem 0.000B:         output_attn = self.attn(
/home/patrick/python_bin/transformers/modeling_gpt2.py:226: mem 0.000B:             self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask
/home/patrick/python_bin/transformers/modeling_gpt2.py:178: mem 0.000B:         x = self.c_attn(x)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:179: mem 0.000B:         query, key, value = x.split(self.split_size, dim=2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:180: mem 0.000B:         query = self.split_heads(query)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:181: mem 0.000B:         key = self.split_heads(key, k=True)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:173: mem 0.000B:             return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)
/home/patrick/python_bin/transformers/modeling_gpt2.py:182: mem 0.000B:         value = self.split_heads(value)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:183: mem 0.000B:         if layer_past is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:187: mem 0.000B:         present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking
/home/patrick/python_bin/transformers/modeling_gpt2.py:189: mem 0.000B:         attn_outputs = self._attn(query, key, value, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_gpt2.py:141: mem 0.000B:         w = torch.matmul(q, k)
/home/patrick/python_bin/transformers/modeling_gpt2.py:142: mem 0.000B:         if self.scale:
/home/patrick/python_bin/transformers/modeling_gpt2.py:143: mem 0.000B:             w = w / math.sqrt(v.size(-1))
/home/patrick/python_bin/transformers/modeling_gpt2.py:144: mem 0.000B:         nd, ns = w.size(-2), w.size(-1)
/home/patrick/python_bin/transformers/modeling_gpt2.py:145: mem 0.000B:         b = self.bias[:, :, ns - nd : ns, :ns]
/home/patrick/python_bin/transformers/modeling_gpt2.py:146: mem 0.000B:         w = w * b - 1e4 * (1 - b)
/home/patrick/python_bin/transformers/modeling_gpt2.py:148: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:152: mem 0.000B:         w = nn.Softmax(dim=-1)(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:153: mem 0.000B:         w = self.attn_dropout(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:156: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:159: mem 0.000B:         outputs = [torch.matmul(w, v)]
/home/patrick/python_bin/transformers/modeling_gpt2.py:160: mem 0.000B:         if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:162: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_gpt2.py:190: mem 0.000B:         a = attn_outputs[0]
/home/patrick/python_bin/transformers/modeling_gpt2.py:192: mem 0.000B:         a = self.merge_heads(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:165: mem 0.000B:         x = x.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_gpt2.py:166: mem 0.000B:         new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)
/home/patrick/python_bin/transformers/modeling_gpt2.py:167: mem 0.000B:         return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:193: mem 0.000B:         a = self.c_proj(a)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:194: mem 0.000B:         a = self.resid_dropout(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:196: mem 0.000B:         outputs = [a, present] + attn_outputs[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:197: mem 0.000B:         return outputs  # a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:228: mem 0.000B:         a = output_attn[0]  # output_attn: a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:230: mem 0.000B:         x = x + a
/home/patrick/python_bin/transformers/modeling_gpt2.py:231: mem 0.000B:         m = self.mlp(self.ln_2(x))
/home/patrick/python_bin/transformers/modeling_gpt2.py:210: mem 0.000B:         h = self.act(self.c_fc(x))
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:211: mem 0.000B:         h2 = self.c_proj(h)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:212: mem 0.000B:         return self.dropout(h2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:232: mem 0.000B:         x = x + m
/home/patrick/python_bin/transformers/modeling_gpt2.py:234: mem 0.000B:         outputs = [x] + output_attn[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:235: mem 0.000B:         return outputs  # x, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:487: mem 0.000B:             hidden_states, present = outputs[:2]
/home/patrick/python_bin/transformers/modeling_gpt2.py:488: mem 0.000B:             if self.output_past:
/home/patrick/python_bin/transformers/modeling_gpt2.py:489: mem 0.000B:                 presents = presents + (present,)
/home/patrick/python_bin/transformers/modeling_gpt2.py:491: mem 0.000B:             if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:479: mem 0.000B:         for i, (block, layer_past) in enumerate(zip(self.h, past)):
/home/patrick/python_bin/transformers/modeling_gpt2.py:480: mem 0.000B:             if self.output_hidden_states:
/home/patrick/python_bin/transformers/modeling_gpt2.py:483: mem 0.000B:             outputs = block(
/home/patrick/python_bin/transformers/modeling_gpt2.py:484: mem 0.000B:                 hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]
/home/patrick/python_bin/transformers/modeling_gpt2.py:225: mem 0.000B:         output_attn = self.attn(
/home/patrick/python_bin/transformers/modeling_gpt2.py:226: mem 0.000B:             self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask
/home/patrick/python_bin/transformers/modeling_gpt2.py:178: mem 0.000B:         x = self.c_attn(x)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:179: mem 0.000B:         query, key, value = x.split(self.split_size, dim=2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:180: mem 0.000B:         query = self.split_heads(query)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:181: mem 0.000B:         key = self.split_heads(key, k=True)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:173: mem 0.000B:             return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)
/home/patrick/python_bin/transformers/modeling_gpt2.py:182: mem 0.000B:         value = self.split_heads(value)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:183: mem 0.000B:         if layer_past is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:187: mem 0.000B:         present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking
/home/patrick/python_bin/transformers/modeling_gpt2.py:189: mem 0.000B:         attn_outputs = self._attn(query, key, value, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_gpt2.py:141: mem 0.000B:         w = torch.matmul(q, k)
/home/patrick/python_bin/transformers/modeling_gpt2.py:142: mem 0.000B:         if self.scale:
/home/patrick/python_bin/transformers/modeling_gpt2.py:143: mem 0.000B:             w = w / math.sqrt(v.size(-1))
/home/patrick/python_bin/transformers/modeling_gpt2.py:144: mem 0.000B:         nd, ns = w.size(-2), w.size(-1)
/home/patrick/python_bin/transformers/modeling_gpt2.py:145: mem 0.000B:         b = self.bias[:, :, ns - nd : ns, :ns]
/home/patrick/python_bin/transformers/modeling_gpt2.py:146: mem 0.000B:         w = w * b - 1e4 * (1 - b)
/home/patrick/python_bin/transformers/modeling_gpt2.py:148: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:152: mem 0.000B:         w = nn.Softmax(dim=-1)(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:153: mem 0.000B:         w = self.attn_dropout(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:156: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:159: mem 0.000B:         outputs = [torch.matmul(w, v)]
/home/patrick/python_bin/transformers/modeling_gpt2.py:160: mem 0.000B:         if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:162: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_gpt2.py:190: mem 0.000B:         a = attn_outputs[0]
/home/patrick/python_bin/transformers/modeling_gpt2.py:192: mem 0.000B:         a = self.merge_heads(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:165: mem 0.000B:         x = x.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_gpt2.py:166: mem 0.000B:         new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)
/home/patrick/python_bin/transformers/modeling_gpt2.py:167: mem 0.000B:         return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:193: mem 0.000B:         a = self.c_proj(a)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:194: mem 0.000B:         a = self.resid_dropout(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:196: mem 0.000B:         outputs = [a, present] + attn_outputs[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:197: mem 0.000B:         return outputs  # a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:228: mem 0.000B:         a = output_attn[0]  # output_attn: a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:230: mem 0.000B:         x = x + a
/home/patrick/python_bin/transformers/modeling_gpt2.py:231: mem 0.000B:         m = self.mlp(self.ln_2(x))
/home/patrick/python_bin/transformers/modeling_gpt2.py:210: mem 0.000B:         h = self.act(self.c_fc(x))
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:211: mem 0.000B:         h2 = self.c_proj(h)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:212: mem 0.000B:         return self.dropout(h2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:232: mem 0.000B:         x = x + m
/home/patrick/python_bin/transformers/modeling_gpt2.py:234: mem 0.000B:         outputs = [x] + output_attn[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:235: mem 0.000B:         return outputs  # x, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:487: mem 0.000B:             hidden_states, present = outputs[:2]
/home/patrick/python_bin/transformers/modeling_gpt2.py:488: mem 0.000B:             if self.output_past:
/home/patrick/python_bin/transformers/modeling_gpt2.py:489: mem 0.000B:                 presents = presents + (present,)
/home/patrick/python_bin/transformers/modeling_gpt2.py:491: mem 0.000B:             if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:479: mem 0.000B:         for i, (block, layer_past) in enumerate(zip(self.h, past)):
/home/patrick/python_bin/transformers/modeling_gpt2.py:480: mem 0.000B:             if self.output_hidden_states:
/home/patrick/python_bin/transformers/modeling_gpt2.py:483: mem 0.000B:             outputs = block(
/home/patrick/python_bin/transformers/modeling_gpt2.py:484: mem 0.000B:                 hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]
/home/patrick/python_bin/transformers/modeling_gpt2.py:225: mem 0.000B:         output_attn = self.attn(
/home/patrick/python_bin/transformers/modeling_gpt2.py:226: mem 0.000B:             self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask
/home/patrick/python_bin/transformers/modeling_gpt2.py:178: mem 0.000B:         x = self.c_attn(x)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:179: mem 0.000B:         query, key, value = x.split(self.split_size, dim=2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:180: mem 0.000B:         query = self.split_heads(query)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:181: mem 0.000B:         key = self.split_heads(key, k=True)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:173: mem 0.000B:             return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)
/home/patrick/python_bin/transformers/modeling_gpt2.py:182: mem 0.000B:         value = self.split_heads(value)
/home/patrick/python_bin/transformers/modeling_gpt2.py:170: mem 0.000B:         new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)
/home/patrick/python_bin/transformers/modeling_gpt2.py:171: mem 0.000B:         x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:172: mem 0.000B:         if k:
/home/patrick/python_bin/transformers/modeling_gpt2.py:175: mem 0.000B:             return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
/home/patrick/python_bin/transformers/modeling_gpt2.py:183: mem 0.000B:         if layer_past is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:187: mem 0.000B:         present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking
/home/patrick/python_bin/transformers/modeling_gpt2.py:189: mem 0.000B:         attn_outputs = self._attn(query, key, value, attention_mask, head_mask)
/home/patrick/python_bin/transformers/modeling_gpt2.py:141: mem 0.000B:         w = torch.matmul(q, k)
/home/patrick/python_bin/transformers/modeling_gpt2.py:142: mem 0.000B:         if self.scale:
/home/patrick/python_bin/transformers/modeling_gpt2.py:143: mem 0.000B:             w = w / math.sqrt(v.size(-1))
/home/patrick/python_bin/transformers/modeling_gpt2.py:144: mem 0.000B:         nd, ns = w.size(-2), w.size(-1)
/home/patrick/python_bin/transformers/modeling_gpt2.py:145: mem 0.000B:         b = self.bias[:, :, ns - nd : ns, :ns]
/home/patrick/python_bin/transformers/modeling_gpt2.py:146: mem 0.000B:         w = w * b - 1e4 * (1 - b)
/home/patrick/python_bin/transformers/modeling_gpt2.py:148: mem 0.000B:         if attention_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:152: mem 0.000B:         w = nn.Softmax(dim=-1)(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:153: mem 0.000B:         w = self.attn_dropout(w)
/home/patrick/python_bin/transformers/modeling_gpt2.py:156: mem 0.000B:         if head_mask is not None:
/home/patrick/python_bin/transformers/modeling_gpt2.py:159: mem 0.000B:         outputs = [torch.matmul(w, v)]
/home/patrick/python_bin/transformers/modeling_gpt2.py:160: mem 0.000B:         if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:162: mem 0.000B:         return outputs
/home/patrick/python_bin/transformers/modeling_gpt2.py:190: mem 0.000B:         a = attn_outputs[0]
/home/patrick/python_bin/transformers/modeling_gpt2.py:192: mem 0.000B:         a = self.merge_heads(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:165: mem 0.000B:         x = x.permute(0, 2, 1, 3).contiguous()
/home/patrick/python_bin/transformers/modeling_gpt2.py:166: mem 0.000B:         new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)
/home/patrick/python_bin/transformers/modeling_gpt2.py:167: mem 0.000B:         return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states
/home/patrick/python_bin/transformers/modeling_gpt2.py:193: mem 0.000B:         a = self.c_proj(a)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:194: mem 0.000B:         a = self.resid_dropout(a)
/home/patrick/python_bin/transformers/modeling_gpt2.py:196: mem 0.000B:         outputs = [a, present] + attn_outputs[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:197: mem 0.000B:         return outputs  # a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:228: mem 0.000B:         a = output_attn[0]  # output_attn: a, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:230: mem 0.000B:         x = x + a
/home/patrick/python_bin/transformers/modeling_gpt2.py:231: mem 0.000B:         m = self.mlp(self.ln_2(x))
/home/patrick/python_bin/transformers/modeling_gpt2.py:210: mem 0.000B:         h = self.act(self.c_fc(x))
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:211: mem 0.000B:         h2 = self.c_proj(h)
/home/patrick/python_bin/transformers/modeling_utils.py:1590: mem 0.000B:         size_out = x.size()[:-1] + (self.nf,)
/home/patrick/python_bin/transformers/modeling_utils.py:1591: mem 0.000B:         x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
/home/patrick/python_bin/transformers/modeling_utils.py:1592: mem 0.000B:         x = x.view(*size_out)
/home/patrick/python_bin/transformers/modeling_utils.py:1593: mem 0.000B:         return x
/home/patrick/python_bin/transformers/modeling_gpt2.py:212: mem 0.000B:         return self.dropout(h2)
/home/patrick/python_bin/transformers/modeling_gpt2.py:232: mem 0.000B:         x = x + m
/home/patrick/python_bin/transformers/modeling_gpt2.py:234: mem 0.000B:         outputs = [x] + output_attn[1:]
/home/patrick/python_bin/transformers/modeling_gpt2.py:235: mem 0.000B:         return outputs  # x, present, (attentions)
/home/patrick/python_bin/transformers/modeling_gpt2.py:487: mem 0.000B:             hidden_states, present = outputs[:2]
/home/patrick/python_bin/transformers/modeling_gpt2.py:488: mem 0.000B:             if self.output_past:
/home/patrick/python_bin/transformers/modeling_gpt2.py:489: mem 0.000B:                 presents = presents + (present,)
/home/patrick/python_bin/transformers/modeling_gpt2.py:491: mem 0.000B:             if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:479: mem 0.000B:         for i, (block, layer_past) in enumerate(zip(self.h, past)):
/home/patrick/python_bin/transformers/modeling_gpt2.py:494: mem 0.000B:         hidden_states = self.ln_f(hidden_states)
/home/patrick/python_bin/transformers/modeling_gpt2.py:496: mem 0.000B:         hidden_states = hidden_states.view(*output_shape)
/home/patrick/python_bin/transformers/modeling_gpt2.py:498: mem 0.000B:         if self.output_hidden_states:
/home/patrick/python_bin/transformers/modeling_gpt2.py:501: mem 0.000B:         outputs = (hidden_states,)
/home/patrick/python_bin/transformers/modeling_gpt2.py:502: mem 0.000B:         if self.output_past:
/home/patrick/python_bin/transformers/modeling_gpt2.py:503: mem 0.000B:             outputs = outputs + (presents,)
/home/patrick/python_bin/transformers/modeling_gpt2.py:504: mem 0.000B:         if self.output_hidden_states:
/home/patrick/python_bin/transformers/modeling_gpt2.py:506: mem 0.000B:         if self.output_attentions:
/home/patrick/python_bin/transformers/modeling_gpt2.py:511: mem 0.000B:         return outputs  # last hidden state, (presents), (all hidden_states), (attentions)
2020-04-07 14:55:11,920 INFO     
Lines with top memory consumption:
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:178: mem 256.000KB:         x = self.c_attn(x)
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:400: mem 0.000B:         if input_ids is not None and inputs_embeds is not None:
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:402: mem 0.000B:         elif input_ids is not None:
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:403: mem 0.000B:             input_shape = input_ids.size()
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:404: mem 0.000B:             input_ids = input_ids.view(-1, input_shape[-1])
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:405: mem 0.000B:             batch_size = input_ids.shape[0]
2020-04-07 14:55:11,921 INFO     
Lines with lowest memory consumption:
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:501: mem 0.000B:         outputs = (hidden_states,)
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:502: mem 0.000B:         if self.output_past:
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:503: mem 0.000B:             outputs = outputs + (presents,)
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:504: mem 0.000B:         if self.output_hidden_states:
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:506: mem 0.000B:         if self.output_attentions:
=> /home/patrick/python_bin/transformers/modeling_gpt2.py:511: mem 0.000B:         return outputs  # last hidden state, (presents), (all hidden_states), (attentions)
2020-04-07 14:55:11,921 INFO     
Total memory increase: 256.000KB
2020-04-07 14:55:11,921 INFO     Going through model with sequence of shape
2020-04-07 14:55:14,968 INFO     =========== RESULTS ===========
2020-04-07 14:55:14,968 INFO     	======= MODEL CHECKPOINT: distilgpt2 =======
2020-04-07 14:55:14,968 INFO     		===== BATCH SIZE: 1 =====
2020-04-07 14:55:14,969 INFO     		distilgpt2/1/16: 0.034s 256.000KB
