============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0
rootdir: /fsx/arthur/transformers
configfile: setup.cfg
plugins: xdist-3.3.1, instafail-0.5.0, sugar-0.9.7, timeout-2.1.0, dash-2.13.0, hypothesis-6.84.3
collected 93 items

tests/models/big_bird/test_tokenization_big_bird.py .................... [ 21%]
...............s..............................s.......s.ss...s......s... [ 98%]
.                                                                        [100%]

=============================== warnings summary ===============================
../miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4
  /fsx/arthur/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if not hasattr(tensorboard, "__version__") or LooseVersion(

../miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:6
  /fsx/arthur/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    ) < LooseVersion("1.15"):

../miniconda3/envs/py39/lib/python3.9/site-packages/_pytest/config/__init__.py:1373
  /fsx/arthur/miniconda3/envs/py39/lib/python3.9/site-packages/_pytest/config/__init__.py:1373: PytestConfigWarning: Unknown config option: doctest_glob
  
    self._warn_or_fail_if_strict(f"Unknown config option: {key}\n")

tests/models/big_bird/test_tokenization_big_bird.py::BigBirdTokenizationTest::test_alignement_methods
  /fsx/arthur/transformers/src/transformers/tokenization_utils_base.py:348: FutureWarning: `BatchEncoding.words()` property is deprecated and should be replaced with the identical, but more self-explanatory `BatchEncoding.word_ids()` property.
    warnings.warn(

tests/models/big_bird/test_tokenization_big_bird.py::BigBirdTokenizationTest::test_padding
tests/models/big_bird/test_tokenization_big_bird.py::BigBirdTokenizationTest::test_padding_to_max_length
  /fsx/arthur/transformers/src/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
    warnings.warn(

tests/models/big_bird/test_tokenization_big_bird.py::BigBirdTokenizationTest::test_padding
  /fsx/arthur/transformers/src/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
    warnings.warn(

tests/models/big_bird/test_tokenization_big_bird.py::BigBirdTokenizationTest::test_prepare_seq2seq_batch
  /fsx/arthur/transformers/src/transformers/tokenization_utils_base.py:3786: FutureWarning: 
  `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
  `__call__` method to prepare your inputs and targets.
  
  Here is a short example:
  
  model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
  
  If you either need to use different keyword arguments for the source and target texts, you should do two calls like
  this:
  
  model_inputs = tokenizer(src_texts, ...)
  labels = tokenizer(text_target=tgt_texts, ...)
  model_inputs["labels"] = labels["input_ids"]
  
  See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
  For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  
    warnings.warn(formatted_warning, FutureWarning)

tests/models/big_bird/test_tokenization_big_bird.py::BigBirdTokenizationTest::test_prepare_seq2seq_batch
  /fsx/arthur/transformers/src/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
    warnings.warn(

tests/models/big_bird/test_tokenization_big_bird.py::BigBirdTokenizationTest::test_saving_tokenizer_trainer
  /fsx/arthur/transformers/src/transformers/training_args.py:1269: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================= 86 passed, 7 skipped, 10 warnings in 56.50s ==================
