{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd007c4df7ea0886a603057dcbd29d886eaa26a0122c4b0c39e2937eba8f7497417",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BigBirdForPreTraining, FlaxBigBirdForPreTraining\n",
    "import numpy as np\n",
    "import jax\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "\n",
    "MODEL_ID = \"google/bigbird-roberta-base\"\n",
    "\n",
    "def get_difference(torch_array, jx_array):\n",
    "    torch_array = torch_array.detach().numpy()\n",
    "    jx_array = np.array(jx_array)\n",
    "    return np.max(torch_array - jx_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing FlaxBigBirdForPreTraining: {('cls', 'predictions', 'decoder', 'kernel'), ('bert', 'embeddings', 'position_ids'), ('cls', 'predictions', 'decoder', 'bias')}\n",
      "- This IS expected if you are initializing FlaxBigBirdForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBigBirdForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "fx_model = FlaxBigBirdForPreTraining.from_pretrained(MODEL_ID, from_pt=True)\n",
    "model = BigBirdForPreTraining.from_pretrained(MODEL_ID, attention_type=\"original_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(DeviceArray([[ 685,  560, 1654, ..., 1515, 1078,  817],\n",
       "              [1119,   60,  337, ...,   51,  243,  871]], dtype=int32),\n",
       " tensor([[ 685,  560, 1654,  ..., 1515, 1078,  817],\n",
       "         [1119,   60,  337,  ...,   51,  243,  871]]))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "array = np.random.randint(1, 512, size=(2, 512))\n",
    "\n",
    "jx_array = jnp.array(array)\n",
    "torch_array = torch.tensor(array, dtype=torch.long)\n",
    "\n",
    "jx_array, torch_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DeviceArray([[[ -8.147562 ,  -9.594704 ,  -7.671412 , ...,  -8.259321 ,\n",
       "                -4.6958823,  -7.7564836],\n",
       "              [ -7.5017667, -10.02726  ,  -9.095716 , ...,  -5.152514 ,\n",
       "                -5.898193 ,  -7.626057 ],\n",
       "              [ -8.320391 , -10.639635 ,  -9.125765 , ...,  -5.7325406,\n",
       "                -7.0324764,  -7.56709  ],\n",
       "              ...,\n",
       "              [ -6.192581 , -10.898497 ,  -8.557592 , ...,  -7.6627326,\n",
       "                -7.0683737,  -8.802221 ],\n",
       "              [ -6.2537403,  -8.485796 ,  -7.414158 , ...,  -6.094995 ,\n",
       "                -8.348713 ,  -6.63145  ],\n",
       "              [ -7.934296 , -10.426262 ,  -9.944581 , ...,  -8.070741 ,\n",
       "                -7.530947 ,  -8.704152 ]],\n",
       "\n",
       "             [[ -8.230836 ,  -9.261623 ,  -7.3377166, ...,  -8.558403 ,\n",
       "                -4.5617714,  -8.134953 ],\n",
       "              [ -6.553763 ,  -8.414845 ,  -8.131858 , ...,  -2.8671875,\n",
       "                -4.0934815,  -7.7758017],\n",
       "              [ -7.222993 ,  -7.805408 ,  -8.3035555, ...,  -3.1251235,\n",
       "                -5.369872 ,  -6.2097783],\n",
       "              ...,\n",
       "              [ -8.028701 ,  -9.787689 ,  -9.05151  , ...,  -4.42139  ,\n",
       "                -4.69197  ,  -8.106948 ],\n",
       "              [ -8.019056 ,  -9.218069 ,  -8.833986 , ...,  -4.724732 ,\n",
       "                -4.6358542,  -8.106748 ],\n",
       "              [ -8.149213 ,  -9.039522 ,  -8.731536 , ...,  -4.9262457,\n",
       "                -4.805724 ,  -8.170534 ]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "fx_out = fx_model(jx_array)[0]\n",
    "fx_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ -8.1476,  -9.5947,  -7.6714,  ...,  -8.2593,  -4.6959,  -7.7565],\n",
       "         [ -7.5018, -10.0273,  -9.0957,  ...,  -5.1525,  -5.8982,  -7.6261],\n",
       "         [ -8.3204, -10.6396,  -9.1258,  ...,  -5.7325,  -7.0325,  -7.5671],\n",
       "         ...,\n",
       "         [ -6.1926, -10.8985,  -8.5576,  ...,  -7.6627,  -7.0684,  -8.8022],\n",
       "         [ -6.2537,  -8.4858,  -7.4142,  ...,  -6.0950,  -8.3487,  -6.6314],\n",
       "         [ -7.9343, -10.4263,  -9.9446,  ...,  -8.0707,  -7.5309,  -8.7042]],\n",
       "\n",
       "        [[ -8.2308,  -9.2616,  -7.3377,  ...,  -8.5584,  -4.5618,  -8.1350],\n",
       "         [ -6.5538,  -8.4148,  -8.1319,  ...,  -2.8672,  -4.0935,  -7.7758],\n",
       "         [ -7.2230,  -7.8054,  -8.3035,  ...,  -3.1251,  -5.3699,  -6.2098],\n",
       "         ...,\n",
       "         [ -8.0287,  -9.7877,  -9.0515,  ...,  -4.4214,  -4.6920,  -8.1069],\n",
       "         [ -8.0191,  -9.2181,  -8.8340,  ...,  -4.7247,  -4.6359,  -8.1067],\n",
       "         [ -8.1492,  -9.0395,  -8.7315,  ...,  -4.9262,  -4.8057,  -8.1705]]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch_out = model(torch_array)[0]\n",
    "torch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference in inputs: 0\n",
      "difference in logits: 5.8412552e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"difference in inputs:\", get_difference(torch_array, jx_array))\n",
    "print(\"difference in logits:\", get_difference(torch_out, fx_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}