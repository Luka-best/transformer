# coding=utf-8
# Copyright 2023 The Pop2Piano Authors and The HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Tokenization class for Pop2Piano."""

import json
import os
from typing import List, Optional, Tuple, Union

import numpy as np
import pretty_midi
import torch

from ...feature_extraction_utils import BatchFeature
from ...tokenization_utils import AddedToken, PreTrainedTokenizer
from ...utils import logging, to_numpy


logger = logging.get_logger(__name__)

VOCAB_FILES_NAMES = {
    "vocab_file": "vocab.json",
}

PRETRAINED_VOCAB_FILES_MAP = {
    "vocab_file": {
        "susnato/pop2piano_dev": "https://huggingface.co/susnato/pop2piano_dev/blob/main/vocab.json",
    },
}


class Pop2PianoTokenizer(PreTrainedTokenizer):
    """
    Constructs a Pop2Piano tokenizer. This tokenizer does not require training. The `Pop2PianoTokenizer.__call__()`
    method can only process one sequence at a time.

    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer
    to: this superclass for more information regarding those methods. However the code does not allow that and only
    supports composing from various genres.

    Args:
        vocab_file (`str`):
            Path to the tokenizer file which contains token-ids such as `TOKEN_SPECIAL`, `DEFAULT_VELOCITY`.
        vocab_size_special (`int`, *optional*, defaults to 4):
            Number of special values.
        vocab_size_note (`int`, *optional*, defaults to 128):
            Number of MIDI note tokens. Only the 88 pitches corresponding to piano keys are actually used.
        vocab_size_velocity (`int`, *optional*, defaults to 2):
            Number of velocity tokens.
        vocab_size_time (`int`, *optional*, defaults to 100):
            Number of beat shift tokens. Beat shifts indicate the relative time shift within the segment quantized into
            8th-notes (half-beats).
        num_bars (`int`, *optional*, defaults to 2):
            Determines cutoff_time_idx in for each token.
    """

    vocab_files_names = VOCAB_FILES_NAMES
    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP

    def __init__(
        self,
        vocab_file,
        vocab_size_special: int = 4,
        vocab_size_note: int = 128,
        vocab_size_velocity: int = 2,
        vocab_size_time: int = 100,
        num_bars: int = 2,
        unk_token="-1",
        eos_token="1",
        pad_token="0",
        bos_token="2",
        **kwargs,
    ):
        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token
        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token
        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token
        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token

        super().__init__(
            unk_token=unk_token,
            eos_token=eos_token,
            pad_token=pad_token,
            bos_token=bos_token,
            vocab_size_special=vocab_size_special,
            vocab_size_note=vocab_size_note,
            vocab_size_velocity=vocab_size_velocity,
            vocab_size_time=vocab_size_time,
            num_bars=num_bars,
            **kwargs,
        )

        with open(vocab_file, "rb") as t_file:
            self.encoder = json.load(t_file)

        self.vocab_size_special = vocab_size_special
        self.vocab_size_note = vocab_size_note
        self.vocab_size_velocity = vocab_size_velocity
        self.vocab_size_time = vocab_size_time
        self.num_bars = num_bars

    @property
    def vocab_size(self):
        return self.vocab_size_special + self.vocab_size_note + self.vocab_size_time + self.vocab_size_velocity

    def get_vocab(self):
        return self.encoder

    def _convert_id_to_token(self, token, time_idx_offset):
        """Decodes the tokens generated by the transformer"""

        if token >= (self.vocab_size_special + self.vocab_size_note + self.vocab_size_velocity):
            type = self.encoder["TOKEN_TIME"]
            value = (
                token - (self.vocab_size_special + self.vocab_size_note + self.vocab_size_velocity)
            ) + time_idx_offset
        elif token >= (self.vocab_size_special + self.vocab_size_note):
            type = self.encoder["TOKEN_VELOCITY"]
            value = int(token - (self.vocab_size_special + self.vocab_size_note))
        elif token >= self.vocab_size_special:
            type = self.encoder["TOKEN_NOTE"]
            value = int(token - self.vocab_size_special)
        else:
            type = self.encoder["TOKEN_SPECIAL"]
            value = int(token)

        return [type, value]

    def _convert_token_to_id(self, token, token_type="3"):
        if token_type == self.encoder["TOKEN_TIME"]:
            return self.vocab_size_special + self.vocab_size_note + self.vocab_size_velocity + token
        elif token_type == self.encoder["TOKEN_VELOCITY"]:
            return self.vocab_size_special + self.vocab_size_note + token
        elif token_type == self.encoder["TOKEN_NOTE"]:
            return self.vocab_size_special + token
        elif token_type == self.encoder["TOKEN_SPECIAL"]:
            return token
        else:
            return -1

    def relative_batch_tokens_to_midi(
        self,
        tokens,
        beatstep,
        beat_offset_idx=None,
        bars_per_batch=None,
        cutoff_time_idx=None,
    ):
        """Converts tokens to midi"""

        beat_offset_idx = 0 if beat_offset_idx is None else beat_offset_idx
        notes = None
        bars_per_batch = 2 if bars_per_batch is None else bars_per_batch

        N = len(tokens)
        for n in range(N):
            _tokens = tokens[n]
            _start_idx = beat_offset_idx + n * bars_per_batch * 4
            _cutoff_time_idx = cutoff_time_idx + _start_idx
            _notes = self.relative_tokens_to_notes(
                _tokens,
                start_idx=_start_idx,
                cutoff_time_idx=_cutoff_time_idx,
            )

            if len(_notes) == 0:
                pass
            elif notes is None:
                notes = _notes
            else:
                notes = np.concatenate((notes, _notes), axis=0)

        if notes is None:
            notes = []
        midi = self.notes_to_midi(notes, beatstep, offset_sec=beatstep[beat_offset_idx])
        return midi, notes

    def relative_tokens_to_notes(self, tokens, start_idx, cutoff_time_idx=None):
        if tokens[0] >= (
            self.vocab_size_special + self.vocab_size_note + self.vocab_size_velocity + self.vocab_size_time
        ):
            tokens = tokens[1:]

        words = [self._convert_id_to_token(token, time_idx_offset=0) for token in tokens]

        if hasattr(start_idx, "item"):
            """if numpy or torch tensor"""
            start_idx = start_idx.item()

        current_idx = start_idx
        current_velocity = 0
        note_onsets_ready = [None for i in range(self.vocab_size_note + 1)]
        notes = []
        for type, number in words:
            if type == self.encoder["TOKEN_SPECIAL"]:
                if number == 1:
                    break
            elif type == self.encoder["TOKEN_TIME"]:
                current_idx += number
                if cutoff_time_idx is not None:
                    current_idx = min(current_idx, cutoff_time_idx)

            elif type == self.encoder["TOKEN_VELOCITY"]:
                current_velocity = number
            elif type == self.encoder["TOKEN_NOTE"]:
                pitch = number
                if current_velocity == 0:
                    # note_offset
                    if note_onsets_ready[pitch] is None:
                        # offset without onset
                        pass
                    else:
                        onset_idx = note_onsets_ready[pitch]
                        if onset_idx >= current_idx:
                            # No time shift after previous note_on
                            pass
                        else:
                            offset_idx = current_idx
                            notes.append([onset_idx, offset_idx, pitch, self.encoder["DEFAULT_VELOCITY"]])
                            note_onsets_ready[pitch] = None
                else:
                    # note_on
                    if note_onsets_ready[pitch] is None:
                        note_onsets_ready[pitch] = current_idx
                    else:
                        # note-on already exists
                        onset_idx = note_onsets_ready[pitch]
                        if onset_idx >= current_idx:
                            # No time shift after previous note_on
                            pass
                        else:
                            offset_idx = current_idx
                            notes.append([onset_idx, offset_idx, pitch, self.encoder["DEFAULT_VELOCITY"]])
                            note_onsets_ready[pitch] = current_idx
            else:
                raise ValueError("Token type not understood!")

        for pitch, note_on in enumerate(note_onsets_ready):
            # force offset if no offset for each pitch
            if note_on is not None:
                if cutoff_time_idx is None:
                    cutoff = note_on + 1
                else:
                    cutoff = max(cutoff_time_idx, note_on + 1)

                offset_idx = max(current_idx, cutoff)
                notes.append([note_on, offset_idx, pitch, self.encoder["DEFAULT_VELOCITY"]])

        if len(notes) == 0:
            return []
        else:
            notes = np.array(notes)
            note_order = notes[:, 0] * 128 + notes[:, 1]
            notes = notes[note_order.argsort()]
            return notes

    def notes_to_midi(self, notes, beatstep, offset_sec=None):
        """Converts notes to midi"""

        new_pm = pretty_midi.PrettyMIDI(resolution=384, initial_tempo=120.0)
        new_inst = pretty_midi.Instrument(program=0)
        new_notes = []
        if offset_sec is None:
            offset_sec = 0.0

        for onset_idx, offset_idx, pitch, velocity in notes:
            new_note = pretty_midi.Note(
                velocity=velocity,
                pitch=pitch,
                start=beatstep[onset_idx] - offset_sec,
                end=beatstep[offset_idx] - offset_sec,
            )
            new_notes.append(new_note)
        new_inst.notes = new_notes
        new_pm.instruments.append(new_inst)
        new_pm.remove_invalid_notes()
        return new_pm

    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:
        """
        Args:
        Saves the tokenizer's vocabulary dictionary to the provided save_directory.
            save_directory (`str`):
                A path to the directory where to saved. It will be created if it doesn't exist.
            filename_prefix (`Optional[str]`, *optional*):
                A prefix to add to the names of the files saved by the tokenizer.
        """
        if not os.path.isdir(save_directory):
            logger.error(f"Vocabulary path ({save_directory}) should be a directory")
            return

        vocab_file = os.path.join(
            save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["vocab_file"]
        )
        with open(vocab_file, "w", encoding="utf-8") as f:
            f.write(json.dumps(self.encoder))

        return (vocab_file,)

    def _to_np(self, tensor):
        """Converts pytorch tensor to np.ndarray."""
        if isinstance(tensor, np.ndarray):
            return tensor
        elif isinstance(tensor, torch.Tensor):
            return tensor.numpy()
        else:
            raise ValueError("dtype not understood! Please use either torch.Tensor or np.ndarray")

    def __call__(
        self,
        token_ids: Union[List, List[np.ndarray], List[torch.Tensor]],
        input_features: BatchFeature,
    ):
        r"""
        This is the `__call__` method for `Pop2PianoTokenizer`. It converts the tokens generated by the transformer to
        midi_tokens and returns it.

        Args:
            token_ids (List of [`~utils.TensorType`]):
                Output tokens of `Pop2PianoConditionalGeneration` model.
            input_features (`BatchFeature`):
                `input_features` returned by `Pop2PianoFeatureExtractor.__call__`.
        Returns:
            List[`pretty_midi.pretty_midi.PrettyMIDI`]:
                returns list of pretty_midi objects.
        """

        # check if they have attention_masks(attention_mask, attention_mask_beatsteps, attention_mask_extrapolated_beatstep) or not
        attention_masks_present = bool(
            hasattr(input_features, "attention_mask")
            and hasattr(input_features, "attention_mask_beatsteps")
            and hasattr(input_features, "attention_mask_extrapolated_beatstep")
        )

        # if we are processing batched inputs then we must need attention_masks
        if not attention_masks_present and input_features["beatsteps"].shape[0] > 1:
            raise ValueError(
                "attention_mask, attention_mask_beatsteps and attention_mask_extrapolated_beatstep must be present for batched inputs! But one of them were not present."
            )

        # check for length mismatch between inputs_embeds, beatsteps and extrapolated_beatstep
        if attention_masks_present:
            # since we know about the number of examples in token_ids from attention_mask
            if (
                sum(input_features["attention_mask"][:, 0] == 0) != input_features["beatsteps"].shape[0]
                or input_features["beatsteps"].shape[0] != input_features["extrapolated_beatstep"].shape[0]
            ):
                raise ValueError(
                    "Length mistamtch between token_ids, beatsteps and extrapolated_beatstep! "
                    f"Found token_ids length - {token_ids.shape[0]}, beatsteps shape - {input_features['beatsteps'].shape[0]}, extrapolated_beatsteps shape - {input_features['extrapolated_beatstep'].shape[0]}"
                )
            if input_features["attention_mask"].shape[0] != token_ids.shape[0]:
                raise ValueError(
                    f"Found attention_mask of length - {input_features['attention_mask'].shape[0]} but token_ids of length - {token_ids.shape[0]}"
                )
        else:
            # if there is no attention mask present then it's surely a single example
            if input_features["beatsteps"].shape[0] != 1 or input_features["extrapolated_beatstep"].shape[0] != 1:
                raise ValueError(
                    "Length mistamtch of beatsteps and extrapolated_beatstep! Since attention_mask is not present the number of examples must be 1, "
                    f"But found beatsteps length - {input_features['beatsteps'].shape[0]}, extrapolated_beatsteps length - {input_features['extrapolated_beatstep'].shape[0]}."
                )

        if attention_masks_present:
            # check for zeros(since token_ids are seperated by zero arrays)
            batch_idx = [e for e, i in enumerate(input_features["attention_mask"][:, 0]) if i == 0]
        else:
            batch_idx = [token_ids.shape[0]]

        pm_object_list = []
        start_idx = 0
        for index, end_idx in enumerate(batch_idx):
            each_tokens_ids = token_ids[start_idx:end_idx]
            # check where the whole example ended by searching for eos_token_id and getting the upper bound
            each_tokens_ids = each_tokens_ids[
                :, : torch.max(torch.where(each_tokens_ids == int(self.eos_token))[1]) + 1
            ]
            beatsteps = input_features["beatsteps"][index]
            extrapolated_beatstep = input_features["extrapolated_beatstep"][index]

            # if attention mask is present then mask out real array/tensor
            if attention_masks_present:
                attention_mask_beatsteps = input_features["attention_mask_beatsteps"][index]
                attention_mask_extrapolated_beatstep = input_features["attention_mask_extrapolated_beatstep"][index]
                beatsteps = beatsteps[: torch.max(torch.where(attention_mask_beatsteps == 1)[0]) + 1]
                extrapolated_beatstep = extrapolated_beatstep[
                    : torch.max(torch.where(attention_mask_extrapolated_beatstep == 1)[0]) + 1
                ]

            each_tokens_ids = to_numpy(each_tokens_ids)
            beatsteps = to_numpy(beatsteps)
            extrapolated_beatstep = to_numpy(extrapolated_beatstep)

            pm, notes = self.relative_batch_tokens_to_midi(
                tokens=each_tokens_ids,
                beatstep=extrapolated_beatstep,
                bars_per_batch=self.num_bars,
                cutoff_time_idx=(self.num_bars + 1) * 4,
            )
            for n in pm.instruments[0].notes:
                n.start += beatsteps[0]
                n.end += beatsteps[0]

            pm_object_list.append(pm)
            start_idx += end_idx + 1  # 1 represents the zero array

        # if single audio then just return the object but otherwise return in list
        if len(pm_object_list) == 1:
            return pm_object_list[0]

        return pm_object_list
