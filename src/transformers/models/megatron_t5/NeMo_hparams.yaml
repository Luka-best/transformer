cfg:
  micro_batch_size: 32
  global_batch_size: 16384
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  resume_from_checkpoint: null
  pipeline_model_parallel_split_rank: 0
  make_vocab_size_divisible_by: 128
  pre_process: true
  post_process: true
  megatron_amp_O2: true
  grad_allreduce_chunk_size_mb: 125
  gradient_as_bucket_view: true
  seq_length: 512
  max_position_embeddings: 512
  encoder:
    num_layers: 24
    hidden_size: 1024
    ffn_hidden_size: 2816
    num_attention_heads: 16
    kv_channels: 64
    init_method_std: 0.015
    hidden_dropout: 0.1
    attention_dropout: 0.1
    position_embedding_type: learned_absolute
    relative_attention_num_buckets: 32
    relative_attention_max_distance: 128
    relative_position_bias_self_attention_only: true
    apply_query_key_layer_scaling: true
    layernorm_epsilon: 1.0e-05
    persist_layer_norm: true
    bias_activation_fusion: true
    grad_div_ar_fusion: true
    masked_softmax_fusion: true
    bias_dropout_add_fusion: true
    bias: true
    normalization: layernorm
    arch: transformer
    activation: geglu
    headscale: false
    transformer_block_type: pre_ln
    openai_gelu: false
    onnx_safe: false
    fp32_residual_connection: false
    activations_checkpoint_method: null
    activations_checkpoint_num_layers: 0
  decoder:
    num_layers: 24
    hidden_size: 1024
    ffn_hidden_size: 2816
    num_attention_heads: 16
    kv_channels: 64
    init_method_std: 0.015
    hidden_dropout: 0.1
    attention_dropout: 0.1
    position_embedding_type: learned_absolute
    relative_attention_num_buckets: 32
    relative_attention_max_distance: 128
    relative_position_bias_self_attention_only: true
    apply_query_key_layer_scaling: true
    layernorm_epsilon: 1.0e-05
    persist_layer_norm: true
    bias_activation_fusion: true
    grad_div_ar_fusion: true
    masked_softmax_fusion: true
    bias_dropout_add_fusion: true
    bias: true
    normalization: layernorm
    arch: transformer
    activation: geglu
    headscale: false
    transformer_block_type: pre_ln
    openai_gelu: false
    onnx_safe: false
    fp32_residual_connection: false
    activations_checkpoint_method: null
    activations_checkpoint_num_layers: 0
  tokenizer:
    library: sentencepiece
    type: null
    model: <path>/spiece.model
    vocab_file: null
    merge_file: null
    num_sentinel_tokens: 100
  native_amp_init_scale: 4294967296
  native_amp_growth_interval: 1000
  fp16_lm_cross_entropy: false
  seed: 1234
  use_cpu_initialization: false
  apex_transformer_log_level: 30
  share_token_embeddings: true
  share_decoder_tokens_head_embeddings: true
  nsys_profile:
    enabled: false
    trace:
    - nvtx
    - cuda
    start_step: 10
    end_step: 10
    ranks:
    - 0
    gen_shape: false
  optim:
    name: fused_adam
    lr: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    weight_decay: 0.01
    sched:
      name: WarmupAnnealing
      min_lr: 1.0e-05
      last_epoch: -1
      warmup_ratio: 0.01
  data:
    data_impl: mmap
    splits_string: 949,50,1
    seq_length: 512
    seq_length_dec: 128
    skip_warmup: true
    num_workers: 8
    dataloader_type: single
    masked_lm_prob: 0.15
    dataset_type: t5
    short_seq_prob: 0.0
    max_ngram_size: 10
    mean_ngram_size: null
    geometric_dist: true
    permutation: false
    whole_word_masking: false
    favor_longer_ngrams: false
    respect_document_boundaries: true
  precision: bf16
