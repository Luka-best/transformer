{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-fine-tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellozhaojian/transformers/blob/master/bert_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_blJrCE1frH",
        "colab_type": "text"
      },
      "source": [
        "**简介**\n",
        "\n",
        "本文介绍如何使用TPU完成对bert的基准模型进行fine-tuning。\n",
        "\n",
        "前置条件：\n",
        "1. 在google cloud里有一个项目。\n",
        "   本次教程中项目名称为 pre-train-bert-sogou； \n",
        "2. 事先我们要准备如下数据：\n",
        "   1. 基准模型， [bert 中文基准模型](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)\n",
        "   2. 文本数据。 数据格式：每行一个句子；一篇文档的句子之间没有空行。不同文档之间有一个空行。\n",
        "\n",
        "\n",
        "主要步骤如下：\n",
        "\n",
        "1. 将基准模型和数据放置到google cloud项目中。\n",
        "\n",
        "2. 将数据准备为tf-record的格式。\n",
        "\n",
        "3. 训练模型。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBmboZA9DEMG",
        "colab_type": "text"
      },
      "source": [
        "**数据、配置、模型准备**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3MlUkJELbrh",
        "colab_type": "text"
      },
      "source": [
        "* 登录google cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q2IkQmjLe3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! gcloud auth application-default login\n",
        "! gcloud auth login\n",
        "from google.colab import auth\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoKQHs6lDQux",
        "colab_type": "text"
      },
      "source": [
        "* 代码准备"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYvGdISK1SME",
        "colab_type": "code",
        "outputId": "3de9d4f2-19fc-4b4c-8c93-5de958cf67ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "! git clone https://github.com/google-research/bert\n",
        "! ls ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'bert' already exists and is not an empty directory.\n",
            "bert  fine_tuning  sample_data\ttmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZAvvq3ZDyQG",
        "colab_type": "text"
      },
      "source": [
        "* 数据模型准备"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHLRYQa-D2oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "GOOGLE_CLOUD_PROJECT_NAME = \"pre-train-bert-sogou\" #@param {type: \"string\" }\n",
        "BUCKET_NAME = \"bert-sogou-pretrain\"  #@param {type: \"string\"}\n",
        "BASE_MODEL_DIR = \"fine_tuning/base_model\" #@param {type: \"string\"}\n",
        "NEW_MODEL_DIR = \"fine_tuning/model\" #@param {type: \"string\"}\n",
        "MODEL_NAME = \"chinese_L-12_H-768_A-12\" #@param {type: \"string\"}\n",
        "#\n",
        "# Fine-Tuning 模型保存的位置 \n",
        "NEW_MODEL_NAME = \"chinese_L-12_H-768_A-12_tutorial\" #@param {type: \"string\"}\n",
        "# 数据文件存放的地址\n",
        "# 每行一个句子。每篇文档的句子之间没有空行。 不同篇的文章有一个空行。\n",
        "INPUT_DATA_DIR = \"fine_tuning/data/tutorial\" #@param {type: \"string\"}\n",
        "\n",
        "PROCESSES = 4 #@param {type: \"integer\"}\n",
        "DO_LOWER_CASE = True\n",
        "MAX_SEQ_LENGTH = 128 #@param {type : \"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param {type: \"number\" }\n",
        "# xxxx\n",
        "MAX_PREDICTIONS = 20 #@param {type: \"integer\"\n",
        "\n",
        "# google 的基准模型放置在这个目录， 其中文件包括：\n",
        "#  bert_config.json；  bert_model.ckpt.data-00000-of-00001 ； bert_model.ckpt.index；\n",
        "#  bert_model.ckpt.meta ；  vocab.txt\n",
        "\n",
        "base_model_name = \"gs://{}/{}/{}\".format(BUCKET_NAME, BASE_MODEL_DIR, MODEL_NAME)\n",
        "\n",
        "INIT_CHECKPOINT = \"{}/bert_model.ckpt\".format(base_model_name)\n",
        "\n",
        "BERT_GCS_DIR = \"gs://{}/{}/{}_latest\".format(BUCKET_NAME, NEW_MODEL_DIR, NEW_MODEL_NAME)\n",
        "VOCAB_FILE = \"gs://{}/{}/{}/vocab.txt\".format(BUCKET_NAME, BASE_MODEL_DIR, MODEL_NAME)\n",
        "TF_RECORD_DIR = \"gs://{}/{}_tfrecord\".format(BUCKET_NAME, INPUT_DATA_DIR)\n",
        "CONFIG_FILE = \"gs://{}/{}/{}/bert_config.json\".format(BUCKET_NAME, BASE_MODEL_DIR, MODEL_NAME)\n",
        "\n",
        "! gcloud config set project $GOOGLE_CLOUD_PROJECT_NAME\n",
        "print(BERT_GCS_DIR)\n",
        "# trick， 在bucket里创建目录的方法\n",
        "! gsutil ls $BERT_GCS_DIR\n",
        "! mkdir -p need_remove\n",
        "! touch need_remove/.tmp\n",
        "! gsutil -m cp -r need_remove/ $BERT_GCS_DIR\n",
        "! gsutil -m cp -r need_remove/ $TF_RECORD_DIR\n",
        "! gsutil rm $BERT_GCS_DIR/.tmp\n",
        "! gsutil rm $TF_RECORD_DIR/.tmp\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRF-jo5-F4Fa",
        "colab_type": "text"
      },
      "source": [
        "**准备tf-record数据**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFXxYpXCF7mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import storage\n",
        "from google.colab import auth, drive\n",
        "\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "file_partitions = [[]]\n",
        "index = 0\n",
        "\n",
        "def list_files(bucketFolder):\n",
        "    \"\"\"List all files in GCP bucket.\"\"\"\n",
        "    files = bucket.list_blobs(prefix=bucketFolder, max_results=1000)\n",
        "    fileList = [file.name for file in files ]\n",
        "    return fileList\n",
        "\n",
        "\n",
        "procesed_set = set([])\n",
        "\n",
        "for filename in list_files(INPUT_DATA_DIR) :\n",
        "    \n",
        "    if filename.find(\"tf\") != -1 : \n",
        "        org_filename = filename.split(\"/\")[-1].split(\".\")[0]    \n",
        "        procesed_set.add(org_filename)\n",
        "        continue\n",
        "\n",
        "\n",
        "for filename in list_files(INPUT_DATA_DIR) :\n",
        "    if filename.find(\"tf\") != -1 or filename.endswith(\"/\"):\n",
        "        continue\n",
        "    \n",
        "    if filename.split(\"/\")[-1] in procesed_set:\n",
        "        continue\n",
        "\n",
        "    if len(file_partitions[index]) == PROCESSES:\n",
        "        file_partitions.append([])\n",
        "        index += 1\n",
        "    file_partitions[index].append(\"gs://{}/{}\".format(BUCKET_NAME, filename))\n",
        "\n",
        "! gsutil ls $TF_RECORD_DIR\n",
        "\n",
        "\n",
        "index = 0\n",
        "for partition in file_partitions:\n",
        "    \n",
        "    for filename in partition:\n",
        "        print(filename, \"----\", index)\n",
        "    index += 1\n",
        "    XARGS_CMD = (\"gsutil ls {} | \"\n",
        "             \"awk 'BEGIN{{FS=\\\"/\\\"}}{{print $NF}}' | \"\n",
        "             \"xargs -n 1 -P {} -I{} \"\n",
        "             \"python3 bert/create_pretraining_data.py \"\n",
        "             \"--input_file=gs://{}/{}/{} \"\n",
        "             \"--output_file={}/{}.tfrecord \"\n",
        "             \"--vocab_file={} \"\n",
        "             \"--do_lower_case={} \"\n",
        "             \"--max_predictions_per_seq={} \"\n",
        "             \"--max_seq_length={} \"\n",
        "             \"--masked_lm_prob={} \"\n",
        "             \"--random_seed=34 \"\n",
        "             \"--dupe_factor=5\")\n",
        "\n",
        "\n",
        "    XARGS_CMD = XARGS_CMD.format(\" \".join(partition),\n",
        "                             PROCESSES, '{}',  BUCKET_NAME, INPUT_DATA_DIR, '{}', \n",
        "                             TF_RECORD_DIR, '{}',\n",
        "                             VOCAB_FILE, DO_LOWER_CASE, \n",
        "                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)\n",
        "\n",
        "    print (XARGS_CMD)\n",
        "\n",
        "    ! $XARGS_CMD\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VY-_C5aa8sb",
        "colab_type": "text"
      },
      "source": [
        "**训练模型**\n",
        "\n",
        "* 链接TPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqrq6u0QbHeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "\n",
        "log = logging.getLogger(\"pre-train-bert\")\n",
        "auth.authenticate_user()\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  USE_TPU = True\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    print(TPU_ADDRESS)\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    tf.contrib.cloud.configure_gcs(session)\n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "  USE_TPU = False\n",
        "print(USE_TPU)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qchAMhyYb3Y0",
        "colab_type": "text"
      },
      "source": [
        "* 设置训练参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG9UxP13cXyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bert import modeling, optimization, tokenization\n",
        "\n",
        "# Input data pipeline config\n",
        "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "\n",
        "# Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
        "SAVE_CHECKPOINTS_STEPS = 250 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "TMP_INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "if TMP_INIT_CHECKPOINT is not None:\n",
        "    INIT_CHECKPOINT = TMP_INIT_CHECKPOINT\n",
        "\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(TF_RECORD_DIR,'*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))\n",
        "\n",
        "! gsutil ls $INIT_CHECKPOINT*\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JkbPIRggtbF",
        "colab_type": "text"
      },
      "source": [
        "* 训练模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU2RlwK0gv36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"bert\")\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
        "from bert import modeling, optimization, tokenization\n",
        "\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=TRAIN_STEPS,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=True)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=BERT_GCS_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)\n",
        "  \n",
        "train_input_fn = input_fn_builder(\n",
        "        input_files=input_files,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "        is_training=True)\n",
        "\n",
        "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fk3fq-KA3C9",
        "colab_type": "text"
      },
      "source": [
        "**在TPU机器上训练模型**\n",
        "\n",
        "将如下代码拷贝到TPU机器上，下载最新的bert代码，然后用python3 执行即可。\n",
        "\n",
        "\n",
        "```python\n",
        "  \"\"\"\n",
        "@file: bert_with_tpu.py\n",
        "@time: 2019/11/10 7:07 上午\n",
        "\"\"\"\n",
        "#######\n",
        "## please exe \"git clone https://github.com/google-research/bert\" before run this script\n",
        "######\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "sys.path.append(\"bert\")\n",
        "from bert import modeling, optimization, tokenization\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "USE_TPU=True\n",
        "TPU_ADDRESS = \"taey2113\"\n",
        "GOOGLE_CLOUD_PROJECT_NAME = \"pre-train-bert-sogou\" #@param {type: \"string\" }\n",
        "BUCKET_NAME = \"bert-sogou-pretrain\"  #@param {type: \"string\"}\n",
        "BASE_MODEL_DIR = \"fine_tuning/base_model\" #@param {type: \"string\"}\n",
        "NEW_MODEL_DIR = \"fine_tuning/model\" #@param {type: \"string\"}\n",
        "MODEL_NAME = \"chinese_L-12_H-768_A-12\" #@param {type: \"string\"}\n",
        "## 以下两个变量是区分不同中文预训练的关键参数\n",
        "## 1. 模型存储目录\n",
        "NEW_MODEL_NAME = \"chinese_L-12_H-768_A-12_tutorial\" #@param {type: \"string\"}\n",
        "## 2. 数据目录\n",
        "INPUT_DATA_DIR = \"fine_tuning/data/tutorial\" #@param {type: \"string\"}\n",
        "\n",
        "PROCESSES = 4 #@param {type: \"integer\"}\n",
        "DO_LOWER_CASE = True\n",
        "MAX_SEQ_LENGTH = 128 #@param {type : \"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param {type: \"number\" }\n",
        "MAX_PREDICTIONS = 20 #@param {type: \"integer\"\n",
        "\n",
        "\n",
        "base_model_name = \"gs://{}/{}/{}\".format(BUCKET_NAME, BASE_MODEL_DIR, MODEL_NAME)\n",
        "\n",
        "INIT_CHECKPOINT = \"{}/bert_model.ckpt\".format(base_model_name)\n",
        "BERT_GCS_DIR = \"gs://{}/{}/{}_latest\".format(BUCKET_NAME, NEW_MODEL_DIR, NEW_MODEL_NAME)\n",
        "VOCAB_FILE = \"gs://{}/{}/{}/vocab.txt\".format(BUCKET_NAME, BASE_MODEL_DIR, MODEL_NAME)\n",
        "TF_RECORD_DIR = \"gs://{}/{}_tfrecord\".format(BUCKET_NAME, INPUT_DATA_DIR)\n",
        "CONFIG_FILE = \"gs://{}/{}/{}/bert_config.json\".format(BUCKET_NAME, BASE_MODEL_DIR, MODEL_NAME)\n",
        "\n",
        "\n",
        "# Input data pipeline config\n",
        "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "\n",
        "# Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-7\n",
        "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
        "SAVE_CHECKPOINTS_STEPS = 250 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "\n",
        "\n",
        "#\"gs://bert-sogou-pretrain/fine_tuning/base_model/chinese_L-12_H-768_A-12/bert_model.ckpt\"\n",
        "TMP_INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "if TMP_INIT_CHECKPOINT is not None:\n",
        "    INIT_CHECKPOINT = TMP_INIT_CHECKPOINT\n",
        "\n",
        "\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(TF_RECORD_DIR,'*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))\n",
        "\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=TRAIN_STEPS,\n",
        "    num_warmup_steps=10,\n",
        "    use_tpu=USE_TPU,\n",
        "    use_one_hot_embeddings=True)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=BERT_GCS_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)\n",
        "\n",
        "train_input_fn = input_fn_builder(\n",
        "    input_files=input_files,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "    is_training=True)\n",
        "\n",
        "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4f4M4DfFa8z",
        "colab_type": "text"
      },
      "source": [
        "**Notice**\n",
        "\n",
        "colab的脚本需要有特定的权限访问google cloud的bucket。如果发现代码在执行TPU的训练任务的时候，发现服务无法访问google cloud里的文件。那么解决方案是到google cloud的bucket里给当前的任务增加权限。"
      ]
    }
  ]
}