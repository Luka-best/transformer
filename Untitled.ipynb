{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import tokenization, BertTokenizer, BertModel, BertForMaskedLM, BertForPreTraining, BertConfig\n",
    "from examples.extract_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/29/2018 10:11:00 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/xd/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "11/29/2018 10:11:00 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /nas/pretrain-bert/pretrain-tensorflow/uncased_L-12_H-768_A-12/\n",
      "11/29/2018 10:11:00 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForPreTraining(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "CONFIG_NAME = 'bert_config.json'\n",
    "BERT_DIR = '/nas/pretrain-bert/pretrain-tensorflow/uncased_L-12_H-768_A-12/'\n",
    "config_file = os.path.join(BERT_DIR, CONFIG_NAME)\n",
    "config = BertConfig.from_json_file(config_file)\n",
    "\n",
    "model = BertForPreTraining.from_pretrained(BERT_DIR)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_attn_topk = 3\n",
    "\n",
    "def has_chinese_label(labels):\n",
    "    labels = [label.split('->')[0].strip() for label in labels]\n",
    "    r = sum([len(label) > 1 for label in labels if label not in ['BOS', 'EOS']]) * 1. / (len(labels) - 1)\n",
    "    return 0 < r < 0.5  # r == 0 means empty query labels used in self attention\n",
    "\n",
    "def _plot_attn(ax1, attn_name, attn, key_labels, query_labels, col, color='b'):\n",
    "    assert len(query_labels) == attn.size(0)\n",
    "    assert len(key_labels) == attn.size(1)\n",
    "\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_xticks([])\n",
    "    ax2 = ax1.twinx()\n",
    "    nlabels = max(len(key_labels), len(query_labels))\n",
    "    pos = range(nlabels)\n",
    "    \n",
    "    if 'self' in attn_name and col < ncols - 1:\n",
    "        query_labels = ['' for _ in query_labels]\n",
    "\n",
    "    for ax, labels in [(ax1, key_labels), (ax2, query_labels)]:\n",
    "        ax.set_yticks(pos)\n",
    "        if has_chinese_label(labels):\n",
    "            ax.set_yticklabels(labels, fontproperties=zhfont)\n",
    "        else:\n",
    "            ax.set_yticklabels(labels)\n",
    "        ax.set_ylim([nlabels - 1, 0])\n",
    "        ax.tick_params(width=0, labelsize='xx-large')\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "#     mask, attn = filter_attn(attn)\n",
    "    for qi in range(attn.size(0)):\n",
    "#         if not mask[qi]:\n",
    "#             continue\n",
    "#         for ki in range(attn.size(1)):\n",
    "        for ki in attn[qi].topk(vis_attn_topk)[1]:\n",
    "            a = attn[qi, ki]\n",
    "            ax1.plot((-1, 1), (ki, qi), color, alpha=a)\n",
    "#     print(attn.mean(dim=0).topk(5)[0])\n",
    "#     ax1.barh(pos, attn.mean(dim=0).data.cpu().numpy())\n",
    "\n",
    "def plot_layer_attn(result_tuple, attn_name='dec_self_attns', layer=0, heads=None):\n",
    "    hypo, nheads, labels_dict = result_tuple\n",
    "    key_labels, query_labels = labels_dict[attn_name]\n",
    "    if heads is None:\n",
    "        heads = range(nheads)\n",
    "    else:\n",
    "        nheads = len(heads)\n",
    "    \n",
    "    stride = 2 if attn_name == 'dec_enc_attns' else 1\n",
    "    nlabels = max(len(key_labels), len(query_labels))\n",
    "    rcParams['figure.figsize'] = 20, int(round(nlabels * stride * nheads / 8 * 1.0))\n",
    "    \n",
    "    rows = nheads // ncols * stride\n",
    "    fig, axes = plt.subplots(rows, ncols)\n",
    "    \n",
    "    # for head in range(nheads):\n",
    "    for head_i, head in enumerate(heads):\n",
    "        row, col = head_i * stride // ncols, head_i * stride % ncols\n",
    "        ax1 = axes[row, col]\n",
    "        attn = hypo[attn_name][layer][head]\n",
    "        _plot_attn(ax1, attn_name, attn, key_labels, query_labels, col)\n",
    "        if attn_name == 'dec_enc_attns':\n",
    "            col = col + 1\n",
    "            axes[row, col].axis('off')  # next subfig acts as blank place holder\n",
    "    # plt.suptitle('%s with %d heads, Layer %d' % (attn_name, nheads, layer), fontsize=20)\n",
    "    plt.show()  \n",
    "            \n",
    "ncols = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_text_to_examples(text):\n",
    "    examples = []\n",
    "    unique_id = 0\n",
    "    if True:\n",
    "        for line in text:\n",
    "            line = tokenization.convert_to_unicode(line)\n",
    "            line = line.strip()\n",
    "            text_a = None\n",
    "            text_b = None\n",
    "            m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "            if m is None:\n",
    "                text_a = line\n",
    "            else:\n",
    "                text_a = m.group(1)\n",
    "                text_b = m.group(2)\n",
    "            examples.append(\n",
    "                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "            unique_id += 1\n",
    "    return examples\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, append_special_tokens=True, replace_mask=True, print_info=False):\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        tokens = []\n",
    "        input_type_ids = []\n",
    "        if append_special_tokens:\n",
    "            tokens.append(\"[CLS]\")\n",
    "            input_type_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            if replace_mask and token == '_':  # XD\n",
    "                token = \"[MASK]\"\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(0)\n",
    "        if append_special_tokens:\n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                if replace_mask and token == '_':  # XD\n",
    "                    token = \"[MASK]\"\n",
    "                tokens.append(token)\n",
    "                input_type_ids.append(1)\n",
    "            if append_special_tokens:\n",
    "                tokens.append(\"[SEP]\")\n",
    "                input_type_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        if ex_index < 5 and print_info:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                unique_id=example.unique_id,\n",
    "                tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                input_type_ids=input_type_ids))\n",
    "    return features\n",
    "\n",
    "def copy_and_mask_features(features):\n",
    "    import copy\n",
    "    masked_feature_copies = []\n",
    "    for feature in features:\n",
    "        for masked_pos in range(len(feature.tokens)):\n",
    "            feature_copy = copy.deepcopy(feature)\n",
    "            feature_copy.input_ids[masked_pos] = tokenizer.vocab[\"[MASK]\"]\n",
    "            masked_feature_copies.append(feature_copy)\n",
    "    return masked_feature_copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lm_probs(tokens, input_ids, probs, topk=5, firstk=20):\n",
    "    def print_pair(token, prob, end_str='', hit_mark=' '):\n",
    "        # token = token.replace('</w>', '').replace('\\n', '/n')\n",
    "        print('{}{: >3} | {: <12}'.format(hit_mark, int(round(prob*100)), token), end=end_str)\n",
    "    \n",
    "    ret = None\n",
    "    for i in range(len(tokens)):\n",
    "        if i >= firstk:\n",
    "            break\n",
    "#         if \"[MASK]\" in tokens and tokens[i] != \"[MASK]\":\n",
    "#             continue\n",
    "        ind_ = input_ids[i].item() if input_ids is not None else tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item()\n",
    "        print_pair(tokens[i], prob_, end_str='\\t')\n",
    "        values, indices = probs[i].topk(topk)\n",
    "        top_pairs = []\n",
    "        for j in range(topk):\n",
    "            ind, prob = indices[j].item(), values[j].item()\n",
    "            hit_mark = '*' if ind == ind_ else ' '\n",
    "            token = tokenizer.ids_to_tokens[ind]\n",
    "            print_pair(token, prob, hit_mark=hit_mark)\n",
    "            top_pairs.append((token, prob))\n",
    "        print()\n",
    "        if tokens[i] == \"[MASK]\":\n",
    "            ret = top_pairs\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "def show_abnormals(tokens, probs, show_suggestions=False):\n",
    "    def gap2color(gap):\n",
    "        if gap <= 5:\n",
    "            return 'yellow_1'\n",
    "        elif gap <= 10:\n",
    "            return 'orange_1'\n",
    "        else:\n",
    "            return 'red_1'\n",
    "        \n",
    "    def print_token(token, suggestion, gap):\n",
    "        if gap == 0:\n",
    "            print(stylize(token + ' ', colored.fg('white') + colored.bg('black')), end='')\n",
    "        else:\n",
    "            print(stylize(token, colored.fg(gap2color(gap)) + colored.bg('black')), end='')\n",
    "            if show_suggestions and gap > 5:\n",
    "                print(stylize('/' + suggestion + ' ', colored.fg('green' if gap > 10 else 'cyan') + colored.bg('black')), end='')\n",
    "            else:\n",
    "                print(stylize(' ', colored.fg(gap2color(gap)) + colored.bg('black')), end='')\n",
    "                # print('/' + suggestion, end=' ')\n",
    "            # print('%.2f' % gap, end=' ')\n",
    "        \n",
    "    avg_gap = 0.\n",
    "    for i in range(1, len(tokens) - 1):  # skip first [CLS] and last [SEP]\n",
    "        ind_ = tokenizer.vocab[tokens[i]]\n",
    "        prob_ = probs[i][ind_].item()\n",
    "        top_prob = probs[i].max().item()\n",
    "        top_ind = probs[i].argmax().item()\n",
    "        gap = math.log(top_prob) - math.log(prob_)\n",
    "        suggestion = tokenizer.ids_to_tokens[top_ind]\n",
    "        print_token(tokens[i], suggestion, gap)\n",
    "        avg_gap += gap\n",
    "    avg_gap /= (len(tokens) - 2)\n",
    "    print()\n",
    "    print(avg_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_cache = {}\n",
    "\n",
    "def analyze_text(text, show_suggestions=False, show_firstk_probs=20):\n",
    "    if text[0] in analyzed_cache:\n",
    "        features, mlm_probs = analyzed_cache[text[0]]\n",
    "        given_mask = \"[MASK]\" in features[0].tokens\n",
    "    else:\n",
    "        examples = convert_text_to_examples(text)\n",
    "        features = convert_examples_to_features(examples, tokenizer, print_info=False)\n",
    "        given_mask = \"[MASK]\" in features[0].tokens\n",
    "        if not given_mask:\n",
    "            features = copy_and_mask_features(features)\n",
    "\n",
    "        input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long)\n",
    "\n",
    "        mlm_logits, _ = model(input_ids, input_type_ids)\n",
    "        mlm_probs = F.softmax(mlm_logits, dim=-1)\n",
    "\n",
    "        if not given_mask:\n",
    "            seq_len, _, vocab_size = mlm_probs.size()\n",
    "            reduced_mlm_probs = torch.Tensor(1, seq_len, vocab_size)\n",
    "            for i in range(seq_len):\n",
    "                reduced_mlm_probs[0, i] = mlm_probs[i, i]\n",
    "            mlm_probs = reduced_mlm_probs\n",
    "        \n",
    "        analyzed_cache[text[0]] = (features, mlm_probs)\n",
    "        \n",
    "    top_pairs = show_lm_probs(features[0].tokens, None, mlm_probs[0], firstk=show_firstk_probs)\n",
    "    if not given_mask:\n",
    "        show_abnormals(features[0].tokens, mlm_probs[0], show_suggestions=show_suggestions)\n",
    "    return top_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 | [CLS]       \t   3 | .              1 | the            1 | )              1 | ,              1 | \"           \n",
      " 100 | paul        \t*100 | paul           0 | he             0 | i              0 | she            0 | they        \n",
      " 100 | tried       \t*100 | tried          0 | try            0 | attempted      0 | trying         0 | tries       \n",
      " 100 | to          \t*100 | to             0 | at             0 | a              0 | and            0 | by          \n",
      " 100 | call        \t*100 | call           0 | contact        0 | get            0 | reach          0 | phone       \n",
      " 100 | linda       \t*100 | linda          0 | her            0 | barbara        0 | ellie          0 | sandra      \n",
      " 100 | on          \t*100 | on             0 | by             0 | over           0 | with           0 | in          \n",
      " 100 | the         \t*100 | the            0 | his            0 | her            0 | my             0 | '           \n",
      "  98 | phone       \t* 98 | phone          1 | telephone      1 | line           0 | call           0 | ##phone     \n",
      " 100 | ,           \t*100 | ,              0 | )              0 | '              0 | of             0 | -           \n",
      " 100 | but         \t*100 | but            0 | when           0 | and            0 | yet            0 | only        \n",
      "   0 | [MASK]      \t  96 | she            3 | linda          0 | her            0 | he             0 | it          \n",
      "  87 | wasn        \t* 87 | wasn          11 | was            1 | she            0 | isn            0 | didn        \n",
      "   0 | '           \t 100 | .              0 | ,              0 | the         *  0 | '              0 | )           \n",
      " 100 | t           \t*100 | t              0 | s              0 | not            0 | didn           0 | wasn        \n",
      "  99 | available   \t* 99 | available      0 | there          0 | answering      0 | unavailable    0 | phone       \n",
      " 100 | .           \t*100 | .              0 | ;              0 | )              0 | ,              0 | !           \n",
      "   0 | [SEP]       \t  26 | .             17 | ,              8 | him            8 | he             5 | her         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('she', 0.9639211893081665),\n",
       " ('linda', 0.031717054545879364),\n",
       " ('her', 0.00037445133784785867),\n",
       " ('he', 0.0003222190134692937),\n",
       " ('it', 0.00027434033108875155)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = [\"Who was Jim Henson? Jim Henson _ a puppeteer.\"]\n",
    "# text = [\"I went to school by bus. I was very tired.\"]\n",
    "# text = [\"Last week I went to the theatre. I had a very good seat. The play was very interesting. But I didn't enjoy it. A young man and a young woman were sitting behind me. They were talking loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angrily. They didn't pay any attention.In the end, I couldn't bear it. I turned round again. 'I can't hear a word!' I said angrily. 'It's none of your business,' the young man said rudely. 'This is a private conversation!'\"]\n",
    "# text = [\"Last week I went to the theatre. I had a very good seat. The play was very interesting. But I didn't enjoy it. A young man and a young woman were sitting behind me. They were talking loudly. I got very angry. I couldn't hear a word. I turned round. I looked at the man angrily. They didn't pay any attention.\"]\n",
    "# text = [\"After the outbreak of the disease, the Ministry of Agriculture and rural areas immediately sent a supervision team to the local. Local Emergency Response Mechanism has been activated in accordance with the requirements, to take blockade, culling, harmless treatment, disinfection and other treatment measures to all disease and culling of pigs for harmless treatment. At the same time, all live pigs and their products are prohibited from transferring out of the blockade area, and live pigs are not allowed to be transported into the blockade area. At present, all the above measures have been implemented.\"]\n",
    "text = [\"The trophy doesn't fit into the brown suitcase because the _ is too large.\"]\n",
    "text = [\"The trophy doesn't fit into the brown suitcase because the _ is too small.\"]\n",
    "\n",
    "text = [\"John couldn't see the stage with Mary in front of him because _ is so short.\"]\n",
    "text = [\"Linda couldn't see the stage with Sam in front of him because _ is so short.\"]\n",
    "text = [\"John couldn't see the stage with Mary in front of him because _ is so tall.\"]\n",
    "text = [\"Mary couldn't see the stage with Tom in front of him because _ is so tall.\"]\n",
    "text = [\"Paul tried to call Linda on the phone, but _ wasn't available.\"]\n",
    "top_pairs = analyze_text(text)\n",
    "top_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have the same hair color.',\n",
       " 'Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have different hair colors.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    # same / different\n",
    "    \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have the same hair color.\",\n",
    "    \"Tom has black hair. Mary has black hair. John has yellow hair. _  and Mary have different hair colors.\",\n",
    "    \"Tom has yellow hair. Mary has black hair. John has black hair. Mary and _ have the same hair color.\",\n",
    "    # because / although\n",
    "    \"John is taller than Mary because/although _ is older/younger.\",\n",
    "    \"The trophy doesn't fit into the brown suitcase because/although the _ is too small/large.\",\n",
    "    # reverse\n",
    "    \"John came then Mary came. They left in reverse order. _ left then _ left.\",\n",
    "    \"John came after Mary. They left in reverse order. _ left after _ .\",\n",
    "    \"John came first, then came Mary. They left in reverse order: _ left first, then left _ .\",\n",
    "    # compare\n",
    "    \"Though John is tall, Tom is taller than John. So John is _ than Tom.\",\n",
    "    \"Tom is taller than John. So _ is shorter than _.\",\n",
    "    \n",
    "    \"The trophy doesn't fit into the brown suitcase because/although the _ is too small/large.\",\n",
    "    \"John thanked Mary because  _ had given help to _ . \",\n",
    "    \"John felt vindicated/crushed when his longtime rival Mary revealed that _ was the winner of the competition.\",\n",
    "    \"John couldn't see the stage with Mary in front of him because _ is so short/tall.\",\n",
    "    \"Although they ran at about the same speed, John beat Sally because _ had such a bad start.\"\n",
    "]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_name = 'enc_self_attns'\n",
    "hypo = {attn_name: [model.bert.encoder.layer[i].attention.self.attention_probs[0] for i in range(config.num_hidden_layers)]}\n",
    "key_labels = query_labels = features[0].tokens\n",
    "labels_dict = {attn_name: (key_labels, query_labels)}\n",
    "result_tuple = (hypo, config.num_attention_heads, labels_dict)\n",
    "plot_layer_attn(result_tuple, attn_name=attn_name, layer=10, heads=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
