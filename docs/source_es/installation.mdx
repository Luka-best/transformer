<!---
Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Gu√≠a de instalaci√≥n

En esta gu√≠a puedes encontrar informaci√≥n para instalar ü§ó Transformers para cualquier biblioteca de Machine Learning con la que est√©s trabajando. Adem√°s, encontrar√°s informaci√≥n sobre c√≥mo establecer el cach√© y c√≥mo configurar ü§ó Transformers para correrlo de manera offline (opcional).

ü§ó Transformers ha sido probada en Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, y Flax. Para instalar la librer√≠a de deep learning con la que desees trabajar, sigue las instrucciones correspondientes listadas a continuaci√≥n:

* [PyTorch](https://pytorch.org/get-started/locally/)
* [TensorFlow 2.0](https://www.tensorflow.org/install/pip)
* [Flax](https://flax.readthedocs.io/en/latest/)

## Instalaci√≥n con pip

Es necesario instalar ü§ó Transformers en un [ambiente virtual](https://docs.python.org/3/library/venv.html). Si necesitas m√°s informaci√≥n sobre ambientes virtuales de Python, consulta esta [gu√≠a](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/
). Un ambiente virtual facilita el manejo de proyectos y evita problemas de compatibilidad entre dependencias.

Comienza por crear un ambiente virtual en el directorio de tu proyecto:

```bash
python -m venv .env
```

Activa el ambiente virtual:

```bash
source .env/bin/activate
```

Ahora est√°s listo para instalar ü§ó Transformers con el siguiente comando:

```bash
pip install transformers
```

Para obtener soporte solo-CPU, puedes instalar ü§ó Transformers y una biblioteca de deep learning con un comando de una sola l√≠nea.

Por ejemplo, para instalar  ü§ó Transformers y Pytorch:

```bash
pip install transformers[torch]
```

ü§ó Transformers y TensorFlow 2.0:

```bash
pip install transformers[tf-cpu]
```

ü§ó Transformers y Flax:

```bash
pip install transformers[flax]
```

Por √∫ltimo, revisa si ü§ó Transformers ha sido instalada exitosamente con el siguiente comando que descarga un modelo pre-entrenado:

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```
Despu√©s imprime la etiqueta y el puntaje:

```bash
[{'label': 'POSITIVE', 'score': 0.9998704791069031}]
```

## Instalaci√≥n desde la fuente

Instala ü§ó Transformers desde la fuente con el siguiente comando:

```bash
pip install git+https://github.com/huggingface/transformers
```

El comando de arriba instala la versi√≥n `master` m√°s actual en vez de la √∫ltima versi√≥n estable. La versi√≥n `master` es √∫til para obtener los √∫ltimos avances de  ü§ó Transformers. Por ejemplo, se puede dar el caso de que un error fue corregido despu√©s de la √∫ltima versi√≥n estable pero a√∫n no se ha liberado un nuevo lanzamiento. Sin embargo, existe la posibilidad de que la versi√≥n `master` no sea estable. El equipo trata de mantener la versi√≥n `master` operacional y la mayor√≠a de los errores son resueltos en unas cuantas horas o un d√≠a. Si encuentras alg√∫n problema, por favor abre un [Issue](https://github.com/huggingface/transformers/issues) para que pueda ser corregido m√°s r√°pido.

Verifica si ü§ó Transformers est√° instalada apropiadamente con el siguiente comando:

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
```

## Instalaci√≥n editable

Quiz√° necesites una instalaci√≥n editable si requieres hacer lo siguiente:
Usar la versi√≥n `master` del c√≥digo fuente.
Contribuir a ü§ó Transformers y probar las modificaciones al c√≥digo fuente.

Clona el repositorio e instalar ü§ó Transformers con los siguientes comandos:

```bash
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .
```

√âstos comandos van a ligar el directorio desde donde clonamos el repositorio al path de las bibliotecas de Python. De ahora en adelante, Python buscar√° dentro de dicho directorio adem√°s de el path de las bibliotecas usuales. Por ejemplo, si los paquetes de Python se encuentran instalados en `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python tambi√©n buscar√° en el directorio desde donde clonamos el repositorio `~/transformers/`.

<Tip warning={true}>

Debes mantener el directorio `transformers`  si deseas seguir usando la biblioteca.

</Tip>

Puedes actualizar tu copia local a la √∫ltima versi√≥n de ü§ó Transformers con el siguiente comando:

```bash
cd ~/transformers/
git pull
```

El ambiente de Python que creaste para la instalaci√≥n de ü§ó Transformers encontrar√° la versi√≥n `master` en la siguiente ejecuci√≥n.

## Instalaci√≥n con conda

Puedes instalar  ü§ó Transformers desde el canal de conda `huggingface` con el siguiente comando:

```bash
conda install -c huggingface transformers
```

## Configuraci√≥n de Cach√©

Los modelos pre entrenados que han sido descargados se encuentran en el cach√© local en  `~/.cache/huggingface/transformers/`. √âste es el directorio por defecto que proporciona la variable de ambiente `TRANSFORMERS_CACHE` del shell. En Windows, el directorio por defecto es `C:\Users\username\.cache\huggingface\transformers`. Puedes cambiar la variables de ambiente del shell mostradas a continuaci√≥n (en √≥rden de prioridad para especificar un directorio de cach√© diferente):

1. Variable de ambiente por defecto del shell: `TRANSFORMERS_CACHE`.
2. Variable de ambiente del shell:`HF_HOME` + `transformers/`.
3. Variable de ambiente del shell: `XDG_CACHE_HOME` + `/huggingface/transformers`.

<Tip>

Si usaste una versi√≥n anterior de la biblioteca,  ü§ó Transformers usar√° la variable de ambiente del shell `PYTORCH_TRANSFORMERS_CACHE` o `PYTORCH_PRETRAINED_BERT_CACHE` a menos que especifiques la variable de ambiente `TRANSFORMERS_CACHE`.

</Tip>


## Modo Offline

ü§ó Transformers puede ejecutarse en un ambiente detr√°s de un firewall o fuera de l√≠nea (Offline) utilizando archivos locales. Para habilitar √©ste comportamiento, declara la variable de ambiente `HF_DATASETS_OFFLINE=1`.

<Tip>

Puedes a√±adir  [ü§ó Datasets](https://huggingface.co/docs/datasets/) al flujo de entrenamiento offline declarando la variable de ambiente  `HF_DATASETS_OFFLINE=1`.

</Tip>

Por ejemplo, puedes ejecutar un programa dentro de un ambiente detr√°s de un firewall con el siguiente comando:

```bash
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

Ejecuta este mismo programa en una instancia offline con el siguiente comando:

```bash
HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

El programa deber√≠a de ejecutarse sin problemas de retraso (timeout) porque ahora sabe que s√≥lo debe buscar archivos locales.

### Obtener modelos y tokenizers para uso offline

Otra opci√≥n para usar ü§ó Transformers offline es descargando previamente los archivos y despu√©s apuntar al path local donde se encuentren. Existen tres maneras diferentes de utilizar ü§ó Transformers offline:

Descarga un archivo mediante la interfaz de usuario del [Model Hub](https://huggingface.co/models) haciendo click en el √≠cono ‚Üì.

![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png)


Utiliza el flujo de [`PreTrainedModel.from_pretrained`] y [`PreTrainedModel.save_pretrained`]:
1. Descarga previamente los archivos con [`PreTrainedModel.from_pretrained`]:

    ```py
                >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

    >>> tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
    >>> model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")
    ```


2. Guarda los archivos en un directorio espec√≠fico con [`PreTrainedModel.save_pretrained`]:

```py
 >>> tokenizer.save_pretrained("./your/path/bigscience_t0")
 >>> model.save_pretrained("./your/path/bigscience_t0")
 ```

3. Cuando te encuentres offline, recarga los archivos con [`PreTrainedModel.from_pretrained`] desde el directorio especificado: 
  
```py
 >>> tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
 >>> model = AutoModel.from_pretrained("./your/path/bigscience_t0")
 ```

Descarga de manera program√°tica los archivos con la biblioteca [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub):

1. Instala la biblioteca [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub) en tu ambiente virtual: 

```bash
  python -m pip install huggingface_hub
 ```

2. Utiliza la funci√≥n [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) para descargar un archivo a un path espec√≠fico. Por ejemplo, el siguiente comando descarga el archivo `config.json` del modelo [T0](https://huggingface.co/bigscience/T0_3B) al path deseado:

```py
 >>> from huggingface_hub import hf_hub_download

 >>> hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
 ```

Una vez que el archivo ha sido descargado y guardado en el cach√© local, es necesario especificar su path local para cargarlo y utilizarlo:

```py
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

<Tip>

Para m√°s detalles sobre c√≥mo descargar archivos almacenados en el Hub, consulta la secci√≥n  [How to download files from the Hub](https://huggingface.co/docs/hub/how-to-downstream).

</Tip>






