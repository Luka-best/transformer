<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Haciendo fine-tuning a un modelo pre-entrenado

El uso de un modelo pre-entrenado tiene importantes ventajas. Reduce los costes de computaciÃ³n, su huella de carbono, y te permite utilizar modelos de Ãºltima generaciÃ³n sin tener que entrenar uno desde cero. ðŸ¤— Transformers proporciona acceso a miles de modelos pre-entrenados en una amplia gama de tareas. Cuando utilizas un modelo pre-entrenado, lo entrenas con un dataset especÃ­fico para tu tarea. Esto se conoce como fine-tuning, una tÃ©cnica de entrenamiento increÃ­blemente poderosa. En este tutorial haremos fine-tuning a un modelo pre-entrenado con un framework de Deep Learning de tu elecciÃ³n:

- Aplicar fine-tuning a un modelo pre-entrenado con ðŸ¤— Transformers [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer).
- Aplicar fine-tuning a un modelo pre-entrenado en TensorFlow con Keras.
- Aplicar fine-tuning** a un modelo pre-entrenado en PyTorch nativo.

## Prepara un dataset

[https://youtu.be/_BZearw7f0w](https://youtu.be/_BZearw7f0w)

Antes de aplicar fine-tuning a un modelo pre-entrenado, descarga un dataset y prepÃ¡ralo para el entrenamiento. El tutorial anterior nos enseÃ±Ã³ cÃ³mo procesar los datos para el entrenamiento, y ahora es la oportunidad de poner a prueba estas habilidades.

Comienza cargando el dataset de [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full):

```python
from datasets import load_dataset

# Cargamos el dataset
dataset = load_dataset("yelp_review_full")
dataset[100]

# Salida
{'label': 0,
 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}
```

Como ya sabes, necesitas un tokenizador para procesar el texto e incluir una estrategia para el padding y el truncamiento, manejando cualquier tamaÃ±o de una variable secuencial. Para procesar tu dataset en un solo paso, utiliza el mÃ©todo ðŸ¤— Datasets [map](https://huggingface.co/docs/datasets/process#map) para aplicar una funciÃ³n de preprocesamiento sobre todo el dataset:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

Si lo deseas, puedes crear un subconjunto mÃ¡s pequeÃ±o del dataset completo para aplicarle fine-tuning y asÃ­ reducir el tiempo.

```python
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

## Fine-tuning con [Trainer]

[https://youtu.be/nvBXf7s7vTI](https://youtu.be/nvBXf7s7vTI)

ðŸ¤— Transformers proporciona una clase **[Trainer**](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) optimizada para el entrenamiento de modelos ðŸ¤— Transformers, haciendo mÃ¡s fÃ¡cil el inicio del entrenamiento sin necesidad de escribir manualmente su propio ciclo. La API del [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) soporta una amplia gama de opciones de entrenamiento y caracterÃ­sticas como el logging, el gradient accumulation y la mixed precision.

Comienza cargando tu modelo y especifica el nÃºmero de labels previstas. A partir del Card Dataset Yelp Review, que como ya sabemos tiene 5 labels:

Note: Cada dataset deberÃ­a ir acompaÃ±ado de un [Dataset Card](https://huggingface.co/docs/datasets/v1.12.0/dataset_card.html) para promover el uso responsable, y alertar al usuario de cualquier posible sesgo dentro del conjunto de datos.

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

> VerÃ¡s una advertencia acerca de que algunos de los pesos pre-entrenados que no estÃ¡n siendo utilizados y que algunos pesos estÃ¡n siendo inicializados al azar. No te preocupes, esto es completamente normal. El head/cabezal pre-entrenado del modelo BERT se descarta y se sustituye por un head de clasificaciÃ³n inicializado aleatoriamente. Puedes aplicar fine-tuning a este nuevo head del modelo en su tarea de clasificaciÃ³n de secuencias, haciendo transfer learning del modelo pre-entrenado.
>

## HiperparÃ¡metros de entrenamiento

A continuaciÃ³n, crea una clase [TrainingArguments](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.TrainingArguments) que contenga todos los hiperparÃ¡metros que puedes ajustar, asÃ­ como los indicadores para activar las diferentes opciones de entrenamiento. Para este tutorial puedes empezar con los [hiperparÃ¡metros](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) de entrenamiento por defecto, pero siÃ©ntete libre de experimentar con ellos para encontrar tu configuraciÃ³n Ã³ptima.

Especifica dÃ³nde vas a guardar los checkpoints de tu entrenamiento:

```python
from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer")
```

## MÃ©tricas
El [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) no evalÃºa automÃ¡ticamente el rendimiento del modelo durante el entrenamiento. TendrÃ¡s que pasarle a [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) una funciÃ³n para calcular y hacer un reporte de las mÃ©tricas. La librerÃ­a de ðŸ¤— Datasets proporciona una funciÃ³n de [accuracy](https://huggingface.co/metrics/accuracy) simple que puedes cargar con la funciÃ³n `load_metric` (ver este [tutorial](https://huggingface.co/docs/datasets/metrics) para mÃ¡s informaciÃ³n):

```python
import numpy as np
from datasets import load_metric

metric = load_metric("accuracy")
```

Definamos la funciÃ³n `compute_metric` para calcular el accuracy de tus predicciones. Antes de pasar tus predicciones a procesar, necesitas convertir las predicciones a logits (recuerda que todos los modelos de ðŸ¤— Transformers devuelven logits).

```python
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

Si quieres controlar tus mÃ©tricas de evaluaciÃ³n durante el fine-tuning, especifica el parÃ¡metro `evaluation_strategy` en tus argumentos de entrenamiento para que el modelo tenga en cuenta la mÃ©trica de evaluaciÃ³n al final de cada Ã©poca:

```python
from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")
```

## Trainer

Crea un objeto [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) con su modelo, argumentos de entrenamiento, conjuntos de datos de entrenamiento y de prueba, y su funciÃ³n de evaluaciÃ³n:

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
```

A continuaciÃ³n, aplique fine-tuning a su modelo llamando a [train()](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer.train):

```python
trainer.train()
```

## Fine-tuning con Keras

[https://youtu.be/rnTGBy2ax1c](https://youtu.be/rnTGBy2ax1c)

Los modelos de ðŸ¤— Transformers tambiÃ©n permite realizar el entrenamiento en TensorFlow con la API de Keras. SÃ³lo es necesario hacer algunos cambios antes de hacer fine-tuning.

## Convierte el dataset al formato de TensorFlow

El [DefaultDataCollator](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/data_collator#transformers.DefaultDataCollator) junta los tensores en un batch para que el modelo se entrene en Ã©l. AsegÃºrate de especificar `return_tensors` para devolver los tensores de TensorFlow:

```python
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")
```

> [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) utiliza [DataCollatorWithPadding](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/data_collator#transformers.DataCollatorWithPadding) por defecto, por lo que no es necesario especificar explÃ­citamente un intercalador de datos.
>

A continuaciÃ³n, convierte los datasets0 tokenizados en datasets de TensorFlow con el mÃ©todo `[to_tf_dataset](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset)`. Especifica tus entradas en `columns`, y tu etiqueta en `label_cols`:

```python
tf_train_dataset = small_train_dataset.to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = small_eval_dataset.to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

## Compilar y ajustar(fit)

Carguemos un modelo TensorFlow con el nÃºmero esperado de labels:

```python
import tensorflow as tf
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

A continuaciÃ³n, compila y aplica fine-tuning a tu modelo con `fit` como lo harÃ­as con cualquier otro modelo Keras:

```python
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=tf.metrics.SparseCategoricalAccuracy(),
)

model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

## Fine-tune en PyTorch nativo

[https://youtu.be/Dh9CL8fyG80](https://youtu.be/Dh9CL8fyG80)

El [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer) se encarga del ciclo de entrenamiento y permite aplicar fine-tuning a un modelo en una sola lÃ­nea de cÃ³digo. Para los usuarios que prefieren escribir su propio ciclo de entrenamiento, tambiÃ©n puedes aplicar fine-tuning a un modelo ðŸ¤— Transformers en PyTorch nativo.

En este punto, es posible que necesites reiniciar tu notebook o ejecutar el siguiente cÃ³digo para liberar algo de memoria:

```python
del model
del pytorch_model
del trainer
torch.cuda.empty_cache()
```

A continuaciÃ³n, haz un post-proceso manualmente al 
`tokenized_dataset` y asÃ­ prepararlo para el entrenamiento.

1. Elimina la columna de `text` porque el modelo no acepta texto en crudo como entrada:

```python
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
```

1. Cambia el nombre de la columna de `label` a `labels` porque el modelo espera que el argumento se llame `labels`:

```python
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
```

1. Establece el formato del dataset para devolver tensores PyTorch en lugar de listas:

```python
tokenized_datasets.set_format("torch")
```

A continuaciÃ³n, crea un subconjunto mÃ¡s pequeÃ±o del dataset, como se ha mostrado anteriormente, para acelerar el fine-tuning:

```python
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

## DataLoader

Crea un DataLoader para sus datasets de entrenamiento y de prueba para poder iterar sobre batches de datos:

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)
```

Cargue su modelo con el nÃºmero de labels previstas:

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

## Optimizador y learning rate

Crea un optimizador y el learning rate para aplicar fine-tuning al modelo. Vamos a utilizar el optimizador [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) de PyTorch:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

Crea el learning rate por defecto desde el [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer):

```python
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)
```

Por Ãºltimo, especifique el entorno de ejecuciÃ³n para utilizar una GPU si tiene acceso a una. De lo contrario, el entrenamiento en una CPU puede llevar varias horas en lugar de un par de minutos.

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
```

> Consigue acceso gratuito a una GPU en la nube si es que no tienes este recurso de forma local, con un notebook alojado como [Colaboratory](https://colab.research.google.com/) o [SageMaker StudioLab](https://studiolab.sagemaker.aws/).
>

Genial, Â¡ahora estÃ¡s listo para entrenar! ðŸ¥³

## Ciclo de entrenamiento

Para hacer un seguimiento del progreso del entrenamiento, utiliza la librerÃ­a [tqdm](https://tqdm.github.io/) para aÃ±adir una barra de progreso sobre el nÃºmero de pasos de entrenamiento:

```python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

## MÃ©tricas

De la misma manera que necesitas aÃ±adir una funciÃ³n de evaluaciÃ³n al [Trainer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer), necesitas hacer lo mismo cuando escribas tu propio ciclo de entrenamiento. Pero en lugar de calcular y reportar la mÃ©trica al final de cada Ã©poca, esta vez acumularÃ¡s todos los batches con [add_batch](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=add_batch#datasets.Metric.add_batch) y calcularÃ¡s la mÃ©trica al final.

```python
metric = load_metric("accuracy")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

## Recursos adicionales

Para mÃ¡s ejemplos de Fine-tuning, consulta:

ðŸ¤— [Transformers Examples](https://github.com/huggingface/transformers/tree/master/examples) incluye scripts para entrenar tareas comunes de NLP en PyTorch y TensorFlow.

ðŸ¤— [Transformers Notebooks](https://huggingface.co/docs/transformers/notebooks) contiene varios notebooks sobre cÃ³mo aplicar fine-tuning un modelo para tareas especÃ­ficas en PyTorch y TensorFlow.