<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Fine-tuning de um modelo pr√©-treinado

[[open-in-colab]]

O uso de um modelo pr√©-treinado tem importantes vantagens. Redu√ß√£o do custo computacional, a pegada de carbono, e te
permite utilizar modelos de √∫ltima gera√ß√£o sem ter que treinar um novo desde o in√≠cio.
O ü§ó Transformers proporciona acesso a milhares de modelos pr√©-treinados numa ampla gama de tarefas.
Quando utilizar um modelo pr√©-treinado, treine-o com um dataset espec√≠fico para a sua tarefa.
Isto √© chamado de fine-tuning, uma t√©cnica de treinamento incrivelmente poderosa. Neste tutorial faremos o fine-tuning
de um modelo pr√©-treinado com um framework de Deep Learning da sua escolha:

* Fine-tuning de um modelo pr√©-treinado com o ü§ó Transformers [`Trainer`].
* Fine-tuning de um modelo pr√©-treinado no TensorFlow com o Keras.
* Fine-tuning de um modelo pr√©-treinado em PyTorch nativo.

<a id='data-processing'></a>

## Preparando um dataset

<Youtube id="_BZearw7f0w"/>

Antes de aplicar o fine-tuning a um modelo pr√©-treinado, baixe um dataset e prepare-o para o treinamento.
O tutorial anterior ensinar√° a processar os dados para o treinamento, e ent√£o poder√° ter a oportunidade de testar
esse novo conhecimento em algo pr√°tico.

Comece carregando o dataset [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full):

```py
>>> from datasets import load_dataset

```

Como j√° sabe, √© necess√°rio ter um tokenizador para processar o texto e incluir uma estrat√©gia de padding e truncamento,
para manejar qualquer tamanho var√≠avel de sequ√™ncia. Para processar o seu dataset em apenas um passo, utilize o m√©todo de
ü§ó Datasets [`map`](https://huggingface.co/docs/datasets/process.html#map) para aplicar uma fun√ß√£o de preprocessamento sobre
todo o dataset.

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


>>> def tokenize_function(examples):
```

Se desejar, √© poss√≠vel criar um subconjunto menor do dataset completo para aplicar o fine-tuning e assim reduzir o tempo necess√°rio.

``````

<a id='trainer'></a>

## Fine-tuning com o `Trainer`

<Youtube id="nvBXf7s7vTI"/>

O ü§ó Transformers proporciona uma classe [`Trainer`] otimizada para o treinamento de modelos de ü§ó Transformers,
facilitando os primeiros passos do treinamento sem a necessidade de escrever manualmente o seu pr√≥prio ciclo.
A API do [`Trainer`] suporta um grande conjunto de op√ß√µes de treinamento e funcionalidades, como o logging,
o gradient accumulation e o mixed precision.

Comece carregando seu modelo e especifique o n√∫mero de labels de previs√£o.
A partir do [Card Dataset](https://huggingface.co/datasets/yelp_review_full#data-fields) do Yelp Reveiw, que ja
sabemos ter 5 labels usamos o seguinte c√≥digo:

``````

<Tip>

    Voc√™ ver√° um alerta sobre alguns pesos pr√©-treinados que n√£o est√£o sendo utilizados e que alguns pesos est√£o
    sendo inicializados aleatoriamente. N√£o se preocupe, essa mensagem √© completamente normal.
    O header/cabe√ß√°rio pr√©-treinado do modelo BERT √© descartado e substitui-se por um header de classifica√ß√£o
    inicializado aleatoriamente. Assim, pode aplicar o fine-tuning a este novo header do modelo em sua tarefa
    de classifica√ß√£o de sequ√™ncias fazendo um transfer learning do modelo pr√©-treinado.

</Tip>

### Hiperpar√¢metros de treinamento

Em seguida, crie uma classe [`TrainingArguments`] que contenha todos os hiperpar√¢metros que possam ser ajustados, assim
como os indicadores para ativar as diferentes op√ß√µes de treinamento. Para este tutorial, voc√™ pode come√ßar o treinamento
usando os [hiperpar√°metros](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) padr√£o,
por√©m, sinta-se livre para experimentar com eles e encontrar uma configura√ß√£o √≥tima.

Especifique onde salvar os checkpoints do treinamento:

``````

### M√©tricas

O [`Trainer`] n√£o avalia automaticamente o rendimento do modelo durante o treinamento. Ser√° necess√°rio passar ao
[`Trainer`] uma fun√ß√£o para calcular e fazer um diagn√≥stico sobre as m√©tricas. A biblioteca ü§ó Datasets proporciona
uma fun√ß√£o de [`accuracy`](https://huggingface.co/metrics/accuracy) simples que pode ser carregada com a fun√ß√£o
`load_metric` (ver este [tutorial](https://huggingface.co/docs/datasets/metrics.html) para mais informa√ß√µes):

```py
```

Defina a fun√ß√£o `compute` dentro de `metric` para calcular a precis√£o das suas predi√ß√µes.
Antes de passar as suas predi√ß√µes ao `compute`, √© necess√°rio converter as predi√ß√µes √† logits (lembre-se que
todos os modelos de ü§ó Transformers retornam logits).

```py
```

Se quiser controlar as suas m√©tricas de avalia√ß√£o durante o fine-tuning, especifique o par√¢metro `evaluation_strategy`
nos seus argumentos de treinamento para que o modelo considere a m√©trica de avalia√ß√£o ao final de cada √©poca:

``````

### Trainer

Crie um objeto [`Trainer`] com o seu modelo, argumentos de treinamento, conjuntos de dados de treinamento e de teste, e a sua fun√ß√£o de avalia√ß√£o:

```py
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
```

Em seguida, aplique o fine-tuning a seu modelo chamado [`~transformers.Trainer.train`]:

``````

<a id='keras'></a>

## Fine-tuning com Keras

<Youtube id="rnTGBy2ax1c"/>

Os modelos de ü§ó Transformers tamb√©m permitem realizar o treinamento com o TensorFlow com a API do Keras.
Contudo, ser√° necess√°rio fazer algumas mudan√ßas antes de realizar o fine-tuning.

### Convers√£o do dataset ao formato do TensorFlow

O [`DefaultDataCollator`] junta os tensores em um batch para que o modelo possa ser treinado em cima deles.
Assegure-se de especificar os `return_tensors` para retornar os tensores do TensorFlow:

``````

<Tip>

    O [`Trainer`] utiliza [`DataCollatorWithPadding`] por padr√£o, ent√£o voc√™ n√£o precisa especificar explicitamente um
    colador de dados (data collator).

</Tip>

Em seguida, converta os datasets tokenizados em datasets do TensorFlow com o m√©todo
[`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset).
Especifique suas entradas em `columns` e seu r√≥tulo em `label_cols`:

```py
>>> tf_train_dataset = small_train_dataset.to_tf_dataset(
...     columns=["attention_mask", "input_ids", "token_type_ids"],
...     label_cols=["labels"],
...     shuffle=True,
...     collate_fn=data_collator,
...     batch_size=8,
... )

>>> tf_validation_dataset = small_eval_dataset.to_tf_dataset(
...     columns=["attention_mask", "input_ids", "token_type_ids"],
...     label_cols=["labels"],
```

### Compila√ß√£o e ajustes

Carregue um modelo do TensorFlow com o n√∫mero esperado de r√≥tulos:

```py
```

A seguir, compile e ajuste o fine-tuning a seu modelo com [`fit`](https://keras.io/api/models/model_training_apis/) como
faria com qualquer outro modelo do Keras:

```py
>>> model.compile(
...     optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
...     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
```

<a id='pytorch_native'></a>

## Fine-tune em PyTorch nativo

<Youtube id="Dh9CL8fyG80"/>

O [`Trainer`] se encarrega do ciclo de treinamento e permite aplicar o fine-tuning a um modelo em uma linha de c√≥digo apenas.
Para os usu√°rios que preferirem escrever o seu pr√≥prio ciclo de treinamento, tamb√©m √© poss√≠vel aplicar o fine-tuning a um
modelo de ü§ó Transformers em PyTorch nativo.

Neste momento, talvez ocorra a necessidade de reinicar seu notebook ou executar a seguinte linha de c√≥digo para liberar
mem√≥ria:

```py
```

Em sequ√™ncia, faremos um post-processing manual do `tokenized_dataset` e assim prepar√°-lo para o treinamento.

1. Apague a coluna de `text` porque o modelo n√£o aceita texto cru como entrada:

    ```    ```

2. Troque o nome da coluna `label` para `labels`, pois o modelo espera um argumento de mesmo nome:

    ```    ```

3. Defina o formato do dataset para retornar tensores do PyTorch no lugar de listas:

    ```    ```

Em sequ√™ncia, crie um subconjunto menor do dataset, como foi mostrado anteriormente, para aceler√°-lo o fine-tuning.

``````

### DataLoader

Crie um `DataLoader` para os seus datasets de treinamento e de teste para poder iterar sobre batches de dados:

```py
```

Carregue seu modelo com o n√∫mero de labels esperados:

``````

### Otimiza√ß√£o e configura√ß√£o do Learning Rate

Crie um otimizador e um learning rate para aplicar o fine-tuning ao modelo.
Iremos utilizar o otimizador [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) do PyTorch:

``````

Defina o learning rate do [`Trainer`]:

```py
>>> from transformers import get_scheduler

>>> num_epochs = 3
```

Por √∫ltimo, especifique o `device` do ambiente para utilizar uma GPU se tiver acesso √† alguma. Caso contr√°rio, o treinamento
em uma CPU pode acabar levando v√°rias horas em vez de minutos.

```py
```

<Tip>

    Se necess√°rio, voc√™ pode obter o acesso gratuito a uma GPU na n√∫vem por meio de um notebook no
    [Colaboratory](https://colab.research.google.com/) ou [SageMaker StudioLab](https://studiolab.sagemaker.aws/)
    se n√£o tiver esse recurso de forma local.

</Tip>

Perfeito, agora estamos prontos para come√ßar o treinamento! ü•≥

### Ciclo de treinamento

Para visualizar melhor o processo de treinamento, utilize a biblioteca [tqdm](https://tqdm.github.io/) para adicionar
uma barra de progresso sobre o n√∫mero de passos percorridos no treinamento atual:

```py
>>> from tqdm.auto import tqdm

>>> progress_bar = tqdm(range(num_training_steps))

>>> model.train()
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         batch = {k: v.to(device) for k, v in batch.items()}
...         outputs = model(**batch)
...         loss = outputs.loss
...         loss.backward()

```

### M√©tricas

Da mesma forma que √© necess√°rio adicionar uma fun√ß√£o de avalia√ß√£o ao [`Trainer`], √© necess√°rio fazer o mesmo quando
escrevendo o pr√≥prio ciclo de treinamento. Contudo, em vez de calcular e retornar a m√©trica final de cada √©poca,
voc√™ dever√° adicionar todos os batches com [`add_batch`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=add_batch#datasets.Metric.add_batch)
e calcular a m√©trica apenas no final.

```py
>>> metric = load_metric("accuracy")
>>> model.eval()
>>> for batch in eval_dataloader:
...     batch = {k: v.to(device) for k, v in batch.items()}
...     with torch.no_grad():
...         outputs = model(**batch)

...     logits = outputs.logits
```

<a id='additional-resources'></a>

## Recursos adicionais

Para mais exemplos de fine-tuning acesse:

- [ü§ó Transformers Examples](https://github.com/huggingface/transformers/tree/main/examples) inclui scripts
para treinas tarefas comuns de NLP em PyTorch e TensorFlow.

- [ü§ó Transformers Notebooks](notebooks) cont√©m v√°rios notebooks sobre como aplicar o fine-tuning a um modelo
para tarefas espec√≠ficas no PyTorch e TensorFlow.
