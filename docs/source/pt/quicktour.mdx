<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Tour r√°pido

[[open-in-colab]]

Inicie aqui a jornada com ü§ó Transformers! Comece usando [`pipeline`] para r√°pida infer√™ncia e facilmente carregue um modelo pr√©-treinado e um tokenizer com [AutoClass](./model_doc/auto) para resolver tarefas de texto, vis√£o ou √°udio.

<Tip>

Todos exemplos de c√≥digo apresentados na documenta√ß√£o possuem um substituto no canto superior esquerdo entre PyTorch e TensorFlow. Caso contr√°rio, √© esperado que funcione para ambos back-ends sem nenhuma altera√ß√£o.

</Tip>

## Pipeline

[`pipeline`] √© a maneira mais f√°cil de usar um modelo pr√©-treinado para uma dada tarefa.

<Youtube id="tiZFewofSLM"/>

A [`pipeline`] apoia diversas tarefas fora da caixa:

**Texto**:
* An√°lise sentimental: classifica a polaridade de um texto.
* Gera√ß√£o de texto (em Ingl√™s): gera texto a partir de uma entrada.
* Reconhecimento de entidade mencionada: legenda cada palavra com uma classe que a representa (pessoa, data, local, etc...) 
* Respostas: extrai uma resposta dado algum contexto e uma quest√£o
* M√°scara de preenchimento: preenche o espa√ßo, dado um texto com m√°scaras de palavras.
* Sumariza√ß√£o: gera o resumo de um texto longo ou documento.
* Tradu√ß√£o: traduz texto para outra l√≠ngua.
* Extra√ß√£o de caracter√≠sticas: cria um tensor que representa o texto.

**Imagem**:
* Classifica√ß√£o de imagens: classifica uma imagem.
* Segmenta√ß√£o de imagem: classifica cada pixel da imagem.
* Detec√ß√£o de objetos: detecta objetos em uma imagem.

**Audio**:
* Classfica√ß√£o de √°udio: legenda um trecho de √°udio fornecido.
* Reconhecimento de fala autom√°tico: transcreve audio em texto.

<Tip>

Para mais detalhes sobre a [`pipeline`] e tarefas associadas, siga a documenta√ß√£o [aqui](./main_classes/pipelines).

</Tip>

### Uso da pipeline

No exemplo a seguir, voc√™ usar√° [`pipeline`] para an√°lise sentimental.

Instale as seguintes depend√™ncias se voc√™ ainda n√£o o fez:


<frameworkcontent>
<pt>
```bash
pip install torch
```
</pt>
<tf>
```bash
pip install tensorflow
```
</tf>
</frameworkcontent>

Importe [`pipeline`] e especifique a tarefa que deseja completar:

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

A pipeline baixa and armazena um [modelo pr√©-treinado](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) padr√£o e tokenizer para an√°lise sentimental. Agora voc√™ pode usar `classifier` no texto alvo: 

```py
>>> classifier("We are very happy to show you the ü§ó Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

Para mais de uma senten√ßa, passe uma lista para a [`pipeline`], a qual retornar√° uma lista de dicion√°rios:

```py
>>> results = classifier(["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

A [`pipeline`] tamb√©m pode iterar sobre um Dataset inteiro. Comece instalando a biblioteca de [ü§ó Datasets](https://huggingface.co/docs/datasets/):

```bash
pip install datasets 
```

Crie uma [`pipeline`] com a tarefa que deseja resolver e o modelo que deseja usar.

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

A seguir, carregue uma base de dados (confira a ü§ó [Inicia√ß√£o em Datasets](https://huggingface.co/docs/datasets/quickstart.html) para mais detalhes) que voc√™ gostaria de iterar sobre. Por exemplo, vamos carregar o dataset [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14):

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
```

Precisamos garantir que a taxa de amostragem do conjunto de dados corresponda √† taxa de amostragem em que o facebook/wav2vec2-base-960h foi treinado.

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

Os arquivos de √°udio s√£o carregados e re-amostrados automaticamente ao chamar a coluna `"audio"`. 
Vamos extrair as arrays de formas de onda originais das primeiras 4 amostras e pass√°-las como uma lista para o pipeline:

```py
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I TURN A JOIN A COUNT']
```

Para um conjunto de dados maior onde as entradas s√£o maiores (como em fala ou vis√£o), ser√° necess√°rio passar um gerador em vez de uma lista que carregue todas as entradas na mem√≥ria. Consulte a [documenta√ß√£o do pipeline](./main_classes/pipelines) para mais informa√ß√µes.

### Use outro modelo e tokenizer na pipeline

A [`pipeline`] pode acomodar qualquer modelo do [Model Hub](https://huggingface.co/models), facilitando sua adapta√ß√£o para outros casos de uso. Por exemplo, se voc√™ quiser um modelo capaz de lidar com texto em franc√™s, use as tags no Model Hub para filtrar um modelo apropriado. O principal resultado filtrado retorna um [modelo BERT](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) bil√≠ngue ajustado para an√°lise de sentimentos. √ìtimo, vamos usar este modelo!

```py
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

<frameworkcontent>
<pt>
Use o [`AutoModelForSequenceClassification`] e [`AutoTokenizer`] para carregar o modelo pr√©-treinado e seu tokenizer associado (mais em `AutoClass` abaixo):

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</pt>
<tf>

Use the [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the pretrained model and it's associated tokenizer (more on an `TFAutoClass` below):

```py
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```
</tf>
</frameworkcontent>

Then you can specify the model and tokenizer in the [`pipeline`], and apply the `classifier` on your target text:

```py
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes tr√®s heureux de vous pr√©senter la biblioth√®que ü§ó Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

If you can't find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our [fine-tuning tutorial](./training) to learn how. Finally, after you've fine-tuned your pretrained model, please consider sharing it (see tutorial [here](./model_sharing)) with the community on the Model Hub to democratize NLP for everyone! ü§ó

## AutoClass

<Youtube id="AhChOFRegn4"/>

Under the hood, the [`AutoModelForSequenceClassification`] and [`AutoTokenizer`] classes work together to power the [`pipeline`]. An [AutoClass](./model_doc/auto) is a shortcut that automatically retrieves the architecture of a pretrained model from it's name or path. You only need to select the appropriate `AutoClass` for your task and it's associated tokenizer with [`AutoTokenizer`]. 

Let's return to our example and see how you can use the `AutoClass` to replicate the results of the [`pipeline`].

### AutoTokenizer

A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called *tokens*. There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization [here](./tokenizer_summary)). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.

Load a tokenizer with [`AutoTokenizer`]:

```py
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model's *vocabulary*.

Pass your text to the tokenizer:

```py
>>> encoding = tokenizer("We are very happy to show you the ü§ó Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

The tokenizer will return a dictionary containing:

* [input_ids](./glossary#input-ids): numerical representions of your tokens.
* [atttention_mask](.glossary#attention-mask): indicates which tokens should be attended to.

Just like the [`pipeline`], the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:

<frameworkcontent>
<pt>
```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```
</pt>
<tf>
```py
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```
</tf>
</frameworkcontent>

Read the [preprocessing](./preprocessing) tutorial for more details about tokenization.

### AutoModel

<frameworkcontent>
<pt>
ü§ó Transformers provides a simple and unified way to load pretrained instances. This means you can load an [`AutoModel`] like you would load an [`AutoTokenizer`]. The only difference is selecting the correct [`AutoModel`] for the task. Since you are doing text - or sequence - classification, load [`AutoModelForSequenceClassification`]:

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

See the [task summary](./task_summary) for which [`AutoModel`] class to use for which task.

</Tip>

Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding `**`:

```py
>>> pt_outputs = pt_model(**pt_batch)
```

The model outputs the final activations in the `logits` attribute. Apply the softmax function to the `logits` to retrieve the probabilities:

```py
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```
</pt>
<tf>
ü§ó Transformers provides a simple and unified way to load pretrained instances. This means you can load an [`TFAutoModel`] like you would load an [`AutoTokenizer`]. The only difference is selecting the correct [`TFAutoModel`] for the task. Since you are doing text - or sequence - classification, load [`TFAutoModelForSequenceClassification`]:

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

See the [task summary](./task_summary) for which [`AutoModel`] class to use for which task.

</Tip>

Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:

```py
>>> tf_outputs = tf_model(tf_batch)
```

The model outputs the final activations in the `logits` attribute. Apply the softmax function to the `logits` to retrieve the probabilities:

```py
>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> tf_predictions  # doctest: +IGNORE_RESULT
```
</tf>
</frameworkcontent>

<Tip>

All ü§ó Transformers models (PyTorch or TensorFlow) outputs the tensors *before* the final activation
function (like softmax) because the final activation function is often fused with the loss.

</Tip>

Models are a standard [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) so you can use them in your usual training loop. However, to make things easier, ü§ó Transformers provides a [`Trainer`] class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the `fit` method from [Keras](https://keras.io/). Refer to the [training tutorial](./training) for more details.

<Tip>

ü§ó Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `None` are ignored.

</Tip>

### Save a model

<frameworkcontent>
<pt>
Once your model is fine-tuned, you can save it with its tokenizer using [`PreTrainedModel.save_pretrained`]:

```py
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)
```

When you are ready to use the model again, reload it with [`PreTrainedModel.from_pretrained`]:

```py
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```
</pt>
<tf>
Once your model is fine-tuned, you can save it with its tokenizer using [`TFPreTrainedModel.save_pretrained`]:

```py
>>> tf_save_directory = "./tf_save_pretrained"
>>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT
>>> tf_model.save_pretrained(tf_save_directory)
```

When you are ready to use the model again, reload it with [`TFPreTrainedModel.from_pretrained`]:

```py
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```
</tf>
</frameworkcontent>

One particularly cool ü§ó Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The `from_pt` or `from_tf` parameter can convert the model from one framework to the other:

<frameworkcontent>
<pt>
```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
</pt>
<tf>
```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
```
</tf>
</frameworkcontent>
