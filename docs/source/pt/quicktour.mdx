<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Tour r√°pido

[[open-in-colab]]

Comece a trabalhar com ü§ó Transformers! Comece usando [`pipeline`] para r√°pida infer√™ncia e facilmente carregue um modelo pr√©-treinado e um tokenizer com [AutoClass](./model_doc/auto) para resolver tarefas de texto, vis√£o ou √°udio.

<Tip>

Todos os exemplos de c√≥digo apresentados na documenta√ß√£o t√™m um bot√£o no canto superior direito para escolher se voc√™ deseja ocultar ou mostrar o c√≥digo no Pytorch ou no TensorFlow. Caso contr√°rio, √© esperado que funcione para ambos back-ends sem nenhuma altera√ß√£o.

</Tip>

## Pipeline

[`pipeline`] √© a maneira mais f√°cil de usar um modelo pr√©-treinado para uma dada tarefa.

<Youtube id="tiZFewofSLM"/>

A [`pipeline`] apoia diversas tarefas fora da caixa:

**Texto**:
* An√°lise sentimental: classifica a polaridade de um texto.
* Gera√ß√£o de texto (em Ingl√™s): gera texto a partir de uma entrada.
* Reconhecimento de entidade mencionada: legenda cada palavra com uma classe que a representa (pessoa, data, local, etc...) 
* Respostas: extrai uma resposta dado algum contexto e uma quest√£o
* M√°scara de preenchimento: preenche o espa√ßo, dado um texto com m√°scaras de palavras.
* Sumariza√ß√£o: gera o resumo de um texto longo ou documento.
* Tradu√ß√£o: traduz texto para outra l√≠ngua.
* Extra√ß√£o de caracter√≠sticas: cria um tensor que representa o texto.

**Imagem**:
* Classifica√ß√£o de imagens: classifica uma imagem.
* Segmenta√ß√£o de imagem: classifica cada pixel da imagem.
* Detec√ß√£o de objetos: detecta objetos em uma imagem.

**Audio**:
* Classfica√ß√£o de √°udio: legenda um trecho de √°udio fornecido.
* Reconhecimento de fala autom√°tico: transcreve audio em texto.

<Tip>

Para mais detalhes sobre a [`pipeline`] e tarefas associadas, siga a documenta√ß√£o [aqui](./main_classes/pipelines).

</Tip>

### Uso da pipeline

No exemplo a seguir, voc√™ usar√° [`pipeline`] para an√°lise sentimental.

Instale as seguintes depend√™ncias se voc√™ ainda n√£o o fez:


<frameworkcontent>
<pt>
```bash
pip install torch
```
</pt>
<tf>
```bash
pip install tensorflow
```
</tf>
</frameworkcontent>

Importe [`pipeline`] e especifique a tarefa que deseja completar:

``````

A pipeline baixa and armazena um [modelo pr√©-treinado](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) padr√£o e tokenizer para an√°lise sentimental. Agora voc√™ pode usar `classifier` no texto alvo: 

``````

Para mais de uma senten√ßa, passe uma lista para a [`pipeline`], a qual retornar√° uma lista de dicion√°rios:

```py
>>> results = classifier(["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."])
```

A [`pipeline`] tamb√©m pode iterar sobre um Dataset inteiro. Comece instalando a biblioteca de [ü§ó Datasets](https://huggingface.co/docs/datasets/):

```bash
pip install datasets 
```

Crie uma [`pipeline`] com a tarefa que deseja resolver e o modelo que deseja usar.

```py
```

A seguir, carregue uma base de dados (confira a ü§ó [Inicia√ß√£o em Datasets](https://huggingface.co/docs/datasets/quickstart.html) para mais detalhes) que voc√™ gostaria de iterar sobre. Por exemplo, vamos carregar o dataset [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14):

``````

Precisamos garantir que a taxa de amostragem do conjunto de dados corresponda √† taxa de amostragem em que o facebook/wav2vec2-base-960h foi treinado.

``````

Os arquivos de √°udio s√£o carregados e re-amostrados automaticamente ao chamar a coluna `"audio"`. 
Vamos extrair as arrays de formas de onda originais das primeiras 4 amostras e pass√°-las como uma lista para o pipeline:

``````

Para um conjunto de dados maior onde as entradas s√£o maiores (como em fala ou vis√£o), ser√° necess√°rio passar um gerador em vez de uma lista que carregue todas as entradas na mem√≥ria. Consulte a [documenta√ß√£o do pipeline](./main_classes/pipelines) para mais informa√ß√µes.

### Use outro modelo e tokenizer na pipeline

A [`pipeline`] pode acomodar qualquer modelo do [Model Hub](https://huggingface.co/models), facilitando sua adapta√ß√£o para outros casos de uso. Por exemplo, se voc√™ quiser um modelo capaz de lidar com texto em franc√™s, use as tags no Model Hub para filtrar um modelo apropriado. O principal resultado filtrado retorna um [modelo BERT](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) bil√≠ngue ajustado para an√°lise de sentimentos. √ìtimo, vamos usar este modelo!

``````

<frameworkcontent>
<pt>
Use o [`AutoModelForSequenceClassification`] e [`AutoTokenizer`] para carregar o modelo pr√©-treinado e seu tokenizer associado (mais em `AutoClass` abaixo):

```py
```
</pt>
<tf>

Use o [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] para carregar o modelo pr√©-treinado e o tokenizer associado (mais em `TFAutoClass` abaixo):

```py
```
</tf>
</frameworkcontent>

Ent√£o voc√™ pode especificar o modelo e o tokenizador na [`pipeline`] e aplicar o `classifier` no seu texto alvo: 

``````

Se voc√™ n√£o conseguir achar um modelo para o seu caso de uso, precisar√° usar fine-tune em um modelo pr√©-treinado nos seus dados. Veja nosso [tutorial de fine-tuning](./training) para descobrir como. Finalmente, depois que voc√™ tiver usado esse processo em seu modelo, considere compartilh√°-lo conosco (veja o tutorial [aqui](./model_sharing)) na plataforma Model Hub afim de democratizar NLP! ü§ó

## AutoClass

<Youtube id="AhChOFRegn4"/>

Por baixo dos panos, as classes [`AutoModelForSequenceClassification`] e [`AutoTokenizer`] trabalham juntas para fortificar o [`pipeline`]. Um [AutoClass](./model_doc/auto) √© um atalho que automaticamente recupera a arquitetura de um modelo pr√©-treinado a partir de seu nome ou caminho. Basta selecionar a `AutoClass` apropriada para sua tarefa e seu tokenizer associado com [`AutoTokenizer`]. 

Vamos voltar ao nosso exemplo e ver como voc√™ pode usar a `AutoClass` para replicar os resultados do [`pipeline`].

### AutoTokenizer

Um tokenizer √© respons√°vel por pr√©-processar o texto em um formato que seja compreens√≠vel para o modelo. Primeiro, o tokenizer dividir√° o texto em palavras chamadas *tokens*. Existem v√°rias regras que regem o processo de tokeniza√ß√£o, incluindo como dividir uma palavra e em que n√≠vel (saiba mais sobre tokeniza√ß√£o [aqui](./tokenizer_summary)). A coisa mais importante a lembrar, por√©m, √© que voc√™ precisa instanciar o tokenizer com o mesmo nome do modelo para garantir que est√° usando as mesmas regras de tokeniza√ß√£o com as quais um modelo foi pr√©-treinado.

Carregue um tokenizer com [`AutoTokenizer`]:

```py
```

Em seguida, o tokenizer converte os tokens em n√∫meros para construir um tensor como entrada para o modelo. Isso √© conhecido como o *vocabul√°rio* do modelo.

Passe o texto para o tokenizer:

```py
>>> encoding = tokenizer("We are very happy to show you the ü§ó Transformers library.")
```

O tokenizer retornar√° um dicion√°rio contendo:

* [input_ids](./glossary#input-ids): representa√ß√µes num√©ricas de seus tokens.
* [atttention_mask](.glossary#attention-mask): indica quais tokens devem ser atendidos.

Assim como o [`pipeline`], o tokenizer aceitar√° uma lista de entradas. Al√©m disso, o tokenizer tamb√©m pode preencher e truncar o texto para retornar um lote com comprimento uniforme:

<frameworkcontent>
<pt>
```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ü§ó transformers library.", "We hope you don't hate it."],
...     padding=True,
```
</pt>
<tf>
```py
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ü§ó Transformers library.", "We hope you don't hate it."],
...     padding=True,
```
</tf>
</frameworkcontent>

Leia o tutorial de [pr√©-processamento](./pr√©-processamento) para obter mais detalhes sobre tokeniza√ß√£o.

### AutoModel

<frameworkcontent>
<pt>
ü§ó Transformers fornecem uma maneira simples e unificada de carregar inst√¢ncias pr√©-treinadas. Isso significa que voc√™ pode carregar um [`AutoModel`] como carregaria um [`AutoTokenizer`]. A √∫nica diferen√ßa √© selecionar o [`AutoModel`] correto para a tarefa. Como voc√™ est√° fazendo classifica√ß√£o de texto ou sequ√™ncia, carregue [`AutoModelForSequenceClassification`]:

```py
```

<Tip>

Veja o [sum√°rio de tarefas](./task_summary) para qual classe de [`AutoModel`] usar para cada tarefa.

</Tip>

Agora voc√™ pode passar seu grupo de entradas pr√©-processadas diretamente para o modelo. Voc√™ apenas tem que descompactar o dicion√°rio usando `**`:

``````

O modelo gera as ativa√ß√µes finais no atributo `logits`. Aplique a fun√ß√£o softmax aos `logits` para recuperar as probabilidades:

```py
>>> from torch import nn

```
</pt>
<tf>
ü§ó Transformers fornecem uma maneira simples e unificada de carregar inst√¢ncias pr√©-treinadas. Isso significa que voc√™ pode carregar um [`TFAutoModel`] como carregaria um [`AutoTokenizer`]. A √∫nica diferen√ßa √© selecionar o [`TFAutoModel`] correto para a tarefa. Como voc√™ est√° fazendo classifica√ß√£o de texto ou sequ√™ncia, carregue [`TFAutoModelForSequenceClassification`]:

```py
```

<Tip>

Veja o [sum√°rio de tarefas](./task_summary) para qual classe de [`AutoModel`] usar para cada tarefa.

</Tip>

Agora voc√™ pode passar seu grupo de entradas pr√©-processadas diretamente para o modelo atrav√©s da passagem de chaves de dicion√°rios ao tensor.

``````

O modelo gera as ativa√ß√µes finais no atributo `logits`. Aplique a fun√ß√£o softmax aos `logits` para recuperar as probabilidades:

```py
```
</tf>
</frameworkcontent>

<Tip>

Todos os modelos de ü§ó Transformers (PyTorch ou TensorFlow) geram tensores *antes* da fun√ß√£o de ativa√ß√£o final (como softmax) pois essa fun√ß√£o algumas vezes √© fundida com a perda.


</Tip>

Os modelos s√£o um standard [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) ou um [`tf.keras.Model`](https: //www.tensorflow.org/api_docs/python/tf/keras/Model) para que voc√™ possa us√°-los em seu loop de treinamento habitual. No entanto, para facilitar as coisas, ü§ó Transformers fornece uma classe [`Trainer`] para PyTorch que adiciona funcionalidade para treinamento distribu√≠do, precis√£o mista e muito mais. Para o TensorFlow, voc√™ pode usar o m√©todo `fit` de [Keras](https://keras.io/). Consulte o [tutorial de treinamento](./training) para obter mais detalhes.

<Tip>

As sa√≠das do modelo ü§ó Transformers s√£o classes de dados especiais para que seus atributos sejam preenchidos automaticamente em um IDE.
As sa√≠das do modelo tamb√©m se comportam como uma tupla ou um dicion√°rio (por exemplo, voc√™ pode indexar com um inteiro, uma parte ou uma string), caso em que os atributos `None` s√£o ignorados.

</Tip>

### Salvar um modelo

<frameworkcontent>
<pt>
Uma vez que seu modelo estiver afinado, voc√™ pode salv√°-lo com seu Tokenizer usando [`PreTrainedModel.save_pretrained`]:

``````

Quando voc√™ estiver pronto para us√°-lo novamente, recarregue com [`PreTrainedModel.from_pretrained`]:

``````
</pt>
<tf>
Uma vez que seu modelo estiver afinado, voc√™ pode salv√°-lo com seu Tokenizer usando [`TFPreTrainedModel.save_pretrained`]:

``````

Quando voc√™ estiver pronto para us√°-lo novamente, recarregue com [`TFPreTrainedModel.from_pretrained`]

``````
</tf>
</frameworkcontent>

Um recurso particularmente interessante dos ü§ó Transformers √© a capacidade de salvar um modelo e recarreg√°-lo como um modelo PyTorch ou TensorFlow. Use `from_pt` ou `from_tf` para converter o modelo de um framework para outro:

<frameworkcontent>
<pt>
```py
```
</pt>
<tf>
```py
```
</tf>
</frameworkcontent>