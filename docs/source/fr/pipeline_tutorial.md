<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Pipelines pour l'inf√©rence

Le [`pipeline`] facilite l'utilisation de n'importe quel mod√®le sur le [Hub](https://huggingface.co/models) pour l'inf√©rence sur n'importe quelle t√¢che de langage, vision par ordinateur, parole et multimodale. M√™me si vous n'avez pas d'exp√©rience avec une modalit√© sp√©cifique ou si vous n'√™tes pas familier avec le code associ√© aux mod√®les, vous pouvez toujours les utiliser pour l'inf√©rence avec le [`pipeline`] ! Ce tutoriel vous apprend √† :

  * Utiliser un [`pipeline`] pour l'inf√©rence.
  * Utiliser un tokenizer ou un mod√®le sp√©cifique.
  * Utiliser un [`pipeline`] pour des t√¢ches audio, de vision et multimodales.

<Tip>

Consultez la documentation du [`pipeline`] pour avoir une liste compl√®te des t√¢ches support√©es et des param√®tres disponibles.

</Tip>

## Utilisation du pipeline

Bien que chaque t√¢che ait un [`pipeline`] associ√©, il est plus simple d'utiliser l'abstraction g√©n√©rale [`pipeline`] qui contient tous les pipelines sp√©cifiques aux t√¢ches. Le [`pipeline`] charge automatiquement un mod√®le par d√©faut et une classe de pr√©traitement capable de faire de l'inf√©rence pour votre t√¢che. Prenons l'exemple de l'utilisation du [`pipeline`] pour la reconnaissance automatique de la parole, ou la transcription de la parole en texte.

1. Commencez par cr√©er un [`pipeline`] et sp√©cifiez une t√¢che :

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition")
```

2. Passez vos donn√©es au [`pipeline`]. Pour la reconnaissance automatique de la parole, il s'agit d'un fichier audio :

```py
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}
```

Pas le r√©sultat que vous attendiez ? Cherchez parmi les [mod√®les de reconnaissance automatique de la parole les plus t√©l√©charg√©s](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending) sur le Hub pour voir si vous pouvez obtenir une meilleure transcription.

Essayons le mod√®le [Whisper large-v2](https://huggingface.co/openai/whisper-large) d'OpenAI. Whisper a √©t√© publi√© 2 ans apr√®s Wav2Vec2, et a √©t√© entra√Æn√© avec pr√®s de 10 fois plus de donn√©es. Ainsi, il bat Wav2Vec2 sur la plupart des benchmarks. Il peut √©galement pr√©dire la ponctuation et la casse, ce qui n'est pas possible avec Wav2Vec2.

Voyons comment il se comporte ici :

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

Ce r√©sultat semble plus pr√©cis ! Pour une comparaison approfondie entre Wav2Vec2 et Whisper, consultez le [cours sur les Transformers audio](https://huggingface.co/learn/audio-course/chapter5/asr_models). Nous vous encourageons √©galement √† consulter le Hub pour des mod√®les dans diff√©rentes langues, des mod√®les sp√©cialis√©s dans certains domaines, etc. Vous pouvez v√©rifier et comparer les r√©sultats des mod√®les directement depuis votre navigateur sur le Hub pour voir s'ils sont meilleurs ou traitent mieux les cas particuliers que les autres. Et si vous ne trouvez pas de mod√®le pour votre cas d'utilisation, vous pouvez toujours commencer √† [entra√Æner](training) le v√¥tre !

Si vous avez plusieurs donn√©es d'entr√©e, vous pouvez les passer sous forme de liste :

```py
transcriber(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
)
```

Les pipelines sont id√©aux pour l'exp√©rimentation car passer d'un mod√®le √† un autre est trivial ; cependant, il est possible de les optimiser pour des cas d'utilisation plus intentifs que l'exp√©rimentation. Consultez les guides suivants qui d√©taillent l'utilisation des pipelines sur des ensembles de donn√©es entiers ou dans un serveur web :

  * [Utilisation des pipelines sur un ensemble de donn√©es](#utilisation-des-pipelines-sur-un-ensemble-de-donn√©es)
  * [Utilisation des pipelines pour un serveur web](./pipeline_webserver)

## Param√®tres

[`pipeline`] supporte de nombreux param√®tres ; certains sont sp√©cifiques √† la t√¢che, d'autres sont g√©n√©raux √† tous les pipelines. En g√©n√©ral, vous pouvez sp√©cifier des param√®tres n'importe o√π :

```py
transcriber = pipeline(model="openai/whisper-large-v2", my_parameter=1)

out = transcriber(...)  # This will use `my_parameter=1`.
out = transcriber(..., my_parameter=2)  # This will override and use `my_parameter=2`.
out = transcriber(...)  # This will go back to using `my_parameter=1`.
```

Regardons 3 param√®tres importants :

### Processeur

Si vous utilisez `device=n`, le pipeline met automatiquement le mod√®le sur le processeur (CPU ou GPU) sp√©cifi√©. Cela fonctionnera que vous utilisiez PyTorch ou Tensorflow.

```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0)
```

Si le mod√®le est trop grand pour une seule GPU et que vous utilisez PyTorch, vous pouvez d√©finir `device_map="auto"` pour d√©terminer automatiquement comment charger et stocker les poids du mod√®le. L'utilisation du param√®tre `device_map` n√©cessite la librairie ü§ó [Accelerate](https://huggingface.co/docs/accelerate) :

```bash
pip install --upgrade accelerate
```

Le code suivant charge et stocke automatiquement les poids du mod√®le sur plusieurs processeurs :

```py
transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto")
```

Notez que si `device_map="auto"` est donn√©, il n'est pas n√©cessaire d'ajouter le param√®tre `device=device` lors de la cr√©ation de votre `pipeline` car vous pouvez rencontrer un comportement inattendu !

### Taille de lot

Par d√©faut, les pipelines ne font pas d'inf√©rence par lots pour des raisons expliqu√©es en d√©tail [ici](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching). La raison est que le traitement par lots n'est pas n√©cessairement plus rapide, et peut m√™me √™tre beaucoup plus lent dans certains cas.

Mais si cela peut √™tre utile dans votre cas d'utilisation, vous pouvez utiliser :

```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames)
```

Cet exemple ex√©cute le pipeline sur les 4 fichiers audio fournis, mais il les passera par lots de 2 au mod√®le (qui est sur un GPU, o√π le traitement par lots est plus susceptible d'aider) sans n√©cessiter de code suppl√©mentaire de votre part.
Le r√©sultat sera le m√™me que si vous aviez ex√©cut√© le pipeline sur chaque fichier audio individuellement. L'objectif est de vous aider √† obtenir les r√©sultats plus rapidement.

Les pipelines peuvent √©galement att√©nuer certaines des complexit√©s du traitement par lots car, pour certains pipelines, un seul √©l√©ment (comme un long fichier audio) doit √™tre divis√© en plusieurs parties pour √™tre trait√© par un mod√®le. Le pipeline effectue ce [*traitement par lots de morceaux*](./main_classes/pipelines#pipeline-chunk-batching) pour vous.

### Param√®tres sp√©cifiques √† une t√¢che

Toutes les t√¢ches ont des param√®tres sp√©cifiques qui permettent une flexibilit√© et des options suppl√©mentaires pour vous aider √† compl√©ter cette t√¢che.
Par exemple, la m√©thode [`transformers.AutomaticSpeechRecognitionPipeline.__call__`] a un param√®tre `return_timestamps` qui semble utile pour sous-titrer des vid√©os :

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2", return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]}
```

Comme vous pouvez le voir, le mod√®le a d√©duit le texte et a √©galement indiqu√© **quand** les diff√©rentes phrases ont √©t√© prononc√©es.

Il y a beaucoup de param√®tres disponibles pour chaque t√¢che, donc consultez la documentation de l'API de chaque t√¢che pour voir ce que vous pouvez param√©trer !
Par exemple, le pipeline [`~transformers.AutomaticSpeechRecognitionPipeline`] a un param√®tre `chunk_length_s` qui est utile quand on travaille sur des fichiers audio tr√®s longs (par exemple, sous-titrage de films entiers ou de vid√©os d'une heure) qu'un mod√®le ne peut g√©n√©ralement pas g√©rer seul :

```python
>>> transcriber = pipeline(model="openai/whisper-large-v2", chunk_length_s=30, return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/sanchit-gandhi/librispeech_long/resolve/main/audio.wav")
{'text': " Chapter 16. I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came.  I, too, agree to whatever Marguerite wished, Marguerite to be unable to live apart from me. It was the day after the evening...}
```

Si vous ne trouvez pas un param√®tre qui serait vraiment utile, n'h√©sitez pas √† [le demander](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml) !

## Utilisation des pipelines sur un ensemble de donn√©es

Le pipeline peut faire de l'inf√©rence sur un grand ensemble de donn√©es. La fa√ßon la plus simple de le faire est d'utiliser un it√©rateur :

```py
def data():
    for i in range(1000):
        yield f"My example {i}"


pipe = pipeline(model="gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out[0]["generated_text"])
```

L'it√©rateur `data()` renvoie chaque r√©sultat, et le pipeline reconna√Æt automatiquement que les donn√©es d'entr√©e sont it√©rables et commencera √† r√©cup√©rer les donn√©es tout en continuant √† les traiter sur le GPU (cela utilise un [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) en interne). Ceci est important car vous n'avez pas √† allouer de m√©moire pour l'ensemble du jeu de donn√©es et vous pouvez passer les donn√©es au GPU aussi rapidement que possible.

Comme le traitement par lots peut acc√©l√©rer les choses, il peut √™tre utile d'essayer de r√©gler le param√®tre `batch_size` ici.

La mani√®re la plus simple d'it√©rer sur un ensemble de donn√©es est de simplement d'en charger un depuis ü§ó [Datasets](https://github.com/huggingface/datasets/):

```py
# KeyDataset is a util that will just output the item we're interested in.
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

## Utilisation des pipelines pour un serveur web

<Tip>
La cr√©ation d'un moteur d'inf√©rence est un sujet complexe qui m√©rite sa propre page de documentation.
</Tip>

[Link](./pipeline_webserver)

## Pipeline visuel

Utiliser un [`pipeline`] pour les t√¢ches de vision est quasiment identique.

Specifiez votre t√¢che et passez votre image au classifieur. L'image peut √™tre un lien, un chemin local ou une image encod√©e en base64. Par exemple, quelle esp√®ce de chat est montr√©e ci-dessous ?

![pipeline-cat-chonk](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg)

```py
>>> from transformers import pipeline

>>> vision_classifier = pipeline(model="google/vit-base-patch16-224")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

## Pipeline textuel

Utiliser un [`pipeline`] pour les t√¢ches de traitement du langage est quasiment identique.

```py
>>> from transformers import pipeline

>>> # This model is a `zero-shot-classification` model.
>>> # It will classify text, except you are free to choose any label you might imagine
>>> classifier = pipeline(model="facebook/bart-large-mnli")
>>> classifier(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}
```

## Pipeline multimodale

Le [`pipeline`] supporte plus d'une modalit√©. Par exemple, une t√¢che de question-r√©ponse visuelle combine texte et image. N'h√©sitez pas √† utiliser n'importe quel lien d'image que vous aimez et une question que vous voulez poser sur l'image. L'image peut √™tre un URL ou un emplacement de fichier local.

Par exemple, si vous utilisez cette [image de facture](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png) :

```py
>>> from transformers import pipeline

>>> vqa = pipeline(model="impira/layoutlm-document-qa")
>>> vqa(
...     image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
...     question="What is the invoice number?",
... )
[{'score': 0.42515, 'answer': 'us-001', 'start': 16, 'end': 16}]
```

<Tip>

Pour ex√©cuter l'exemple ci-dessus, [`pytesseract`](https://pypi.org/project/pytesseract/) doit √™tre install√© en plus de ü§ó Transformers :

```bash
sudo apt install -y tesseract-ocr
pip install pytesseract
```

</Tip>

## Utiliser un `pipeline` avec de grands mod√®les avec ü§ó `accelerate`

Vous pouvez facilement utiliser `pipeline` avec de grands mod√®les en utilisant ü§ó `accelerate` ! Assurez-vous d'abord d'avoir install√© `accelerate` avec `pip install accelerate`.

Commencez par charger votre mod√®le en utilisant `device_map="auto"` ! Nous utilisons `facebook/opt-1.3b` pour cette exemple.

```py
# pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

Vous pouvez √©galement charger des mod√®les en 8 bits si vous installez `bitsandbytes` et ajoutez l'argument `load_in_8bit=True`

```py
# pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

Notez que vous pouvez remplacer l'ensemble de poids ("checkpoint" en anglais) par n'importe quel mod√®le Hugging Face qui prend en charge le chargement de grands mod√®les tels que BLOOM !
