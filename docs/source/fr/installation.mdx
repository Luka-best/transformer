<!---
Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Installation

Installez ü§ó Transformers pour la librairie d'apprentissage profond avec laquelle vous travaillez, configurer votre cache et configurer ü§ó Transformers pour un usage hors ligne (facultatif). 

ü§ó Transformers est test√© avec Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+ et Flax.
Consulter les instructions d'installation ci-dessous pour la librairie d'apprentissage profond que vous utilisez:

* Instructions d'installation pour [PyTorch](https://pytorch.org/get-started/locally/).
* Instructions d'installation pour [TensorFlow 2.0](https://www.tensorflow.org/install/pip).
* Instructions d'installation pour [Flax](https://flax.readthedocs.io/en/latest/).

## Installation avec pip

Vous devriez installer ü§ó Transformers dans un [environnement virtuel](https://docs.python.org/3/library/venv.html).
Si vous n'√™tes pas √† l'aise avec les environnement virtuels, consultez ce [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).
Utiliser un environnement virtuel permet de facilement g√©rer diff√©rent projets et d'√©viter des erreurs de compatibilit√© entre les diff√©rentes d√©pendances.

Commencez par cr√©er un environnement virtuel dans l'espace de travail de votre projet :

```bash
python -m venv .env
```

Activez l'envorinnement virtuel. Sur Linux ou MacOs :

```bash
source .env/bin/activate
```
Activez l'envorinnement virtuel sur Windows :

```bash
.env/Scripts/activate
```

Maintenant, ü§ó Transformers peut √™tre install√© avec la commande suivante:

```bash
pip install transformers
```

Pour une utilisation avec CPU seulement, ü§ó Transformers et la librairie d'apprentissage profond de votre choix peuvent √™tre install√©s en seule ligne.
Par exemple, installez ü§ó Transformers et PyTorch avec la commande suivante :

```bash
pip install 'transformers[torch]'
```

ü§ó Transformers et TensorFlow 2.0 :

```bash
pip install 'transformers[tf-cpu]'
```

<Tip warning={true}>

Utilisateurs avec M1 / ARM

Vous devez installer les outils suivant avant d'installer TensorFLow 2.0 

```
brew install cmake
brew install pkg-config
```

</Tip>

ü§ó Transformers et Flax :

```bash
pip install 'transformers[flax]'
```

V√©rifiez si ü§ó Transformers a bien √©t√© install√© avec la commande suivante. La commande va t√©l√©charger un mod√®le pr√©-entra√Æn√© :

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

Le label et score est ensuite affich√© :

```bash
[{'label': 'POSITIVE', 'score': 0.9998704791069031}]
```

## Installation depuis le code source

Installez ü§ó Transformers depuis le code source avec la commande suivante :

```bash
pip install git+https://github.com/huggingface/transformers
```

Cette commande installe la version depuis la branche `main` au lieu de la derni√®re version stable.
La version de la branche `main` est utilie pour avoir les derniers d√©veloppements. 
Par exemple, si un bug a √©t√© r√©solu depuis la derni√®re version stable mais n'a pas encore √©t√© publi√© officiellement.
Cependant, cela veut aussi dire que la version de la branche `main` n'est pas toujours stable.
Nous nous effor√ßons de maintenir la version de la branche `main` op√©rationnelle, et la plupart des probl√®mes sont g√©n√©ralement r√©solus en l'espace de quelques heures ou d'un jour.
Si vous recontrez un probl√®me, n'h√©sitez pas √† cr√©er une [Issue](https://github.com/huggingface/transformers/issues) pour que l'on puisse trouver une solution au plus vite ! 

V√©rifiez si ü§ó Transformers a bien √©t√© install√© avec la commande suivante :

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
```

## Installation modifiable

Vous aurez besoin d'une installation modifiable si vous le souhaitez :

* Utiliser la version de la branche `main` du code source.
* Contribuer √† ü§ó Transformers et vouler tester vos modifications du code source.

Clonez le projet et installez ü§ó Transformers avec les commandes suivantes :

```bash
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .
```

Ces commandes cr√©ent des liens entre le dossier o√π le projet a √©t√© clon√© et les chemins de vos librairies Python.
Python regardera maintenant dans le dossier que vous avez clon√© en plus des chemins de vos librairies normaux.
Par exemple, si vos librairies Python sont install√©es dans `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python cherchera aussi dans le dossier o√π vous avez clon√© : `~/transformers/`.

<Tip warning={true}>

Vous devez garder le dossier `transformers` si vous voulez continuer d'utiliser la librairie.

</Tip>

Maintenant, vous pouvez facilement mettre √† jour votre clone avec la derni√®re version de ü§ó Transformers en utilisant la commande suivante :

```bash
cd ~/transformers/
git pull
```

Votre environnement Python trouvera la version de la branche `main` lors de la prochaine ex√©cution.

## Installation avec conda

Installation avec la cha√Æne `huggingface` de conda :

```bash
conda install -c huggingface transformers
```

## Configuration du cache

Pretrained models are downloaded and locally cached at: `~/.cache/huggingface/hub`. This is the default directory given by the shell environment variable `TRANSFORMERS_CACHE`. On Windows, the default directory is given by `C:\Users\username\.cache\huggingface\hub`. You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:

1. Shell environment variable (default): `HUGGINGFACE_HUB_CACHE` or `TRANSFORMERS_CACHE`.
2. Shell environment variable: `HF_HOME`.
3. Shell environment variable: `XDG_CACHE_HOME` + `/huggingface`.

<Tip>

ü§ó Transformers will use the shell environment variables `PYTORCH_TRANSFORMERS_CACHE` or `PYTORCH_PRETRAINED_BERT_CACHE` if you are coming from an earlier iteration of this library and have set those environment variables, unless you specify the shell environment variable `TRANSFORMERS_CACHE`.

</Tip>

## Mode hors ligne

ü§ó Transformers is able to run in a firewalled or offline environment by only using local files. Set the environment variable `TRANSFORMERS_OFFLINE=1` to enable this behavior.

<Tip>

Add [ü§ó Datasets](https://huggingface.co/docs/datasets/) to your offline training workflow by setting the environment variable `HF_DATASETS_OFFLINE=1`.

</Tip>

For example, you would typically run a program on a normal network firewalled to external instances with the following command:

```bash
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

Run this same program in an offline instance with:

```bash
HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

The script should now run without hanging or waiting to timeout because it knows it should only look for local files.

### Fetch models and tokenizers to use offline

Another option for using ü§ó Transformers offline is to download the files ahead of time, and then point to their local path when you need to use them offline. There are three ways to do this:

* Download a file through the user interface on the [Model Hub](https://huggingface.co/models) by clicking on the ‚Üì icon.

    ![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png)

* Use the [`PreTrainedModel.from_pretrained`] and [`PreTrainedModel.save_pretrained`] workflow:

    1. Download your files ahead of time with [`PreTrainedModel.from_pretrained`]:

    ```py
    >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

    >>> tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
    >>> model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")
    ```

    2. Save your files to a specified directory with [`PreTrainedModel.save_pretrained`]:

    ```py
    >>> tokenizer.save_pretrained("./your/path/bigscience_t0")
    >>> model.save_pretrained("./your/path/bigscience_t0")
    ```

    3. Now when you're offline, reload your files with [`PreTrainedModel.from_pretrained`] from the specified directory:

    ```py
    >>> tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
    >>> model = AutoModel.from_pretrained("./your/path/bigscience_t0")
    ```

* Programmatically download files with the [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub) library:

    1. Install the `huggingface_hub` library in your virtual environment:

    ```bash
    python -m pip install huggingface_hub
    ```

    2. Use the [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) function to download a file to a specific path. For example, the following command downloads the `config.json` file from the [T0](https://huggingface.co/bigscience/T0_3B) model to your desired path:

    ```py
    >>> from huggingface_hub import hf_hub_download

    >>> hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
    ```

Once your file is downloaded and locally cached, specify it's local path to load and use it:

```py
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

<Tip>

See the [How to download files from the Hub](https://huggingface.co/docs/hub/how-to-downstream) section for more details on downloading files stored on the Hub.

</Tip>
