<!--Copyright 2023 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Carga los adaptadores con ü§ó PEFT

[[open-in-colab]]

[Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft) m√©todos congelan los par√°metros del modelo preentrenado durante el punto fino y agregan un peque√±o n√∫mero de par√°metros entrenables (los adaptadores) encima de eso.  Los par√°metros (los adaptadores) est√°n entrenados para aprender la informaci√≥n tarea espec√≠fica.  Este enfoque a sido comprobado en ser muy eficiente en memoria con proceso inferior mientras produciendo resultados comparable a un modelo completamente afinado. 

 Adaptadores entrenados con PEFT tambi√©n son usualmente un orden de magnitud m√°s peque√±os que los modelos enteros, haci√©ndolos m√°s convenientes para compartir, archivar, y cargar. 

<div class="flex flex-col justify-center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png"/>
  <figcaption class="text-center">Los pesos del adaptero para un modelo de OPTForCausalLM guardado en el Hub son solo ~6MB en comparaci√¥n al tama√±o completo de los  pesos de modelo, cuales sean ~700MB.</figcaption>
</div>

Si est√°s interesado en aprender m√°s sobre la librer√≠a de PEFT, mira la [documentaci√≥n](https://huggingface.co/docs/peft/index).

## Configuraci√≥n

Empezar por instalar ü§ó PEFT:

```bash
pip install peft
```

Si quieres tratar las nuevas caracter√≠sticas, instala la librer√≠a de la fuente:

```bash
pip install git+https://github.com/huggingface/peft.git
```

##  Los modelos de PEFT apoyados

Los ü§ó Transformers nativamente apoyan algunos m√©todos de PEFT, que significa puedes cargar los pesos del adaptador archivados localmente o archivados en el Hub y f√°cilmente ejecutar o entrenar los pesos con unas cuantas l√≠neas de c√≥digo.  Los siguientes m√©todos est√°n apoyados:

- [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)
- [AdaLoRA](https://arxiv.org/abs/2303.10512)

Si quieres usar otros m√©todos de PEFT como el aprendizaje de los prompts o afinamientos de los prompts o de la librer√≠a de ü§ó PEFT en general, por favor refiere a la [documentaci√≥n](https://huggingface.co/docs/peft/index).


## Cargar un adaptador de PEFT

Para cargar y usar un modelo adaptador de PEFT desde ü§ó Transformers, asegura que el Hub repositorio o el directorio local contiene un `adapter_config.json` archivo y pesas de adaptadores como presentado en el ejemplo de imagen de arriba. Despu√©s puedes cargar el modelo adaptador de PEFT usando la clase de `AutoModelFor`. Por ejemplo, para cargar el modelo adaptador de PEFT para el lenguaje casual en modelar:

1. espec√≠fica el ID del modelo de PEFT
2. p√°salo a la clase de [`AutoModelForCausalLM`]

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

<Tip>

Puedes cargar al PEFT adaptador con tampoco una clase de `AutoModelFor` o la clase del modelo base como `OPTForCausalLM` o `LlamaForCausalLM`.

</Tip>

Tamb√≠en puedes cargar un adaptador de PEFT llamando el m√©todo de `load_adapter`:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

## Cargar en 8bit o 4bit

La integraci√≥n de `bitsandbytes` apoya los tipos de datos precisos que son √∫tiles para cargar modelos grandes porque
 guarda memoria (mira la [guia](https://huggingface.co/docs/transformers/quantization#bitsandbyes) de `bitsandbytes` para aprender mas). Agrega el parametro `load_in_8bit` o el parametro `load_in_4bit` al [`~PreTrainedModel.from_pretrained`] y coloca `device_map="auto"` para effectivamente distribuir el modelo en tu hardware:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, device_map="auto", load_in_8bit=True)
```

## Agrega un nuevo adaptador

Puedes usar [`~peft.PeftModel.add_adapter`] para agregar un nuevo adaptador a un modelo con un existente adaptador con tal que
 el nuevo sea el mismo tipo que el adaptador actual. Por ejemplo si tienes un existente LoRA adaptador connectado a un modelo:

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import LoraConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id),
lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```

Para agregar un nuevo adaptador:

```py
# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")
```

Ahora puedes usar [`~peft.PeftModel.set_adapter`] para configurar cu√°l adaptador para usar:

```py
# use adapter_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# use adapter_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

## Para activar y desactivar los adaptadores

Cuando has agregado un adaptador a un modelo, puedes activar or desactivar el m√≥dulo de adaptador. Para activar el m√≥dulo de adaptador:

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# to initiate with random weights
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
```

Para desactivar el modulo adaptero:

```py
model.disable_adapters()
output = model.generate(**inputs)
```

## Entrenar un adaptor de PEFT

Los adaptadores de PEFT est√°n apoyados por la clase de [`Trainer`] para que puedas entrenar el adaptador para tu caso de uso espec√≠fico. S√≥lo requiere agregar unas l√≠neas m√°s de c√≥digo.  Por ejemplo, para entrenar un adaptador de LoRA:  

<Tip>

Si no est√°s familiarizado con el afinamiento de un modelo con la clase[`Trainer`], mira el tutorial [Fine-tune a pretrained model](training).

</Tip>

1. Define tu configuraci√¥n de adaptador con el tipo de tarea y hiperpar√°metros (ves [`~peft.LoraConfig`] para m√°s detalles de lo que
 hacen los hiperpar√°metros).

```py
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
```

2. Agrega el adaptador al modelo.

```py
model.add_adapter(peft_config)
```

3. ¬°Ahora puedes pasar el modelo a [`Trainer`]!

```py
trainer = Trainer(model=model, ...)
trainer.train()
```

Para archivar tu adaptador entrenado y volver a cargarlo:

```py
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

## Agrega capas entrenables adicionales a un PEFT adaptador

Tambien puedes afinar adaptadores entrenables adicionales en encima de un modelo que tiene adaptadores connectados por pasar a `modules_to_save` en tu config de PEFT. Por ejemplo, si tu quieres tambi√©n afinar el lm_head encima de un modelo con un adaptador de LoRA:

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import LoraConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    modules_to_save=["lm_head"],
)

model.add_adapter(lora_config)
```


<!--
TODO: (@younesbelkada @stevhliu)
-   Link to PEFT docs for further details
-   Trainer  
-   8-bit / 4-bit examples ?
-->
