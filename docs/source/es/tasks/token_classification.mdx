<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Clasificaci칩n de tokens

<Youtube id="wVHdVlPScxA"/>

La clasificaci칩n de tokens asigna una etiqueta a los tokens individuales de una frase. Una de las tareas m치s comunes de clasificaci칩n de tokens es el Reconocimiento de Entidades Nombradas (en ingl칠s, NER). El NER intenta encontrar una etiqueta para cada entidad en una frase, como persona, ubicaci칩n, u organizaci칩n.

Esta gu칤a te mostrar치 c칩mo afinar el modelo (en ingl칠s, finetunning) [DistilBERT](https://huggingface.co/distilbert-base-uncased) con el conjunto de datos [WNUT 17](https://huggingface.co/datasets/wnut_17) para detectar nuevas entidades.

<Tip>

Consulta la [p치gina de la tarea] de clasificaci칩n de tokens (https://huggingface.co/tasks/token-classification) para obtener m치s informaci칩n sobre otras formas de clasificaci칩n de tokens y sus modelos, conjuntos de datos y m칠tricas asociadas.

</Tip>

## Cargar el conjunto de datos WNUT 17

Cargue el conjunto de datos WNUT 17 de la biblioteca 游뱅 Datasets :

```py
>>> from datasets import load_dataset

>>> wnut = load_dataset("wnut_17")
```

Entonces, observa un ejemplo:

```py
>>> wnut["train"][0]
{'id': '0',
 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],
 'tokens': ['@paulwalk', 'It', "'s", 'the', 'view', 'from', 'where', 'I', "'m", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']
}
```

Cada n칰mero en `ner_tags` representa una entidad. Convierte el n칰mero en un nombre de etiqueta para obtener m치s informaci칩n:

```py
>>> label_list = wnut["train"].features[f"ner_tags"].feature.names
>>> label_list
[
    "O",
    "B-corporation",
    "I-corporation",
    "B-creative-work",
    "I-creative-work",
    "B-group",
    "I-group",
    "B-location",
    "I-location",
    "B-person",
    "I-person",
    "B-product",
    "I-product",
]
```

La etiqueta `ner_tag` describe una entidad, como una empresa, un lugar, o una persona. La letra que precede a cada `ner_tag` indica la posici칩n del token en la entidad:

- La letra `B-` indica el comienzo de la entidad.
- La letra `I-` indica que un token est치 contenido dentro de la misma entidad (por ejemplo, el token `State` es parte de la entidad `Empire State Building`).
- La letra `O`  indica que el token no corresponde a ninguna entidad.

## Preprocesamiento

<Youtube id="iY2AZYdZAr0"/>

Carga el tokenizador de DistilBERT para procesar los `tokens`:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
```

Dado que la entrada ya ha sido dividida en palabras, establece `is_split_into_words=True` para tokenizar las palabras en subpalabras:

```py
>>> tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
>>> tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
>>> tokens
['[CLS]', '@', 'paul', '##walk', 'it', "'", 's', 'the', 'view', 'from', 'where', 'i', "'", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']
```

La incorporaci칩n de los tokens especiales `[CLS]` y `[SEP]` y la tokenizaci칩n de subpalabras crea un desajuste entre la entrada y las etiquetas. Una sola palabra que corresponde a una sola etiqueta puede estar dividida en dos subpalabras. Tendr치s que volver a alinear los tokens y las etiquetas:

1. Asignar todos los tokens a la palabra correspondiente con el m칠todo [`word_ids`](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.Encoding.word_ids).
2. Asignar la etiqueta `-100` a los tokens especiales `[CLS]` y `[SEP]` para que la funci칩n de perdida de PyTorch los ignore.
3. Etiquetar s칩lo el primer token de una palabra dada. Asignar `-100` a otros subtokens de la misma palabra.

As칤 es como se puede crear una funci칩n para realinear los tokens y las etiquetas, y truncar las secuencias para que no sean m치s largas que la longitud m치xima de entrada de DistilBERT::

```py
>>> def tokenize_and_align_labels(examples):
...     tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

...     labels = []
...     for i, label in enumerate(examples[f"ner_tags"]):
...         word_ids = tokenized_inputs.word_ids(batch_index=i)  # Asigna los tokens a la palabra correspondiente.
...         previous_word_idx = None
...         label_ids = []
...         for word_idx in word_ids:  # Asigna -100 a los tokens especiales.
...             if word_idx is None:
...                 label_ids.append(-100)
...             elif word_idx != previous_word_idx:  # Etiqueta solo el primer token de una palabra determinada.
...                 label_ids.append(label[word_idx])
...             else:
...                 label_ids.append(-100)
...             previous_word_idx = word_idx
...         labels.append(label_ids)

...     tokenized_inputs["labels"] = labels
...     return tokenized_inputs
```

Utiliza la funci칩n 游뱅 Datasets [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) para tokenizar y alinear las etiquetas en todo el conjunto de datos. Puedes acelerar la funci칩n `map` estableciendo `batched=True` para procesar varios elementos del conjunto de datos a la vez:

```py
>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)
```

Utiliza [`DataCollatorForTokenClassification`] para crear un conjunto de ejemplos. Tambi칠n *rellenar치 din치micamente* el texto y las etiquetas a la longitud del elemento m치s largo del conjunto, para que tengan una longitud uniforme. Aunque es posible rellenar el texto con la funci칩n `tokenizer` estableciendo `padding=True`, el relleno din치mico es m치s eficiente.

<frameworkcontent>
<pt>
```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```
</pt>
<tf>
```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors="tf")
```
</tf>
</frameworkcontent>

## Entrenamiento

<frameworkcontent>
<pt>
Carga el modelo DistilBERT con [`AutoModelForTokenClassification`] junto con el n칰mero de etiquetas previstas:

```py
>>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased", num_labels=14)
```

<Tip>

Si no estas familiarizado con el ajuste fino de un modelo con el [`Trainer`], da un vistazo al tutorial b치sico [aqu칤](../training#finetune-with-trainer)!

</Tip>

En este punto, solo quedan tres pasos:

1. Definir los hiperpar치metros del entrenamiento en [`TrainingArguments`].
2. Pasar los par치metros de entrenamiento a [`Trainer`] junto con el modelo, el conjunto de datos, el tokenizador y el lote de ejemplos.
3. Llamar [`~Trainer.train`] para ajustar el modelo.

```py
>>> training_args = TrainingArguments(
...     output_dir="./results",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=3,
...     weight_decay=0.01,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_wnut["train"],
...     eval_dataset=tokenized_wnut["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
... )

>>> trainer.train()
```
</pt>
<tf>
Para ajustar un modelo en TensorFlow, empieza por convertir tus conjuntos de datos al formato `tf.data.Dataset` con [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset). Especifica las entradas y las etiquetas en `columns`, si deseas mezclar el orden del conjunto de datos, el tama침o del lote y el lote de ejemplos:

```py
>>> tf_train_set = tokenized_wnut["train"].to_tf_dataset(
...     columns=["attention_mask", "input_ids", "labels"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_validation_set = tokenized_wnut["validation"].to_tf_dataset(
...     columns=["attention_mask", "input_ids", "labels"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

<Tip>

Si no estas familiarizado con el ajuste fino de un modelo con Keras, da un vistazo al tutorial b치sico [aqu칤](training#finetune-with-keras)!

</Tip>

Configura una funci칩n de optimizaci칩n, un programa de tasa de aprendizaje y algunos hiperpar치metros de entrenamiento:

```py
>>> from transformers import create_optimizer

>>> batch_size = 16
>>> num_train_epochs = 3
>>> num_train_steps = (len(tokenized_wnut["train"]) // batch_size) * num_train_epochs
>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=2e-5,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=0.01,
...     num_warmup_steps=0,
... )
```

Carga DistilBERT con [`TFAutoModelForTokenClassification`] junto con el n칰mero de etiquetas previstas:

```py
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
```

Configura el modelo para el entrenamiento con [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)
```

Llame a [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) para afinar el modelo:

```py
>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)
```
</tf>
</frameworkcontent>

<Tip>

Para ver un ejemplo m치s detallado de c칩mo afinar un modelo para la clasificaci칩n de tokens, da un vistazo
a [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)
o [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).

</Tip>