<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Generaci칩n de res칰menes

<Youtube id="yHnr5Dk2zCI"/>

La generaci칩n de res칰menes (summarization, en ingl칠s) crea una versi칩n m치s corta de un documento o un art칤culo que resume toda su informaci칩n importante. Junto con la traducci칩n, es un ejemplo de una tarea que puede ser formulada como una tarea secuencia a secuencia. La generaci칩n de res칰menes puede ser:

- Extractiva: Extrae la informaci칩n m치s relevante de un documento.
- Abstractiva: Genera un texto nuevo que captura la informaci칩n m치s importante.

Esta gu칤a te mostrar치 c칩mo puedes hacer fine-tuning del modelo [T5](https://huggingface.co/t5-small) sobre el subset de proyectos de ley del estado de California, dentro del dataset [BillSum](https://huggingface.co/datasets/billsum) para hacer generaci칩n de res칰menes abstractiva.

<Tip>

Consulta la [p치gina de la tarea](https://huggingface.co/tasks/summarization) de generaci칩n de res칰menes para obtener m치s informaci칩n sobre sus modelos, datasets y m칠tricas asociadas.

</Tip>

## Carga el dataset BillSum

Carga el dataset BillSum de la biblioteca 游뱅 Datasets:

``````

Divide el dataset en un set de train y un set de test:

``````

A continuaci칩n, observa un ejemplo:

```py
```

El campo `text` es el input y el campo `summary` es el objetivo.

## Preprocesa

Carga el tokenizador T5 para procesar `text` y `summary`:

``````

La funci칩n de preprocesamiento necesita:

1. Agregar un prefijo al input; una clave para que T5 sepa que se trata de una tarea de generaci칩n de res칰menes. Algunos modelos capaces de realizar m칰ltiples tareas de NLP requieren una clave que indique la tarea espec칤fica.
2. Usar el argumento `text_target` para tokenizar etiquetas.
3. Truncar secuencias para que no sean m치s largas que la longitud m치xima fijada por el par치metro `max_length`.

```py
>>> prefix = "summarize: "


>>> def preprocess_function(examples):
...     inputs = [prefix + doc for doc in examples["text"]]
...     model_inputs = tokenizer(inputs, max_length=1024, truncation=True)

```

Usa la funci칩n [`~datasets.Dataset.map`] de 游뱅 Datasets para aplicar la funci칩n de preprocesamiento sobre el dataset en su totalidad. Puedes acelerar la funci칩n `map` configurando el argumento `batched=True` para procesar m칰ltiples elementos del dataset a la vez:

``````

Usa [`DataCollatorForSeq2Seq`] para crear un lote de ejemplos. Esto tambi칠n *rellenar치 din치micamente* tu texto y etiquetas a la dimensi칩n del elemento m치s largo del lote para que tengan un largo uniforme. Si bien es posible rellenar tu texto en la funci칩n `tokenizer` mediante el argumento `padding=True`, el rellenado din치mico es m치s eficiente.

<frameworkcontent>
<pt>
``````
</pt>
<tf>
``````
</tf>
</frameworkcontent>

## Entrenamiento

<frameworkcontent>
<pt>
Carga T5 con [`AutoModelForSeq2SeqLM`]:

``````

<Tip>

Para familiarizarte con el proceso para realizar fine-tuning sobre un modelo con [`Trainer`], 춰mira el tutorial b치sico [aqu칤](../training#finetune-with-trainer)!

</Tip>

En este punto, solo faltan tres pasos:

1. Definir tus hiperpar치metros de entrenamiento en [`Seq2SeqTrainingArguments`].
2. Pasarle los argumentos de entrenamiento a [`Seq2SeqTrainer`] junto con el modelo, dataset y data collator.
3. Llamar [`~Trainer.train`] para realizar el fine-tuning sobre tu modelo.

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="./results",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=1,
...     fp16=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_billsum["train"],
...     eval_dataset=tokenized_billsum["test"],
...     tokenizer=tokenizer,
```
</pt>
<tf>
Para hacer fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato `tf.data.Dataset` con [`~datasets.Dataset.to_tf_dataset`]. Especifica los inputs y etiquetas en `columns`, el tama침o de lote, el data collator, y si es necesario mezclar el dataset:

```py
>>> tf_train_set = tokenized_billsum["train"].to_tf_dataset(
...     columns=["attention_mask", "input_ids", "labels"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = tokenized_billsum["test"].to_tf_dataset(
...     columns=["attention_mask", "input_ids", "labels"],
```

<Tip>

Para familiarizarte con el fine-tuning con Keras, 춰mira el tutorial b치sico [aqu칤](training#finetune-with-keras)!

</Tip>

Crea la funci칩n optimizadora, establece la tasa de aprendizaje y algunos hiperpar치metros de entrenamiento:

``````

Carga T5 con [`TFAutoModelForSeq2SeqLM`]:

``````

Configura el modelo para entrenamiento con [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):

``````

Llama a [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) para realizar el fine-tuning del modelo:

``````
</tf>
</frameworkcontent>

<Tip>

Para un ejemplo con mayor profundidad de c칩mo hacer fine-tuning a un modelo para generaci칩n de res칰menes, revisa la
[notebook en PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)
o a la [notebook en TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).

</Tip>