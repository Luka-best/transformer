<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Fine-tuning a un modelo pre-entrenado

[[open-in-colab]]

El uso de un modelo pre-entrenado tiene importantes ventajas. Reduce los costos de computaci√≥n, la huella de carbono y te permite utilizar modelos de √∫ltima generaci√≥n sin tener que entrenar uno desde cero.

* Fine-tuning a un modelo pre-entrenado con ü§ó Transformers [`Trainer`].
* Fine-tuning a un modelo pre-entrenado en TensorFlow con Keras.
* Fine-tuning a un modelo pre-entrenado en PyTorch nativo.

<a id='data-processing'></a>

## Prepara un dataset

<Youtube id="_BZearw7f0w"/>

Antes de aplicar fine-tuning a un modelo pre-entrenado, descarga un dataset y prep√°ralo para el entrenamiento. El tutorial anterior nos ense√±√≥ c√≥mo procesar los datos para el entrenamiento, y ahora es la oportunidad de poner a prueba estas habilidades.

Comienza cargando el dataset de [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full):

```py
>>> from datasets import load_dataset

```

Como ya sabes, necesitas un tokenizador para procesar el texto e incluir una estrategia para el padding y el truncamiento para manejar cualquier longitud de secuencia variable. Para procesar tu dataset en un solo paso, utiliza el m√©todo de ü§ó Datasets¬†mappara aplicar una funci√≥n de preprocesamiento sobre todo el dataset:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


>>> def tokenize_function(examples):
```

Si lo deseas, puedes crear un subconjunto m√°s peque√±o del dataset completo para aplicarle fine-tuning y as√≠ reducir el tiempo.

``````

<a id='trainer'></a>

## Fine-tuning con `Trainer`

<Youtube id="nvBXf7s7vTI"/>

ü§ó Transformers proporciona una clase [`Trainer`] optimizada para el entrenamiento de modelos de ü§ó Transformers, haciendo m√°s f√°cil el inicio del entrenamiento sin necesidad de escribir manualmente tu propio ciclo. La API del [`Trainer`] soporta una amplia gama de opciones de entrenamiento y caracter√≠sticas como el logging, el gradient accumulation y el mixed precision.

Comienza cargando tu modelo y especifica el n√∫mero de labels previstas. A partir del [Card Dataset](https://huggingface.co/datasets/yelp_review_full#data-fields) de Yelp Review, que como ya sabemos tiene 5 labels:

``````

<Tip>

Ver√°s una advertencia acerca de que algunos de los pesos pre-entrenados no est√°n siendo utilizados y que algunos pesos est√°n siendo inicializados al azar. No te preocupes, esto es completamente normal.
No te preocupes, esto es completamente normal. El head/cabezal pre-entrenado del modelo BERT se descarta y se sustituye por un head de clasificaci√≥n inicializado aleatoriamente. Puedes aplicar fine-tuning a este nuevo head del modelo en tu tarea de clasificaci√≥n de secuencias haciendo transfer learning del modelo pre-entrenado.

</Tip>

### Hiperpar√°metros de entrenamiento

A continuaci√≥n, crea una clase [`TrainingArguments`] que contenga todos los hiperpar√°metros que puedes ajustar as√≠ como los indicadores para activar las diferentes opciones de entrenamiento. Para este tutorial puedes empezar con los [hiperpar√°metros](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) de entrenamiento por defecto, pero si√©ntete libre de experimentar con ellos para encontrar tu configuraci√≥n √≥ptima.

Especifica d√≥nde vas a guardar los checkpoints de tu entrenamiento:

``````

### M√©tricas

El [`Trainer`] no eval√∫a autom√°ticamente el rendimiento del modelo durante el entrenamiento. Tendr√°s que pasarle a [`Trainer`] una funci√≥n para calcular y hacer un reporte de las m√©tricas. La biblioteca de ü§ó Datasets proporciona una funci√≥n de [`accuracy`](https://huggingface.co/metrics/accuracy) simple que puedes cargar con la funci√≥n `load_metric` (ver este [tutorial](https://huggingface.co/docs/datasets/metrics.html) para m√°s informaci√≥n):

```py
```

Define la funci√≥n `compute` en `metric` para calcular el accuracy de tus predicciones. Antes de pasar tus predicciones a `compute`, necesitas convertir las predicciones a logits (recuerda que todos los modelos de ü§ó Transformers devuelven logits).

```py
```

Si quieres controlar tus m√©tricas de evaluaci√≥n durante el fine-tuning, especifica el par√°metro `evaluation_strategy` en tus argumentos de entrenamiento para que el modelo tenga en cuenta la m√©trica de evaluaci√≥n al final de cada √©poca:

``````

### Trainer

Crea un objeto [`Trainer`] con tu modelo, argumentos de entrenamiento, datasets de entrenamiento y de prueba, y tu funci√≥n de evaluaci√≥n:

```py
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
```

A continuaci√≥n, aplica fine-tuning a tu modelo llamando [`~transformers.Trainer.train`]:

``````

<a id='keras'></a>

## Fine-tuning con Keras

<Youtube id="rnTGBy2ax1c"/>

Los modelos de ü§ó Transformers tambi√©n permiten realizar el entrenamiento en TensorFlow con la API de Keras. Solo es necesario hacer algunos cambios antes de hacer fine-tuning.

### Convierte el dataset al formato de TensorFlow

El [`DefaultDataCollator`] junta los tensores en un batch para que el modelo se entrene en √©l. Aseg√∫rate de especificar `return_tensors` para devolver los tensores de TensorFlow:

``````

<Tip>

[`Trainer`] utiliza [`DataCollatorWithPadding`] por defecto por lo que no es necesario especificar expl√≠citamente un intercalador de datos (data collator, en ingl√©s).

</Tip>

A continuaci√≥n, convierte los datasets tokenizados en datasets de TensorFlow con el m√©todo [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset). Especifica tus entradas en `columns` y tu etiqueta en `label_cols`:

```py
>>> tf_train_dataset = small_train_dataset.to_tf_dataset(
...     columns=["attention_mask", "input_ids", "token_type_ids"],
...     label_cols=["labels"],
...     shuffle=True,
...     collate_fn=data_collator,
...     batch_size=8,
... )

>>> tf_validation_dataset = small_eval_dataset.to_tf_dataset(
...     columns=["attention_mask", "input_ids", "token_type_ids"],
...     label_cols=["labels"],
```

### Compila y ajusta

Carguemos un modelo TensorFlow con el n√∫mero esperado de labels:

```py
```

A continuaci√≥n, compila y aplica fine-tuning a tu modelo con [`fit`](https://keras.io/api/models/model_training_apis/) como lo har√≠as con cualquier otro modelo de Keras:

```py
>>> model.compile(
...     optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
...     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
```

<a id='pytorch_native'></a>

## Fine-tune en PyTorch nativo

<Youtube id="Dh9CL8fyG80"/>

El [`Trainer`] se encarga del ciclo de entrenamiento y permite aplicar fine-tuning a un modelo en una sola l√≠nea de c√≥digo. Para los que prefieran escribir su propio ciclo de entrenamiento, tambi√©n pueden aplicar fine-tuning a un modelo de ü§ó Transformers en PyTorch nativo.

En este punto, es posible que necesites reiniciar tu notebook o ejecutar el siguiente c√≥digo para liberar algo de memoria:

```py
```

A continuaci√≥n, haremos un post-procesamiento manual al `tokenized_dataset` y as√≠ prepararlo para el entrenamiento.

1. Elimina la columna de `text` porque el modelo no acepta texto en crudo como entrada:

    ```    ```

2. Cambia el nombre de la columna de `label` a `labels` porque el modelo espera que el argumento se llame `labels`:

    ```    ```

3. Establece el formato del dataset para devolver tensores PyTorch en lugar de listas:

    ```    ```

A continuaci√≥n, crea un subconjunto m√°s peque√±o del dataset como se ha mostrado anteriormente para acelerar el fine-tuning:

``````

### DataLoader

Crea un `DataLoader` para tus datasets de entrenamiento y de prueba para poder iterar sobre batches de datos:

```py
```

Carga tu modelo con el n√∫mero de labels previstas:

``````

### Optimiza y programa el learning rate

Crea un optimizador y el learning rate para aplicar fine-tuning al modelo. Vamos a utilizar el optimizador [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) de PyTorch:

``````

Crea el learning rate desde el [`Trainer`]:

```py
>>> from transformers import get_scheduler

>>> num_epochs = 3
```

Por √∫ltimo, especifica el `device` o entorno de ejecuci√≥n para utilizar una GPU si tienes acceso a una. De lo contrario, el entrenamiento en una CPU puede llevarte varias horas en lugar de un par de minutos.

```py
```

<Tip>

Consigue acceso gratuito a una GPU en la nube si es que no tienes este recurso de forma local con un notebook alojado en [Colaboratory](https://colab.research.google.com/) o [SageMaker StudioLab](https://studiolab.sagemaker.aws/).

</Tip>

Genial, ¬°ahora podemos entrenar! ü•≥

### Ciclo de entrenamiento

Para hacer un seguimiento al progreso del entrenamiento, utiliza la biblioteca [tqdm](https://tqdm.github.io/) para a√±adir una barra de progreso sobre el n√∫mero de pasos de entrenamiento:

```py
>>> from tqdm.auto import tqdm

>>> progress_bar = tqdm(range(num_training_steps))

>>> model.train()
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         batch = {k: v.to(device) for k, v in batch.items()}
...         outputs = model(**batch)
...         loss = outputs.loss
...         loss.backward()

```

### M√©tricas

De la misma manera que necesitas a√±adir una funci√≥n de evaluaci√≥n al [`Trainer`], necesitas hacer lo mismo cuando escribas tu propio ciclo de entrenamiento. Pero en lugar de calcular y reportar la m√©trica al final de cada √©poca, esta vez acumular√°s todos los batches con [`add_batch`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=add_batch#datasets.Metric.add_batch) y calcular√°s la m√©trica al final.

```py
>>> metric = load_metric("accuracy")
>>> model.eval()
>>> for batch in eval_dataloader:
...     batch = {k: v.to(device) for k, v in batch.items()}
...     with torch.no_grad():
...         outputs = model(**batch)

...     logits = outputs.logits
```

<a id='additional-resources'></a>

## Recursos adicionales

Para m√°s ejemplos de fine-tuning consulta:

- [ü§ó Transformers Examples](https://github.com/huggingface/transformers/tree/main/examples) incluye scripts
  para entrenar tareas comunes de NLP en PyTorch y TensorFlow.

- [ü§ó Transformers Notebooks](notebooks) contiene varios notebooks sobre c√≥mo aplicar fine-tuning a un modelo para tareas espec√≠ficas en PyTorch y TensorFlow.
