<!--版权2023年HuggingFace团队保留所有权利。

根据Apache许可证第2.0版（“许可证”）许可；除非符合许可证，否则您不得使用此文件。您可以在以下网址获取许可证的副本：

http://www.apache.org/licenses/LICENSE-2.0

除非适用法律要求或书面同意，否则按“按原样”分发的软件，无论是明示还是暗示的，都没有任何担保或条件。请参阅许可证以了解特定语言下的权限和限制。

⚠️ 请注意，本文件虽然使用Markdown编写，但包含了特定的语法，适用于我们的doc-builder（类似于MDX），可能无法在您的Markdown查看器中正常渲染。

-->

# 注意力机制

在注意力矩阵是正方形的意义上，大多数变压器模型使用完全注意力。当你有很长的文本时，这可能是一个很大的计算瓶颈。Longformer和reformer是试图提高效率的模型，它们使用注意力矩阵的稀疏版本来加速训练。

## 局部敏感哈希注意力机制（LSH attention）

[Reformer](model_doc/reformer)使用LSH的注意力。在softmax(QK^t)中，只有矩阵QK^t的最大元素(在softmax维度中)将给出有用的贡献。所以对于Q中的每个查询Q，我们可以只考虑K中靠近Q的键K，使用哈希函数来确定Q和K是否靠近。注意屏蔽被修改以屏蔽当前标记(除了在第一个位置)，因为它将给出相等的查询和键(因此彼此非常相似)。因为散列可能有点随机，所以在实践中使用几个散列函数(由n_rounds参数确定),然后一起求平均。

## 局部注意力机制（Local attention）
[Longformer](model_doc/longformer)使用局部注意力:通常，局部上下文(例如，左边和右边的两个标记是什么？)足以对给定的令牌采取行动。此外，通过堆叠具有小窗口的注意层，最后一层将具有不仅仅是窗口中的标记的感受域，允许它们建立整个句子的表示。

一些预先选择的输入记号也被给予全局关注:对于那些少数记号，关注矩阵可以访问所有记号，并且这个过程是对称的:所有其他记号可以访问那些特定记号(在它们的本地窗口中的记号之上)。这在论文的图2d中示出，参见下面的注意屏蔽示例:

<div class="flex justify-center">
    <img scale="50 %" align="center" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png"/>
</div>

使用那些具有较少参数的注意力矩阵允许模型具有更大序列长度的输入。

## 其他技巧

### 轴向位置编码

[Reformer](model_doc/reformer)使用轴向位置编码:在传统的transformer模型中，位置编码E是一个大小为 \\(l\\) 乘 \\(d\\) 的矩阵，其中 \\(l\\) 是序列长度，而 \\(d\\) 是隐藏状态的维数。如果你有很长的文本，这个矩阵会很大，会占用GPU太多的空间。为了减轻这种情况，轴向位置编码包括将大矩阵E分解成两个更小的矩阵E1和E2，维数为 \\(l_{1} \乘以d_{1}\\) 和 \\(l_{2} \乘以 d_{2}\\),使得 \\(l_{1} \乘以 l_{2} = l\\) 和 \\(d_{1} + d_{2} = d\\) (对于长度的乘积，这最终要小得多)。E中时间步长 \\(j\\) 的嵌入是通过连接E1中时间步长 \\(j \% l1\\) 和E2中时间步长 \\(j // l1\\) 的嵌入而获得的。
