<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# GPT-NeoX-20B

## Overview

The GPT-NeoX-20B model was proposed in [Black et al (2022)](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. It is a 20 billion parameter autoregressive language model.

The abstract from the paper is the following:

*GPT-NeoX-20B is a 20 billion parameter autoregressive language model whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights. In this paper, we describe the model architecture and training, evaluate its performance, and discuss the broader impacts of its release. We are open-sourcing the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.*

Tips:

<INSERT TIPS ABOUT MODEL HERE>

This model was contributed by [zphang](<https://huggingface.co/zphang). The original code can be found [here](https://github.com/EleutherAI/gpt-neox) and [here](https://github.com/zphang/minimal-gpt-neox-20b).

## GPTNeoXConfig

[[autodoc]] GPTNeoXConfig


## GPTNeoXTokenizer

[[autodoc]] GPTNeoXTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary


## GPTNeoXTokenizerFast

[[autodoc]] GPTNeoXTokenizerFast


## GPTNeoXModel

[[autodoc]] GPTNeoXModel
    - forward


## GPTNeoXForCausalLM

[[autodoc]] GPTNeoXForCausalLM
    - forward


## GPTNeoXForMaskedLM

[[autodoc]] GPTNeoXForMaskedLM
    - forward


## GPTNeoXForSequenceClassification

[[autodoc]] transformers.GPTNeoXForSequenceClassification
    - forward

## GPTNeoXForMultipleChoice

[[autodoc]] transformers.GPTNeoXForMultipleChoice
    - forward


## GPTNeoXForTokenClassification

[[autodoc]] transformers.GPTNeoXForTokenClassification
    - forward


## GPTNeoXForQuestionAnswering

[[autodoc]] GPTNeoXForQuestionAnswering
    - forward