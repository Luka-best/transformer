<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# S4

## Overview

The S4 model was proposed in [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396) by <Albert Gu, Karan Goel, Christopher Ré>. The core of S4 is a drop-in
replacement for attention that has extremely good scaling properties as sequence length increases.S4 is able to handle
long-range tasks (like Pathfinder-X) that all previous known architectures fail on.

The abstract from the paper is the following:

*A central goal of sequence modeling is designing a single principled model that can address sequence data across a
range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs,
CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very
long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the
fundamental state space model (SSM) \( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \), and showed that for appropriate
choices of the state matrix \( A \), this system could handle long-range dependencies mathematically and empirically.
However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence
modeling solution. We propose the Structured State Space (S4) sequence model based on a new parameterization for the
SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical
strengths. Our technique involves conditioning \( A \) with a low-rank correction, allowing it to be diagonalized
stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results
across a diverse range of established benchmarks, including (i) 91\% accuracy on sequential CIFAR-10 with no data
augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers
on image and language modeling tasks, while performing generation 60× faster (iii) SoTA on every task from the Long
Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while
being as efficient as all competitors.*

Tips:

<INSERT TIPS ABOUT MODEL HERE>

This model was contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/HazyResearch/state-spaces).

## S4Config

[[autodoc]] S4Config
    - all

## S4Model

[[autodoc]] S4Model
    - forward

## S4LMHeadModel

[[autodoc]] S4LMHeadModel
    - forward
