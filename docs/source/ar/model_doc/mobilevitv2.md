# MobileViTV2

## نظرة عامة
اقترح Sachin Mehta و Mohammad Rastegari نموذج MobileViTV2 في [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680).

MobileViTV2 هو الإصدار الثاني من MobileViT، تم بناؤه عن طريق استبدال الاهتمام الذاتي متعدد الرؤوس في MobileViT باهتمام ذاتي قابل للفصل.

الملخص من الورقة هو ما يلي:

> *يمكن لشبكات المحمول للرؤية المحمولة (MobileViT) تحقيق أداء متميز عبر العديد من مهام الرؤية المحمولة، بما في ذلك التصنيف والكشف. على الرغم من أن هذه النماذج لديها عدد أقل من المعلمات، إلا أن لديها تأخيرًا مرتفعًا مقارنة بالنماذج المستندة إلى الشبكات العصبية التلافيفية. عنق الزجاجة الرئيسي للكفاءة في MobileViT هو الاهتمام الذاتي متعدد الرؤوس (MHA) في المحولات، والذي يتطلب تعقيدًا زمنيًا O(k2) فيما يتعلق بعدد الرموز المميزة (أو الرقعات) k. علاوة على ذلك، يتطلب MHA عمليات مكلفة (على سبيل المثال، الضرب المصفوفي على مستوى الدفعة) لحساب الاهتمام الذاتي، مما يؤثر على التأخير في الأجهزة المحدودة الموارد. تقدم هذه الورقة طريقة اهتمام ذاتي قابلة للفصل ذات تعقيد خطي، أي O(k). إحدى السمات البسيطة والفعالة للأسلوب المقترح هي أنها تستخدم عمليات عنصر-لحساب الاهتمام الذاتي، مما يجعلها خيارًا جيدًا للأجهزة المحدودة الموارد. النموذج المحسن، MobileViTV2، هو الأفضل في فئته على عدة مهام رؤية محمولة، بما في ذلك تصنيف الكائنات ImageNet وكشف الكائنات MS-COCO. وبحوالي ثلاثة ملايين معلمة، يحقق MobileViTV2 دقة أعلى-1 تبلغ 75.6٪ على مجموعة بيانات ImageNet، متفوقًا على MobileViT بحوالي 1٪ مع تشغيل أسرع 3.2× على جهاز محمول.*

تمت المساهمة بهذا النموذج من قبل [shehan97](https://huggingface.co/shehan97). يمكن العثور على الكود الأصلي [هنا](https://github.com/apple/ml-cvnets).

## نصائح الاستخدام

- MobileViTV2 يشبه CNN أكثر من نموذج Transformer. فهو لا يعمل على بيانات التسلسل ولكن على دفعات من الصور. على عكس ViT، لا توجد تضمينات. ينتج نموذج العمود الفقري خريطة ميزات.

- يمكن للمرء استخدام [`MobileViTImageProcessor`] لتحضير الصور للنموذج. لاحظ أنه إذا قمت بمعالجة مسبقة خاصة بك، فإن نقاط التحقق المسبقة التدريب تتوقع أن تكون الصور في ترتيب بكسل BGR (وليس RGB).

- تم تدريب نقاط التصنيف الصورية المتاحة مسبقًا على [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (يشار إليها أيضًا باسم ILSVRC 2012، وهي عبارة عن 1.3 مليون صورة و1000 فئة).

- يستخدم نموذج التجزئة رأس [DeepLabV3](https://arxiv.org/abs/1706.05587). تم تدريب نقاط التحقق من التجزئة الدلالية المتاحة مسبقًا على [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).

## MobileViTV2Config

[[autodoc]] MobileViTV2Config

## MobileViTV2Model

[[autodoc]] MobileViTV2Model

- forward

## MobileViTV2ForImageClassification

[[autodoc]] MobileViTV2ForImageClassification

- forward

## MobileViTV2ForSemanticSegmentation

[[autodoc]] MobileViTV2ForSemanticSegmentation

- forward