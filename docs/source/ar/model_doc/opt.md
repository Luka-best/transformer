# OPT

## ูุธุฑุฉ ุนุงูุฉ
ุงูุชุฑุญ ูููุฐุฌ OPT ูู [ููุงุฐุฌ ุงููุญูู ุงููุบูู ุงููุณุจูุฉ ุงูุชุฏุฑูุจ ุงูููุชูุญุฉ](https://arxiv.org/pdf/2205.01068) ุจูุงุณุทุฉ Meta AI.
OPT ุนุจุงุฑุฉ ุนู ุณูุณูุฉ ูู ููุงุฐุฌ ุงููุบุฉ ุงูุณุจุจูุฉ ููุชูุญุฉ ุงููุตุฏุฑ ุงููุจูุฑุฉ ุงูุชู ุชุชูุชุน ุจุฃุฏุงุก ูุดุงุจู ูู GPT3.

ุงููุณุชุฎูุต ูู ุงููุฑูุฉ ูู ูุง ููู:

*ุฃุธูุฑุช ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉุ ุงูุชู ุบุงูุจูุง ูุง ูุชู ุชุฏุฑูุจูุง ููุฆุงุช ุงูุขูุงู ูู ุฃูุงู ุงูุญูุณุจุฉุ ูุฏุฑุงุช ููุญูุธุฉ ููุชุนูู ุจุฏูู ุจูุงูุงุช ุฃู ุจูุฏุฑ ูููู ูู ุงูุจูุงูุงุช. ููุธุฑูุง ูุชูููุชูุง ุงูุญุณุงุจูุฉุ ูุตุนุจ ุชูุฑุงุฑ ูุฐู ุงูููุงุฐุฌ ุจุฏูู ุฑุฃุณ ูุงู ูุจูุฑ. ูุจุงููุณุจุฉ ูููููู ูููุง ุงููุชุงุญ ูู ุฎูุงู ูุงุฌูุงุช ุจุฑูุฌุฉ ุงูุชุทุจููุงุชุ ูุง ููุณูุญ ุจุงููุตูู ุฅูู ุฃูุฒุงู ุงููููุฐุฌ ุงููุงููุฉุ ููุง ูุฌุนู ุฏุฑุงุณุชูุง ุฃูุฑูุง ุตุนุจูุง. ููุฏู Open Pre-trained Transformers (OPT)ุ ููู ูุฌููุนุฉ ูู ุงููุญููุงุช ุงูููุดูุฑุฉ ูุณุจููุง ูุงูุชู ุชุชุฑุงูุญ ูู 125 ููููู ุฅูู 175 ูููุงุฑ ูุนุงููุ ูุงูุชู ููุฏู ุฅูู ูุดุงุฑูุชูุง ุจุงููุงูู ูุจุดูู ูุณุคูู ูุน ุงูุจุงุญุซูู ุงูููุชููู. ููุธูุฑ ุฃู OPT-175B ููุงุซู ูู GPT-3ุ ูู ุญูู ุฃูู ูุชุทูุจ ููุท 1/7 ูู ุงูุจุตูุฉ ุงููุฑุจูููุฉ ูุชุทููุฑู. ููุง ุฃููุง ูุทูู ุฏูุชุฑ ููุงุญุธุงุชูุง ุงูุฐู ููุตู ุงูุชุญุฏูุงุช ุงูุชู ูุงุฌููุงูุง ุนูู ูุณุชูู ุงูุจููุฉ ุงูุชุญุชูุฉุ ุฅูู ุฌุงูุจ ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ูุชุฌุฑุจุฉ ุฌููุน ุงูููุงุฐุฌ ุงูุชู ุชู ุฅุตุฏุงุฑูุง.*

ุชูุช ุงููุณุงููุฉ ุจูุฐุง ุงููููุฐุฌ ูู ูุจู [Arthur Zucker](https://huggingface.co/ArthurZ) ู [Younes Belkada](https://huggingface.co/ybelkada) ู [Patrick Von Platen](https://huggingface.co/patrickvonplaten).
ูููู ุงูุนุซูุฑ ุนูู ุงูููุฏ ุงูุฃุตูู [ููุง](https://github.com/facebookresearch/metaseq).

ุงููุตุงุฆุญ:

- ูุฏู OPT ููุณ ุจููุฉ [`BartDecoder`].
- ุนูู ุนูุณ GPT2ุ ูุถูู OPT ุฑูุฒ EOS `</s>` ูู ุจุฏุงูุฉ ูู ููุฌู.

## ุงูููุงุฑุฏ

ูููุง ููู ูุงุฆูุฉ ุจููุงุฑุฏ Hugging Face ุงูุฑุณููุฉ ูููุงุฑุฏ ุงููุฌุชูุน (ุงููุดุงุฑ ุฅูููุง ุจู ๐) ููุณุงุนุฏุชู ูู ุงูุจุฏุก ูู ุงุณุชุฎุฏุงู OPT. ุฅุฐุง ููุช ููุชููุง ุจุชูุฏูู ููุฑุฏ ูุฅุฏุฑุงุฌู ููุงุ ููุฑุฌู ูุชุญ ุทูุจ ุณุญุจ ูุณูุฑุงุฌุนู.

ูู ุงููุงุญูุฉ ุงููุซุงููุฉุ ูุฌุจ ุฃู ูุซุจุช ุงูููุฑุฏ ุดูุฆูุง ุฌุฏูุฏูุง ุจุฏูุงู ูู ุชูุฑุงุฑ ููุฑุฏ ููุฌูุฏ.

<PipelineTag pipeline="text-generation" />

- ุฏูุชุฑ ููุงุญุธุงุช ุญูู [ุถุจุท ูููุฐุฌ OPT ุงูุฏููู ุจุงุณุชุฎุฏุงู PEFT ู bitsandbytes ู Transformers](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing). ๐
- ููุดูุฑ ูุฏููุฉ ุญูู [ุงุณุชุฑุงุชูุฌูุงุช ูู ุงูุชุดููุฑ ุจุงุณุชุฎุฏุงู OPT](https://huggingface.co/blog/introducing-csearch#62-example-two---opt).
- ุงููุตู ุงูุฎุงุต ุจู [ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) ูู ุฏูุฑุฉ ๐ค Hugging Face.
- [`OPTForCausalLM`] ูุฏุนูู ุจูุงุณุทุฉ [ูุซุงู ุนูู ูุต ุจุฑูุฌู ูููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling) ู [ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).
- [`TFOPTForCausalLM`] ูุฏุนูู ุจูุงุณุทุฉ [ูุซุงู ุนูู ูุต ุจุฑูุฌู ูููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) ู [ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).
- [`FlaxOPTForCausalLM`] ูุฏุนูู ุจูุงุณุทุฉ [ูุซุงู ุนูู ูุต ุจุฑูุฌู ูููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling).

<PipelineTag pipeline="text-classification" />

- ุฏููู [ูููุฉ ุชุตููู ุงููุต](sequence_classification.md)
- [`OPTForSequenceClassification`] ูุฏุนูู ุจูุงุณุทุฉ [ูุซุงู ุนูู ูุต ุจุฑูุฌู](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) ู [ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb).

<PipelineTag pipeline="question-answering" />

- [`OPTForQuestionAnswering`] ูุฏุนูู ุจูุงุณุทุฉ [ูุซุงู ุนูู ูุต ุจุฑูุฌู ููุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) ู [ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).
- ุงููุตู ุงูุฎุงุต ุจู [ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ](https://huggingface.co/course/chapter7/7?fw=pt) ูู ุฏูุฑุฉ ๐ค Hugging Face.

โก๏ธ ุงูุงุณุชูุชุงุฌ

- ููุดูุฑ ูุฏููุฉ ุญูู [ููููุฉ ุชุดุบูู ๐ค Accelerate ูููุงุฐุฌ ูุจูุฑุฉ ุฌุฏูุง ุจูุถู PyTorch](https://huggingface.co/blog/accelerate-large-models) ูุน OPT.

## ุงูุฌูุน ุจูู OPT ู Flash Attention 2

ุฃููุงูุ ุชุฃูุฏ ูู ุชุซุจูุช ุฃุญุฏุซ ุฅุตุฏุงุฑ ูู Flash Attention 2 ูุชุถููู ููุฒุฉ ูุงูุฐุฉ ุงูุงูุชูุงู ุงูููุฒููุฉ.

```bash
pip install -U flash-attn --no-build-isolation
```

ุชุฃูุฏ ุฃูุถูุง ูู ุฃู ูุฏูู ุฃุฌูุฒุฉ ูุชูุงููุฉ ูุน Flash-Attention 2. ุงูุฑุฃ ุงููุฒูุฏ ุนููุง ูู ุงููุซุงุฆู ุงูุฑุณููุฉ ููุณุชูุฏุน flash-attn. ุชุฃูุฏ ุฃูุถูุง ูู ุชุญููู ูููุฐุฌู ูู ูุตู ุงูุฏูุฉ (ุนูู ุณุจูู ุงููุซุงู `torch.float16``).

ูุชุญููู ูุชุดุบูู ูููุฐุฌ ุจุงุณุชุฎุฏุงู Flash Attention 2ุ ุฑุงุฌุน ุงูููุชุทู ุฃุฏูุงู:

```python
>>> import torch
>>> from transformers import OPTForCausalLM, GPT2Tokenizer
>>> device = "cuda" # ุงูุฌูุงุฒ ุงูุฐู ุณูุชู ุชุญููู ุงููููุฐุฌ ุนููู

>>> model = OPTForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16, attn_implementation="flash_attention_2")
>>> tokenizer = GPT2Tokenizer.from_pretrained("facebook/opt-350m")

>>> prompt = ("A chat between a curious human and the Statue of Liberty.\n\nHuman: What is your name?\nStatue: I am the "
"Statue of Liberty.\nHuman: Where do you live?\nStatue: New York City.\nHuman: How long have you lived "
"there?")

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to(device)
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)
>>> tokenizer.batch_decode(generated_ids)[0]
'</s>A chat between a curious human and the Statue of Liberty.\n\nHuman: What is your name?\nStatue: I am the Statue of Liberty.\nHuman: Where do you live?\nStatue: New York City.\nHuman: How long have you lived there?\nStatue: I have lived here for about a year.\nHuman: What is your favorite place to eat?\nStatue: I love'
```

### ุชุณุฑูุน ุงููุชููุน

ูููุง ููู ุฑุณู ุจูุงูู ููุชุณุฑูุน ุงููุชููุน ุงูุฐู ููุงุฑู ููุช ุงูุงุณุชุฏูุงู ุงูููู ุจูู ุงูุชูููุฐ ุงูุฃุตูู ูู ุงููุญููุงุช ุจุงุณุชุฎุฏุงู ููุทุฉ ุชูุชูุด `facebook/opt-2.7b` ูุฅุตุฏุงุฑ Flash Attention 2 ูู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ุทูููู ุชุณูุณูููู ูุฎุชูููู.

<div style="text-align: center">
<img src="https://user-images.githubusercontent.com/49240599/281101546-d2fca6d2-ee44-48f3-9534-ba8d5bee4531.png">
</div>

ูููุง ููู ุฑุณู ุจูุงูู ููุชุณุฑูุน ุงููุชููุน ุงูุฐู ููุงุฑู ููุช ุงูุงุณุชุฏูุงู ุงูููู ุจูู ุงูุชูููุฐ ุงูุฃุตูู ูู ุงููุญููุงุช ุจุงุณุชุฎุฏุงู ููุทุฉ ุชูุชูุด `facebook/opt-350m` ูุฅุตุฏุงุฑ Flash Attention 2 ูู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ุทูููู ุชุณูุณูููู ูุฎุชูููู.

<div style="text-align-center">
<img src="https://user-images.githubusercontent.com/49240599/281101682-d1144e90-0dbc-46f4-8fc8-c6206cb793c9.png">
</div>

## OPTConfig

[[autodoc]] OPTConfig

<frameworkcontent>
<pt>

## OPTModel

[[autodoc]] OPTModel

- forward

## OPTForCausalLM

[[autodoc]] OPTForCausalLM

- forward

## OPTForSequenceClassification

[[autodoc]] OPTForSequenceClassification

- forward

## OPTForQuestionAnswering

[[autodoc]] OPTForQuestionAnswering

- forward

</pt>
<tf>

## TFOPTModel

[[autodoc]] TFOPTModel

- call

## TFOPTForCausalLM

[[autodoc]] TFOPTForCausalLM

- call

</tf>
<jax>

## FlaxOPTModel

[[autodoc]] FlaxOPTModel

- __call__

## FlaxOPTForCausalLM

[[autodoc]] FlaxOPTForCausalLM

- __call__

</jax>
</frameworkcontent>