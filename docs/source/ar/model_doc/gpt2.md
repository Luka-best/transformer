# OpenAI GPT2

## ูุธุฑุฉ ุนุงูุฉ
ุงูุชุฑุญ ูููุฐุฌ OpenAI GPT-2 ูู [ููุงุฐุฌ ุงููุบุฉ ุบูุฑ ุงูุฎุงุถุนุฉ ููุฅุดุฑุงู ูู ูุชุนููุงุช ูุชุนุฏุฏุฉ ุงูููุงู](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ุจูุงุณุทุฉ Alec Radford ู Jeffrey Wu ู Rewon Child ู David Luan ู Dario Amodei ู Ilya Sutskever ูู [OpenAI] (https://huggingface.co/openai). ุฅูู ูุญูู ุฃุญุงุฏู ุงูุงุชุฌุงู ุชู ุชุฏุฑูุจู ูุณุจููุง ุจุงุณุชุฎุฏุงู ููุฐุฌุฉ ุงููุบุฉ ุนูู ูุฌููุนุฉ ุจูุงูุงุช ูุจูุฑุฉ ุฌุฏูุง ุชุจูุบ ุญูุงูู 40 ุฌูุฌุงุจุงูุช ูู ุจูุงูุงุช ุงููุต.

ุงููุณุชุฎูุต ูู ุงููุฑูุฉ ูู ูุง ููู:

> "GPT-2 ูู ูููุฐุฌ ูุบุฉ ูุจูุฑ ูุงุฆู ุนูู ุงููุญูู ูุน 1.5 ูููุงุฑ ูุนููุฉุ ุชู ุชุฏุฑูุจู ุนูู ูุฌููุนุฉ ุจูุงูุงุช [1] ูู 8 ููุงููู ุตูุญุฉ ููุจ. ูุชู ุชุฏุฑูุจ GPT-2 ุจูุฏู ุจุณูุท: ุงูุชูุจุค ุจุงููููุฉ ุงูุชุงููุฉุ ุจุงููุธุฑ ุฅูู ุฌููุน ุงููููุงุช ุงูุณุงุจูุฉ ุฏุงุฎู ุจุนุถ ุงููุตูุต. ูุคุฏู ุชููุน ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู ุงุญุชูุงุก ูุฐุง ุงููุฏู ุงูุจุณูุท ุจุดูู ุทุจูุนู ุนูู ุนุฑูุถ ุชูุถูุญูุฉ ููุนุฏูุฏ ูู ุงูููุงู ุนุจุฑ ุงููุฌุงูุงุช ุงููุชููุนุฉ. ูุนุฏ GPT-2 ุนูููุฉ ุชูุณูุน ูุจุงุดุฑุฉ ูู GPTุ ูุน ุฃูุซุฑ ูู 10X ูู ุงููุนููุงุช ูุชู ุชุฏุฑูุจู ุนูู ุฃูุซุฑ ูู 10X ูู ูููุฉ ุงูุจูุงูุงุช."

[ุงูุชุจ ุจุงุณุชุฎุฏุงู ุงููุญูู](https://transformer.huggingface.co/doc/gpt2-large) ูู ุชุทุจูู ููุจ ุชู ุฅูุดุงุคู ูุงุณุชุถุงูุชู ุจูุงุณุทุฉ Hugging Face ูุงูุฐู ููุถุญ ุงููุฏุฑุงุช ุงูุชูููุฏูุฉ ูุนุฏุฉ ููุงุฐุฌ. ูุนุฏ GPT-2 ุฃุญุฏ ูุฐู ุงูููุงุฐุฌ ููู ูุชุงุญ ุจุฎูุณุฉ ุฃุญุฌุงู ูุฎุชููุฉ: ุตุบูุฑ ููุชูุณุท ููุจูุฑ ูxl ูุฅุตุฏุงุฑ ููุทุฑ ูู ููุทุฉ ุชูุชูุด ุตุบูุฑุฉ: *distilgpt-2*.

ุชูุช ุงููุณุงููุฉ ุจูุฐุง ุงููููุฐุฌ ูู ูุจู [thomwolf](https://huggingface.co/thomwolf). ูููู ุงูุนุซูุฑ ุนูู ุงูููุฏ ุงูุฃุตูู [ููุง](https://openai.com/blog/better-language-models/).

## ูุตุงุฆุญ ุงูุงุณุชุฎุฏุงู

- GPT-2 ูู ูููุฐุฌ ูุน ุชุถููู ุงูููุถุน ุงููุทููุ ูุฐูู ูููุตุญ ุนุงุฏุฉู ุจุชุนุจุฆุฉ ุงููุฏุฎูุงุช ุนูู ุงููููู ุจุฏูุงู ูู ุงููุณุงุฑ.

- ุชู ุชุฏุฑูุจ GPT-2 ุจูุฏู ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ (CLM) ูุจุงูุชุงูู ููู ููู ูู ุงูุชูุจุค ุจุงูุฑูุฒ ุงูุชุงูู ูู ุชุณูุณู. ุงูุงุณุชูุงุฏุฉ ูู ูุฐู ุงูููุฒุฉ ุชุณูุญ ูู GPT-2 ุจุชูููุฏ ูุต ูุชูุงุณู ูู ุงููุงุญูุฉ ุงูุชุฑููุจูุฉ ููุง ูููู ููุงุญุธุชู ูู ูุซุงู *run_generation.py* ุงููุตู.

- ูููู ุฃู ูุฃุฎุฐ ุงููููุฐุฌ *past_key_values* (ูู PyTorch) ุฃู *past* (ูู TF) ูุฅุฏุฎุงูุ ููู ุฃุฒูุงุฌ ุงูุงูุชูุงู ุงูุฑุฆูุณูุฉ/ุงูููู ุงููุญุณูุจุฉ ูุณุจููุง. ุจุงุณุชุฎุฏุงู ูุฐุง (*past_key_values* ุฃู *past*) ุงููููุฉ ูููุน ุงููููุฐุฌ ูู ุฅุนุงุฏุฉ ุญุณุงุจ ุงูููู ุงููุญุณูุจุฉ ูุณุจููุง ูู ุณูุงู ุชูููุฏ ุงููุต. ุจุงููุณุจุฉ ูู PyTorchุ ุฑุงุฌุน ุญุฌุฉ *past_key_values* ูุทุฑููุฉ [`GPT2Model.forward`]ุ ุฃู ุจุงููุณุจุฉ ูู TFุ ุฑุงุฌุน ุญุฌุฉ *past* ูุทุฑููุฉ [`TFGPT2Model.call`] ููุฒูุฏ ูู ุงููุนูููุงุช ุญูู ุงุณุชุฎุฏุงูู.

- ุณูุทุจู ุชูููู ุนูุงูุงุช *scale_attn_by_inverse_layer_idx* ู *reorder_and_upcast_attn* ุชุญุณููุงุช ุงูุงุณุชูุฑุงุฑ ุงูุชุฏุฑูุจู ูู [Mistral](https://github.com/stanford-crfm/mistral/) (ูู PyTorch ููุท).

## ูุซุงู ุงูุงุณุชุฎุฏุงู

ูููู ุงุณุชุฎุฏุงู ุทุฑููุฉ `generate()` ูุชูููุฏ ูุต ุจุงุณุชุฎุฏุงู ูููุฐุฌ GPT2.

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("gpt2")
>>> tokenizer = AutoTokenizer.from_pretrained("gpt2")

>>> prompt = "GPT2 is a model developed by OpenAI."

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

## ุงุณุชุฎุฏุงู ููุงุด ุงูุงูุชูุงู 2

Flash Attention 2 ูู ุฅุตุฏุงุฑ ุฃุณุฑุน ูุฃูุซุฑ ุชุญุณูููุง ูู ุญุณุงุจ ุฏุฑุฌุงุช ุงูุงูุชูุงู ูุงูุฐู ูุนุชูุฏ ุนูู ููุงุฉ `cuda`.

### ุงูุชุซุจูุช

ุฃููุงูุ ุชุญูู ููุง ุฅุฐุง ูุงู ุงูุฃุฌูุฒุฉ ุงูุฎุงุตุฉ ุจู ูุชูุงููุฉ ูุน Flash Attention 2. ูููู ุงูุนุซูุฑ ุนูู ุฃุญุฏุซ ูุงุฆูุฉ ููุฃุฌูุฒุฉ ุงููุชูุงููุฉ ูู [ุงููุซุงุฆู ุงูุฑุณููุฉ](https://github.com/Dao-AILab/flash-attention#installation-and-features). ุฅุฐุง ูู ููู ุงูุฃุฌูุฒุฉ ุงูุฎุงุต ุจู ูุชูุงูููุง ูุน Flash Attention 2ุ ูููููู ุงูุงุณุชูุงุฏุฉ ูู ุชุญุณููุงุช ููุงุฉ ุงูุงูุชูุงู ูู ุฎูุงู ุฏุนู ุงููุญูู ุงูุฃูุถู ุงููุดููู [ุฃุนูุงู](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).

ุจุนุฏ ุฐููุ ูู ุจุชุซุจูุช ุฃุญุฏุซ ุฅุตุฏุงุฑ ูู Flash Attention 2:

```bash
pip install -U flash-attn --no-build-isolation
```

### ุงูุงุณุชุฎุฏุงู

ูุชุญููู ูููุฐุฌ ุจุงุณุชุฎุฏุงู Flash Attention 2ุ ูููููุง ุชูุฑูุฑ ุงูุญุฌุฉ `attn_implementation="flash_attention_2"` ุฅูู [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). ุณูููู ุฃูุถูุง ุจุชุญููู ุงููููุฐุฌ ูู ูุตู ุงูุฏูุฉ (ุนูู ุณุจูู ุงููุซุงู `torch.float16`)ุ ุญูุซ ูุคุฏู ุฐูู ุฅูู ุงูุฎูุงุถ ูุจูุฑ ูู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ ูุณุฑุนุฉ ุงูุงุณุชุฏูุงู ูุน ุงูุฎูุงุถ ุทููู ูู ุฌูุฏุฉ ุงูุตูุช:

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> device = "cuda" # the device to load the model onto

>>> model = AutoModelForCausalLM.from_pretrained("gpt2", torch_dtype=torch.float16, attn_implementation="flash_attention_2")
>>> tokenizer = AutoTokenizer.from_pretrained("gpt2")

>>> prompt = "def hello_world():"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to(device)
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
```

### ุชุณุฑูุน ุงููุชููุน

ูููุง ููู ุฑุณู ุจูุงูู ููุชุณุฑูุน ุงููุชููุน ุงูุฐู ููุงุฑู ููุช ุงูุงุณุชุฏูุงู ุงูููู ุจูู ุงูุชูููุฐ ุงูุฃุตูู ูู ุงููุญููุงุช ุจุงุณุชุฎุฏุงู ููุทุฉ ุชูุชูุด `gpt2` ูุฅุตุฏุงุฑ Flash Attention 2 ูู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ุทูู ุชุณูุณู ูุจูุบ 512.

<div style="text-align: center">
<img src="https://huggingface.co/datasets/EduardoPacheco/documentation-images/resolve/main/gpt2_flash_attention_2_speedup.jpg">
</div>

## ุงูููุงุฑุฏ

ูููุง ููู ูุงุฆูุฉ ุจููุงุฑุฏ Hugging Face ุงูุฑุณููุฉ ูุงููุฌุชูุนูุฉ (ูุดุงุฑ ุฅูููุง ุจุฑูุฒ ๐) ููุณุงุนุฏุชู ูู ุงูุจุฏุก ุจุงุณุชุฎุฏุงู GPT2. ุฅุฐุง ููุช ููุชููุง ุจุชูุฏูู ููุฑุฏ ูุฅุฏุฑุงุฌู ููุงุ ูุงูุฑุฌุงุก ูุชุญ ุทูุจ ุณุญุจ ูุณูุฑุงุฌุนู! ูุฌุจ ุฃู ููุถุญ ุงูููุฑุฏ ุจุดูู ูุซุงูู ุดูุก ุฌุฏูุฏ ุจุฏูุงู ูู ุชูุฑุงุฑ ููุฑุฏ ููุฌูุฏ.

- ูุฏููุฉ ุญูู ููููุฉ [ุถุจุท ูููุฐุฌ GPT-2 ุบูุฑ ุงูุฅูุฌููุฒู ุจุงุณุชุฎุฏุงู Hugging Face](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface).

- ูุฏููุฉ ุญูู [ููููุฉ ุชูููุฏ ุงููุต: ุงุณุชุฎุฏุงู ุทุฑู ูู ูุฎุชููุฉ ูุชูููุฏ ุงููุบุฉ ูุน ุงููุญููุงุช](https://huggingface.co/blog/how-to-generate) ูุน GPT-2.

- ูุฏููุฉ ุญูู [ุชุฏุฑูุจ CodeParrot ๐ฆ ูู ุงูุตูุฑ](https://huggingface.co/blog/codeparrot)ุ ููู ูููุฐุฌ GPT-2 ูุจูุฑ.

- ูุฏููุฉ ุญูู [ุชูููุฏ ูุต ุฃุณุฑุน ูุน TensorFlow ู XLA](https://huggingface.co/blog/tf-xla-generate) ูุน GPT-2.

- ูุฏููุฉ ุญูู [ููููุฉ ุชุฏุฑูุจ ูููุฐุฌ ูุบุฉ ุจุงุณุชุฎุฏุงู Megatron-LM](https://huggingface.co/blog/megatron-training) ูุน ูููุฐุฌ GPT-2.

- ุฏูุชุฑ ููุงุญุธุงุช ุญูู ููููุฉ [ุถุจุท ุฏููู ูู GPT2 ูุชูููุฏ ูููุงุช ุงูุฃุบุงูู ุนูู ุบุฑุงุฑ ููุงูู ุงูููุถู](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb). ๐

- ุฏูุชุฑ ููุงุญุธุงุช ุญูู ููููุฉ [ุถุจุท ุฏููู ูู GPT2 ูุชูููุฏ ุงูุชุบุฑูุฏุงุช ุนูู ุบุฑุงุฑ ูุณุชุฎุฏู Twitter ุงูููุถู ูุฏูู](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb). ๐

- ูุตู [ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) ูู ุฏูุฑุฉ ๐ค Hugging Face Course.

- [`GPT2LMHeadModel`] ูุฏุนูู ุจูุงุณุทุฉ ูุฐุง [ูุซุงู ุนูู ุงููุต ุงูุจุฑูุฌู ูููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling)ุ [ูุซุงู ุนูู ุงููุต ุงูุจุฑูุฌู ูุชูููุฏ ุงููุต](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation)ุ ู [ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).

- [`TFGPT2LMHeadModel`] ูุฏุนูู ุจูุงุณุทุฉ ูุฐุง [ูุซุงู ุนูู ุงููุต ุงูุจุฑูุฌู ูููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) ู [ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).

- [`FlaxGPT2LMHeadModel`] ูุฏุนูู ุจูุงุณุทุฉ ูุฐุง [ูุซุงู ุนูู ุงููุต ุงูุจุฑูุฌู ูููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#causal-language-modeling) ู [ุฏูุชุฑ ุงูููุงุญุธุงุช](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb).

- ุฏููู ูููุฉ ุชุตููู ุงููุต [](../tasks/sequence_classification)

- ุฏููู ูููุฉ ุชุตููู ุงูุฑููุฒ [](../tasks/token_classification)

- ุฏููู ูููุฉ ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ [](../tasks/language_modeling)

## GPT2Config

[[autodoc]] GPT2Config

## GPT2Tokenizer

[[autodoc]] GPT2Tokenizer

- save_vocabulary

## GPT2TokenizerFast

[[autodoc]] GPT2TokenizerFast

## ูุฎุฑุฌุงุช GPT2 ุงููุญุฏุฏุฉ

[[autodoc]] models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput

[[autodoc]] models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModelOutput

<frameworkcontent>
<pt>

## GPT2Model

[[autodoc]] GPT2Model

- forward

## GPT2LMHeadModel

[[autodoc]] GPT2LMHeadModel

- forward

## GPT2DoubleHeadsModel

[[autodoc]] GPT2DoubleHeadsModel

- forward

## GPT2ForQuestionAnswering

[[autodoc]] GPT2ForQuestionAnswering

- forward

## GPT2ForSequenceClassification

[[autodoc]] GPT2ForSequenceClassification


- forward

## GPT2ForTokenClassification

[[autodoc]] GPT2ForTokenClassification


- forward

</pt>
<tf>

## TFGPT2Model

[[autodoc]] TFGPT2Model

- call

## TFGPT2LMHeadModel


[[autodoc]] TFGPT2LMHeadModel

- call

## TFGPT2DoubleHeadsModel

[[autodoc]] TFGPT2DoubleHeadsModel

- call

## TFGPT2ForSequenceClassification

[[autodoc]] TFGPT2ForSequenceClassification

- call

## TFSequenceClassifierOutputWithPast

[[autodoc]] modeling_tf_outputs.TFSequenceClassifierOutputWithPast

## TFGPT2Tokenizer

[[autodoc]] TFGPT2Tokenizer

</tf>
<jax>

## FlaxGPT2Model

[[autodoc]] FlaxGPT2Model

- __call__

## FlaxGPT2LMHeadModel

[[autodoc]] FlaxGPT2LMHeadModel

- __call__

</jax>
</frameworkcontent>