## Llama3

```py
import transformers
import torch

model_id = "meta-llama/Meta-Llama-3-8B"

pipeline = transformers.pipeline("text-generation", model=model_id, model_kwargs={"torch_dtype": torch.bfloat16}, device_map="auto")
pipeline("Hey how are you doing today?")
```

## ูุธุฑุฉ ุนุงูุฉ
ุงูุชุฑุญ ูุฑูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู Meta ูููุฐุฌ Llama3 ูู ููุดูุฑ ุงููุฏููุฉ [ุชูุฏูู Meta Llama 3: ุฃูุซุฑ ุฃูุธูุฉ ุงููุบุฉ ุงูููุชูุญุฉ ูุฏุฑุฉ ุญุชู ุงูุขู](https://ai.meta.com/blog/meta-llama-3/).

ููููุง ููู ุงูููุฎุต ุงูููุดูุฑ ูู ุงููุฏููุฉ:

*ุงููููุ ูุณุฑูุง ุฃู ูุนูู ุนู ุฃูู ูููุฐุฌูู ูู ุงูุฌูู ุงูุชุงูู ูู Llamaุ Meta Llama 3ุ ูุงููุชุงุญูู ููุงุณุชุฎุฏุงู ุงูุนุงู. ูุชุถูู ูุฐุง ุงูุฅุตุฏุงุฑ ูููุฐุฌูู ููุบุฉ ูุณุจูุฉ ุงูุชุฏุฑูุจ ููุถุจูุทุฉ ุงูุชุนูููุงุช ูุน 8B ู70B ูู ุงููุนููุงุช ุงูุชู ูููู ุฃู ุชุฏุนู ูุฌููุนุฉ ูุงุณุนุฉ ูู ุญุงูุงุช ุงูุงุณุชุฎุฏุงู. ูุชุจูู ุงูุฌูู ุงูุชุงูู ูู Llama ุฃุฏุงุกู ุฑุงุฆุฏูุง ูู ุงููุฌุงู ุนูู ูุฌููุนุฉ ูุงุณุนุฉ ูู ุงููุนุงููุฑ ุงูุตูุงุนูุฉ ูููุฏู ูุฏุฑุงุช ุฌุฏูุฏุฉุ ุจูุง ูู ุฐูู ุชุญุณูู ุงูุงุณุชุฏูุงู. ูุนุชูุฏ ุฃู ูุฐู ุงูููุงุฐุฌ ูู ุงูุฃูุถู ูู ูุฆุชูุง ูู ุงููุตุงุฏุฑ ุงูููุชูุญุฉุ ุจุจุณุงุทุฉ. ูุฏุนููุง ูููุฌูุง ุงูููุชูุญ ุงูุฐู ูุชุจุนู ููุฐ ูุชุฑุฉ ุทูููุฉุ ูุฅููุง ูุถุน Llama 3 ูู ุฃูุฏู ุงููุฌุชูุน. ููุฑูุฏ ุฃู ูุทูู ุดุฑุงุฑุฉ ุงูููุฌุฉ ุงูุชุงููุฉ ูู ุงูุงุจุชูุงุฑ ูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุนุจุฑ ุงูููุฏุณ - ุจุฏุกูุง ูู ุงูุชุทุจููุงุช ูุญุชู ุฃุฏูุงุช ุงููุทูุฑูู ูุญุชู ุนูููุงุช ุงูุชูููู ูุญุชู ุชุญุณููุงุช ุงูุงุณุชุฏูุงู ูุงููุฒูุฏ. ููุง ูููููุง ุงูุงูุชุธุงุฑ ููุฑู ูุง ุณุชุจููู ููุชุทูุน ุฅูู ุชููู ููุงุญุธุงุชู.*

ููููู ุงูุงุทูุงุน ุนูู ุฌููุน ููุงุท ุชูุชูุด ูููุฐุฌ Llama3 [ููุง](https://huggingface.co/models?search=llama3).

ูููู ุงูุนุซูุฑ ุนูู ุงูููุฏ ุงูุฃุตูู ูููุคูููู [ููุง](https://github.com/meta-llama/llama3).

## ูุตุงุฆุญ ุงูุงุณุชุฎุฏุงู

<Tip warning={true}>

ุชู ุชุฏุฑูุจ ููุงุฐุฌ `Llama3` ุจุงุณุชุฎุฏุงู `bfloat16`ุ ูููู ุงูุงุณุชุฏูุงู ุงูุฃุตูู ูุณุชุฎุฏู `float16`. ุชุณุชุฎุฏู ููุงุท ุงูุชูุชูุด ุงูุชู ุชู ุชุญููููุง ุนูู Hub `torch_dtype = 'float16'`ุ ูุงูุชู ุณูุชู ุงุณุชุฎุฏุงููุง ุจูุงุณุทุฉ ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช `AutoModel` ูุชุญููู ููุงุท ุงูุชูุชูุด ูู `torch.float32` ุฅูู `torch.float16`.

ุฅู `dtype` ููุฃูุฒุงู ุนุจุฑ ุงูุฅูุชุฑูุช ุบูุฑ ุฐู ุตูุฉ ุฅูู ุญุฏ ูุจูุฑ ูุง ูู ุชูู ุชุณุชุฎุฏู `torch_dtype="auto"` ุนูุฏ ุชููุฆุฉ ูููุฐุฌ ุจุงุณุชุฎุฏุงู `model = AutoModelForCausalLM.from_pretrained("path", torch_dtype = "auto")`. ูุงูุณุจุจ ูู ุฃู ุงููููุฐุฌ ุณูุชู ุชูุฒููู ุฃููุงู (ุจุงุณุชุฎุฏุงู `dtype` ูููุงุท ุงูุชูุชูุด ุนุจุฑ ุงูุฅูุชุฑูุช)ุ ุซู ุณูุชู ุชุญูููู ุฅูู `dtype` ุงูุงูุชุฑุงุถู ูู `torch` (ูุตุจุญ `torch.float32`)ุ ูุฃุฎูุฑูุงุ ุฅุฐุง ูุงู ููุงู `torch_dtype` ููุฏู ูู ุงูุชููููุ ูุณูุชู ุงุณุชุฎุฏุงูู.

ูุง ููุตู ุจุงูุชุฏุฑูุจ ุนูู ุงููููุฐุฌ ูู `float16` ููู ุงููุนุฑูู ุฃูู ููุชุฌ `nan`ุ ูุฐููุ ูุฌุจ ุชุฏุฑูุจ ุงููููุฐุฌ ูู `bfloat16`.

</Tip>

ูุตุงุฆุญ:

- ูููู ุงูุญุตูู ุนูู ุงูุฃูุฒุงู ุงูุฎุงุตุฉ ุจููุงุฐุฌ Llama3 ุนู ุทุฑูู ููุก [ูุฐุง ุงููููุฐุฌ](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).
- ุงูููุฏุณุฉ ุงููุนูุงุฑูุฉ ูู ููุณูุง ุชูุงููุง ูุซู Llama2.
- ูุญุฏุฏ ุงูุฑููุฒ ูู ูููุฐุฌ BPE ูุนุชูุฏ ุนูู [tiktoken](https://github.com/openai/tiktoken) (ููุงุจู ุงููุณุชูุฏ ุฅูู ุชูููุฐ sentencepiece ูู Llama2). ูุงููุฑู ุงูุฑุฆูุณู ูู ุฃูู ูุชุฌุงูู ููุงุนุฏ ุงูุฏูุฌ BPE ุนูุฏูุง ูููู ุฑูุฒ ุงูุฅุฏุฎุงู ุฌุฒุกูุง ูู ุงูููุฑุฏุงุช. ููุฐุง ูุนูู ุฃูู ุฅุฐุง ูู ููู ููุงู ุฏูุฌ ูุฅูุชุงุฌ "hugging"ุ ุจุฏูุงู ูู ูุฌูุฏ ุฃุตุบุฑ ูุญุฏุงุชุ ูุซู `["hug" ู"ging"] ุชุดูู ุฑูุฒููุ ุฅุฐุง ูุงูุช "hugging" ุฌุฒุกูุง ูู ุงูููุฑุฏุงุชุ ูุณูุชู ุฅุฑุฌุงุนูุง ุชููุงุฆููุง ูุฑููุฒ.
- ูุณุชุฎุฏู ุงููููุฐุฌ ุงูุฃุตูู `pad_id = -1` ููุง ูุนูู ุฃูู ูุง ููุฌุฏ ุฑูุฒ ุฎุงุต ููุชุนุจุฆุฉ. ูุง ูููููุง ุฃู ูุชุจุน ููุณ ุงูููุทูุ ูุชุฃูุฏ ูู ุฅุถุงูุฉ ุฑูุฒ ุฎุงุต ุจุงูุชุนุจุฆุฉ ุจุงุณุชุฎุฏุงู `tokenizer.add_special_tokens({"pad_token":"<pad>"})` ููู ุจุชุบููุฑ ุญุฌู ุชุถููู ุงูุฑูุฒ ููููุง ูุฐูู. ูุฌุจ ุนููู ุฃูุถูุง ุชุนููู `model.config.pad_token_id`. ูุชู ุชููุฆุฉ ุทุจูุฉ `embed_tokens` ูููููุฐุฌ ุจุงุณุชุฎุฏุงู `self.embed_tokens = nn.Embedding(config.vocab_sizeุ config.hidden_sizeุ self.config.padding_idx)`ุ ูุงูุชู ุชุชุฃูุฏ ูู ุฃู ุชุฑููุฒ ุฑูุฒ ุงูุชุนุจุฆุฉ ุณููุชุฌ ุฃุตูุงุฑูุงุ ูุฐุง ููุตู ุจุชูุฑูุฑู ุนูุฏ ุงูุชููุฆุฉ.
- ูููู ุชุญููู ููุทุฉ ุงูุชูุชูุด ุงูุฃุตููุฉ ุจุงุณุชุฎุฏุงู [ุณูุฑูุจุช ุงูุชุญููู](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). ูููู ุงุณุชุฏุนุงุก ุงูุณูุฑูุจุช ุจุงุณุชุฎุฏุงู ุงูุฃูุฑ ุงูุชุงูู (ููุซุงู):

```bash
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path --llama_version 3
```

- ุจุนุฏ ุงูุชุญูููุ ูููู ุชุญููู ุงููููุฐุฌ ููุญูู ุงูุฑููุฒ ุจุงุณุชุฎุฏุงู ูุง ููู:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/output/path")
model = AutoModelForCausalLM.from_pretrained("/output/path")
```

ููุงุญุธุฉ: ูุชุทูุจ ุชูููุฐ ุงูุณูุฑูุจุช ูุณุงุญุฉ ูุงููุฉ ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู (RAM) ูุชูููุฐ ุงููููุฐุฌ ุจุงููุงูู ูู ุฏูุฉ float16 (ุญุชู ุฅุฐุง ูุงูุช ุฃูุจุฑ ุงูุฅุตุฏุงุฑุงุช ุชุฃุชู ูู ุนุฏุฉ ููุงุท ุชูุชูุดุ ูุญุชูู ูู ูููุง ุนูู ุฌุฒุก ูู ูู ูุฒู ูููููุฐุฌุ ูุฐูู ูุญุชุงุฌ ุฅูู ุชุญููููุง ุฌููุนูุง ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู). ุจุงููุณุจุฉ ูููููุฐุฌ 75Bุ ููุงู ุญุงุฌุฉ ุฅูู 145 ุฌูุฌุงุจุงูุช ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู.

- ุนูุฏ ุงุณุชุฎุฏุงู Flash Attention 2 ุนุจุฑ `attn_implementation="flash_attention_2"`ุ ูุง ุชูุฑุฑ `torch_dtype` ุฅูู ุทุฑููุฉ `from_pretrained` ูุงุณุชุฎุฏู ุงูุชุฏุฑูุจ ุนูู ุงูุฃุฑูุงู ุงูุนุดุฑูุฉ ุงูุนุงุฆูุฉ ุงููุฎุชูุทุฉ ุงูุชููุงุฆูุฉ. ุนูุฏ ุงุณุชุฎุฏุงู "ูุฏุฑุจ"ุ ูููู ุงูุฃูุฑ ุจุจุณุงุทุฉ ุจุชุญุฏูุฏ ุฅูุง `fp16` ุฃู `bf16` ุฅูู `True`. ูุฅูุงุ ูุชุฃูุฏ ูู ุงุณุชุฎุฏุงู `torch.autocast`. ูุฐุง ูุทููุจ ูุฃู Flash Attention ูุฏุนู ููุท ุฃููุงุน ุงูุจูุงูุงุช `fp16` ู`bf16`.

## ุงูุงุณุชุฎุฏุงู ุงูุณุฑูุน

```py3
import transformers
import torch

model_id = "meta-llama/Meta-Llama-3-8B"

pipeline = transformers.pipeline("text-generation", model=model_id, model_kwargs={"torch_dtype": torch.bfloat16}, device_map="auto")
pipeline("Hey how are you doing today?")
```

## ุงูููุงุฑุฏ

ุชุชููุฑ ุงููุซูุฑ ูู ุงูููุงุฑุฏ ุงูุฑุงุฆุนุฉ ุจุงููุนู ุนูู ุตูุญุฉ ูุซุงุฆู [~llama2]ุ ููุฏุนู ุงููุณุงูููู ุฅูู ุฅุถุงูุฉ ููุงุฑุฏ ุฌุฏูุฏุฉ ูุฎุตุตุฉ ูู Llama3 ููุง! ๐ค