# LongT5

## نظرة عامة
اقترح نموذج LongT5 في ورقة بحثية بعنوان: "LongT5: Efficient Text-To-Text Transformer for Long Sequences" من قبل مندي جوو، وجوشوا إنسلي، وديفيد يثوس، وسانتياغو أونتانون، وجيانمو ني، ويون-هسوان سونغ، ويينفي يانغ. وهو محول ترميز وفك ترميز تم تدريبه مسبقًا في إعداد توليدي لإزالة تشويش النص إلى نص. يعد نموذج LongT5 امتدادًا لنموذج T5، ويتيح استخدام إحدى آليتي الانتباه الكفءتين المختلفتين - (1) الانتباه المحلي، أو (2) الانتباه العالمي العابر.

مقتبس من الورقة البحثية:

> أظهر العمل الحديث أن إما (1) زيادة طول الإدخال أو (2) زيادة حجم النموذج يمكن أن يحسن أداء النماذج العصبية المستندة إلى محول. في هذه الورقة، نقدم نموذجًا جديدًا، يسمى LongT5، والذي نستكشف من خلاله آثار تغيير حجم كل من طول الإدخال وحجم النموذج في نفس الوقت. على وجه التحديد، قمنا بدمج أفكار الانتباه من المحولات المدخلة الطويلة (ETC)، واعتمدنا استراتيجيات التدريب المسبق من التدريب المسبق للتلخيص (PEGASUS) في بنية T5 القابلة للتطوير. وكانت النتيجة آلية انتباه جديدة نسميها "العالمية العابرة" (TGlobal)، والتي تحاكي آلية الانتباه المحلية/العالمية لـ ETC، ولكن دون الحاجة إلى إدخالات جانبية إضافية. وقد تمكنا من تحقيق نتائج متقدمة في عدة مهام تلخيص، وتفوق نماذج T5 الأصلية في مهام الأسئلة والأجوبة.

تمت المساهمة بهذا النموذج من قبل [stancld]. يمكن العثور على الكود الأصلي [هنا].

## نصائح الاستخدام
- [`LongT5ForConditionalGeneration`] هو امتداد لـ [`T5ForConditionalGeneration`] حيث يستبدل طبقة *الانتباه الذاتي* للترميز بانتباه *محلي* أو *عابر عالمي* (*tglobal*) فعال.
- على عكس نموذج T5، لا يستخدم LongT5 بادئة المهمة. علاوة على ذلك، فإنه يستخدم هدفًا تدريبيًا مسبقًا مختلفًا مستوحى من التدريب المسبق لـ [`PegasusForConditionalGeneration`].
- تم تصميم نموذج LongT5 للعمل بكفاءة وبشكل جيد جدًا في مهام *الترجمة الآلية* ذات النطاق الطويل حيث يتجاوز تسلسل الإدخال 512 رمزًا المستخدمة بشكل شائع. ويمكنه التعامل مع تسلسلات الإدخال بطول يصل إلى 16384 رمزًا.
- بالنسبة لـ *Local Attention*، تسمح عملية الانتباه المحلي المنزلق والمتباعد للرمز المعطى بالانتباه فقط إلى `r` من الرموز الموجودة على اليسار واليمين منه (مع `r=127` بشكل افتراضي). لا يقدم *Local Attention* أي معلمات جديدة للنموذج. تعقيد الآلية خطي في طول تسلسل الإدخال `l`: `O(l*r)`.
- *Transient Global Attention* هو امتداد لـ *Local Attention*. علاوة على ذلك، فإنه يسمح لكل رمز إدخال بالتفاعل مع جميع الرموز الأخرى في الطبقة. يتم ذلك عن طريق تقسيم تسلسل الإدخال إلى كتل ذات طول ثابت `k` (مع `k=16` بشكل افتراضي). بعد ذلك، يتم الحصول على رمز عالمي لمثل هذا الكتلة عن طريق جمع وتطبيع الرموز المضمنة لكل رمز في الكتلة. وبفضل ذلك، تسمح آلية الانتباه لكل رمز بالانتباه إلى كل من الرموز القريبة كما هو الحال في الانتباه المحلي، وإلى كل رمز عالمي كما هو الحال في الانتباه العالمي العادي (*transient* يمثل حقيقة أن الرموز العالمية يتم إنشاؤها ديناميكيًا داخل كل عملية انتباه). ونتيجة لذلك، تقدم آلية *TGlobal* بعض المعلمات الجديدة -- انحيازات الموضع النسبي العالمية وتطبيع الطبقة لرموز الكتل العالمية. تعقيد هذه الآلية هو `O(l(r + l/k))`.

## الموارد
- [دليل مهمة الترجمة](../tasks/translation)
- [دليل مهمة الملخص](../tasks/summarization)

## LongT5Config

[[autodoc]] LongT5Config

<frameworkcontent>
<pt>
## LongT5Model

[[autodoc]] LongT5Model

- forward

## LongT5ForConditionalGeneration

[[autodoc]] LongT5ForConditionalGeneration

- forward

## LongT5EncoderModel

[[autodoc]] LongT5EncoderModel

- forward

</pt>
<jax>
## FlaxLongT5Model

[[autodoc]] FlaxLongT5Model

- __call__
- encode
- decode

## FlaxLongT5ForConditionalGeneration

[[autodoc]] FlaxLongT5ForConditionalGeneration

- __call__
- encode
- decode

</jax>
</frameworkcontent>