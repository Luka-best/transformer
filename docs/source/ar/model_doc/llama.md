# LLaMA

## ูุธุฑุฉ ุนุงูุฉ
ุงูุชุฑุญ ูููุฐุฌ LLaMA ูู [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) ุจูุงุณุทุฉ Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothรฉe Lacroix, Baptiste Roziรจre, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. ููู ูุฌููุนุฉ ูู ููุงุฐุฌ ุงููุบุฉ ุงูุฃุณุงุณูุฉ ุงูุชู ุชุชุฑุงูุญ ูู 7B ุฅูู 65B ูู ุงููุนููุงุช.

ุงูููุฎุต ูู ุงููุฑูุฉ ูู ูุง ููู:

*ูุญู ููุฏู LLaMAุ ููู ูุฌููุนุฉ ูู ููุงุฐุฌ ุงููุบุฉ ุงูุฃุณุงุณูุฉ ุงูุชู ุชุชุฑุงูุญ ูู 7B ุฅูู 65B ูู ุงููุนููุงุช. ูููู ุจุชุฏุฑูุจ ููุงุฐุฌูุง ุนูู ุชุฑูููููุงุช ูู ุงูุฑููุฒุ ููุธูุฑ ุฃูู ูู ุงููููู ุชุฏุฑูุจ ููุงุฐุฌ ูุชูุฏูุฉ ุจุงุณุชุฎุฏุงู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงููุชุงุญุฉ ููุฌูููุฑ ุญุตุฑููุงุ ุฏูู ุงููุฌูุก ุฅูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงููููููุฉ ูุบูุฑ ุงููุชุงุญุฉ. ูุนูู ูุฌู ุงูุฎุตูุตุ ูุชููู LLaMA-13B ุนูู GPT-3 (175B) ูู ูุนุธู ุงููุนุงููุฑุ ูููุงูุณ LLaMA-65B ุฃูุถู ุงูููุงุฐุฌุ Chinchilla-70B ู PaLM-540B. ูููู ุจุฅุทูุงู ุฌููุน ููุงุฐุฌูุง ููุฌุชูุน ุงูุจุญุซ.*

ุชูุช ุงููุณุงููุฉ ุจูุฐุง ุงููููุฐุฌ ูู ูุจู [zphang](https://huggingface.co/zphang) ุจูุณุงููุงุช ูู [BlackSamorez](https://huggingface.co/BlackSamorez). ูุนุชูุฏ ููุฏ ุงูุชูููุฐ ูู Hugging Face ุนูู GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox). ูููู ุงูุนุซูุฑ ุนูู ุงูููุฏ ุงูุฃุตูู ูููุคูููู [here](https://github.com/facebookresearch/llama).

## ูุตุงุฆุญ ุงูุงุณุชุฎุฏุงู

- ูููู ุงูุญุตูู ุนูู ุฃูุฒุงู ููุงุฐุฌ LLaMA ูู ุฎูุงู ููุก [ูุฐุง ุงููููุฐุฌ](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)

- ุจุนุฏ ุชูุฒูู ุงูุฃูุฒุงูุ ุณูุชุนูู ุชุญููููุง ุฅูู ุชูุณูู Hugging Face Transformers ุจุงุณุชุฎุฏุงู [Script ุงูุชุญููู](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). ูููู ุงุณุชุฏุนุงุก ุงููุต ุงูุจุฑูุฌู ุจุงุณุชุฎุฏุงู ุงูุฃูุฑ ุงูุชุงูู (ููุซุงู):

```bash
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
--input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
```

- ุจุนุฏ ุงูุชุญูููุ ูููู ุชุญููู ุงููููุฐุฌ ููุญูู ุงูุฑููุฒ ุจุงุณุชุฎุฏุงู ูุง ููู:

```python
from transformers import LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained("/output/path")
model = LlamaForCausalLM.from_pretrained("/output/path")
```

ููุงุญุธุฉ: ูุชุทูุจ ุชูููุฐ ุงููุต ุงูุจุฑูุฌู ูุณุงุญุฉ ูุงููุฉ ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู CPU ูุงุณุชุถุงูุฉ ุงููููุฐุฌ ุจุงููุงูู ูู ุฏูุฉ float16 (ุญุชู ุฅุฐุง ูุงูุช ุงูุฅุตุฏุงุฑุงุช ุงูุฃูุจุฑ ุชุฃุชู ูู ุนุฏุฉ ููุงุท ูุฑุฌุนูุฉุ ูุฅู ูู ูููุง ูุญุชูู ุนูู ุฌุฒุก ูู ูู ูุฒู ูููููุฐุฌุ ูุฐูู ูุญุชุงุฌ ุฅูู ุชุญููููุง ุฌููุนูุง ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู). ุจุงููุณุจุฉ ูููููุฐุฌ 65Bุ ูุญุชุงุฌ ุฅูู 130 ุฌูุฌุงุจุงูุช ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู.

- ูุญูู ุฑููุฒ LLaMA ูู ูููุฐุฌ BPE ูุนุชูุฏ ุนูู [sentencepiece](https://github.com/google/sentencepiece). ุฅุญุฏู ููุฒุงุช sentencepiece ูู ุฃูู ุนูุฏ ูู ุชุดููุฑ ุชุณูุณูุ ุฅุฐุง ูุงู ุงูุฑูุฒ ุงูุฃูู ูู ุจุฏุงูุฉ ุงููููุฉ (ูุซู "Banana")ุ ูุฅู ุงููุญูู ุงููุบูู ูุง ูุณุจู ุงููุณุงูุฉ ุงูุจุงุฏุฆุฉ ุฅูู ุงูุณูุณูุฉ.

ุชูุช ุงููุณุงููุฉ ุจูุฐุง ุงููููุฐุฌ ูู ูุจู [zphang](https://huggingface.co/zphang) ุจูุณุงููุงุช ูู [BlackSamorez](https://huggingface.co/BlackSamorez). ูุนุชูุฏ ููุฏ ุงูุชูููุฐ ูู Hugging Face ุนูู GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox). ูููู ุงูุนุซูุฑ ุนูู ุงูููุฏ ุงูุฃุตูู ูููุคูููู [here](https://github.com/facebookresearch/llama). ุชู ุชูุฏูู ุฅุตุฏุงุฑ Flax ูู ุงูุชูููุฐ ูู ูุจู [afmck](https://huggingface.co/afmck) ูุน ุงูููุฏ ูู ุงูุชูููุฐ ุจูุงุกู ุนูู Flax GPT-Neo ูู Hugging Face.

ุจูุงุกู ุนูู ูููุฐุฌ LLaMA ุงูุฃุตููุ ุฃุตุฏุฑุช Meta AI ุจุนุถ ุงูุฃุนูุงู ุงููุงุญูุฉ:

- **Llama2**: Llama2 ูู ุฅุตุฏุงุฑ ูุญุณู ูู Llama ูุน ุจุนุถ ุงูุชุนุฏููุงุช ุงููุนูุงุฑูุฉ (Grouped Query Attention)ุ ููู ููุฏุฑุจ ูุณุจููุง ุนูู 2 ุชุฑููููู ุฑูุฒ. ุฑุงุฌุน ูุซุงุฆู Llama2 ุงูุชู ูููู ุงูุนุซูุฑ ุนูููุง [here](llama2).

## ุงูููุงุฑุฏ

ูุงุฆูุฉ ุจููุงุฑุฏ Hugging Face ุงูุฑุณููุฉ ูููุงุฑุฏ ุงููุฌุชูุน (ูุดุงุฑ ุฅูููุง ุจู ๐) ููุณุงุนุฏุชู ูู ุงูุจุฏุก ุจุงุณุชุฎุฏุงู LLaMA. ุฅุฐุง ููุช ููุชููุง ุจุชูุฏูู ููุฑุฏ ูุฅุฏุฑุงุฌู ููุงุ ููุฑุฌู ูุชุญ ุทูุจ ุณุญุจ ูุณูุฑุงุฌุนู! ูุฌุจ ุฃู ููุธูุฑ ุงูููุฑุฏ ุงููุซุงูู ุดูุฆูุง ุฌุฏูุฏูุง ุจุฏูุงู ูู ุชูุฑุงุฑ ููุฑุฏ ููุฌูุฏ.

<PipelineTag pipeline="text-classification"/>

- A [notebook](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb#scrollTo=f04ba4d2) on how to use prompt tuning to adapt the LLaMA model for text classification task. ๐

<PipelineTag pipeline="question-answering"/>

- [StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama#stackllama-a-hands-on-guide-to-train-llama-with-rlhf), a blog post about how to train LLaMA to answer questions on [Stack Exchange](https://stackexchange.com/) with RLHF.

โ๏ธ Optimization

- A [notebook](https://colab.research.google.com/drive/1SQUXq1AMZPSLD4mk3A3swUIc6Y2dclme?usp=sharing) on how to fine-tune LLaMA model using xturing library on GPU which has limited memory. ๐

โก๏ธ Inference

- A [notebook](https://colab.research.google.com/github/DominguesM/alpaca-lora-ptbr-7b/blob/main/notebooks/02%20-%20Evaluate.ipynb) on how to run the LLaMA Model using PeftModel from the ๐ค PEFT library. ๐

- A [notebook](https://colab.research.google.com/drive/1l2GiSSPbajVyp2Nk3CFT4t3uH6-5TiBe?usp=sharing) on how to load a PEFT adapter LLaMA model with LangChain. ๐

๐ Deploy

- A [notebook](https://colab.research.google.com/github/lxe/simple-llama-finetuner/blob/master/Simple_LLaMA_FineTuner.ipynb#scrollTo=3PM_DilAZD8T) on how to fine-tune LLaMA model using LoRA method via the ๐ค PEFT library with intuitive UI. ๐

- A [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb) on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. ๐

## LlamaConfig

[[autodoc]] LlamaConfig

## LlamaTokenizer

[[autodoc]] LlamaTokenizer

- build_inputs_with_special_tokens
- get_special_tokens_mask
- create_token_type_ids_from_sequences
- save_vocabulary

## LlamaTokenizerFast

[[autodoc]] LlamaTokenizerFast

- build_inputs_with_special_tokens
- get_special_tokens_mask
- create_token_type_ids_from_sequences
- update_post_processor
- save_vocabulary

## LlamaModel

[[autodoc]] LlamaModel

- forward

## LlamaForCausalLM

[[autodoc]] LlamaForCausalLM

- forward

## LlamaForSequenceClassification

[[autodoc]] LlamaForSequenceClassification

- forward

## LlamaForQuestionAnswering

[[autodoc]] LlamaForQuestionAnswering

- forward

## LlamaForTokenClassification

[[autodoc]] LlamaForTokenClassification

- forward

## FlaxLlamaModel

[[autodoc]] FlaxLlamaModel

- __call__

## FlaxLlamaForCausalLM

[[autodoc]] FlaxLlamaForCausalLM

- __call__