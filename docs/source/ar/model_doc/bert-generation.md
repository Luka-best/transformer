# BertGeneration

## نظرة عامة
يعد نموذج BertGeneration نموذجًا لـ BERT يمكن الاستفادة منه في مهام التسلسل إلى تسلسل باستخدام EncoderDecoderModel كما هو مقترح في "الاستفادة من نقاط التحقق المسبقة للتدريب لمهام توليد التسلسل" بواسطة Sascha Rothe وShashi Narayan وAliaksei Severyn.

مقتطف من الورقة البحثية هو كما يلي:

"غيرت الطرق غير الخاضعة للإشراف لتمهيد النماذج العصبية الكبيرة مؤخرًا طريقة معالجة اللغات الطبيعية. من خلال البدء من نقاط التحقق التي تم إصدارها للجمهور، قام ممارسو معالجة اللغات الطبيعية بتحسين الحالة الراهنة في العديد من المعايير مع توفير كميات كبيرة من وقت الحوسبة. حتى الآن، كان التركيز بشكل أساسي على مهام فهم اللغة الطبيعية. في هذه الورقة، نثبت فعالية نقاط التحقق المسبقة لتوليد التسلسل. لقد قمنا بتطوير نموذج تسلسل إلى تسلسل يعتمد على المحول متوافق مع نقاط التحقق المسبقة المتاحة للجمهور BERT وGPT-2 وRoBERTa، وأجرينا دراسة تجريبية شاملة حول فائدة تهيئة نموذجنا، كل من المشفر وفك التشفير، مع هذه نقاط التحقق. تحقق نماذجنا نتائج جديدة في حالة الفن في الترجمة الآلية، وملخص النص، وتقسيم الجمل، واندماج الجمل."

تمت المساهمة بهذا النموذج من قبل [باتريك فون بلاتين]. يمكن العثور على الكود الأصلي [هنا].

## أمثلة الاستخدام والنصائح

يمكن استخدام النموذج بالاشتراك مع EncoderDecoderModel للاستفادة من نقطتي تفتيش BERT مسبقة التدريب للضبط الدقيق اللاحق:

يمكن أيضًا العثور على EncoderDecoderModel مسبقة التدريب مباشرة في مركز النماذج، على سبيل المثال:

النصائح:

- يجب استخدام [BertGenerationEncoder] و [BertGenerationDecoder] بالاشتراك مع [EncoderDecoder].
- بالنسبة للتلخيص وتقسيم الجمل ودمج الجمل والترجمة، لا يلزم وجود رموز خاصة للإدخال. لذلك، لا يجب إضافة رمز EOS إلى نهاية الإدخال.

## BertGenerationConfig

## BertGenerationTokenizer

- save_vocabulary

## BertGenerationEncoder

## BertGenerationDecoder