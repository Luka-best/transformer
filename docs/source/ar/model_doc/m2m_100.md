# M2M100  

## نظرة عامة
اقترح نموذج M2M100 في ورقة بحثية بعنوان "Beyond English-Centric Multilingual Machine Translation" (ما بعد الترجمة الآلية متعددة اللغات التي تركز على اللغة الإنجليزية) من قبل أنجيلا فان، وشروتي باهوسالي، وهولجر شونك، وزيهي ما، وأحمد الكشكي، وسيدارث جويال، ومنديپ باينس، وأونور شيلبي، وجيوم وينزيك، وفيشراف تشاوداري، ونيمان جويال، وتوم بيرش، وفيتالي ليبتشينسكي، وسيرجي إدونوف، وإدوارد جريف، ومايكل أولي، وأرماند جولين.

ملخص الورقة البحثية هو كما يلي:

أظهر العمل الحالي في الترجمة الآلية الإمكانات الهائلة للترجمة الآلية متعددة اللغات من خلال تدريب نموذج واحد قادر على الترجمة بين أي زوج من اللغات. ومع ذلك، يركز الكثير من هذا العمل على اللغة الإنجليزية من خلال التدريب فقط على البيانات التي تُرجمت من اللغة الإنجليزية أو إليها. وعلى الرغم من دعم هذا النهج بمصادر كبيرة من بيانات التدريب، إلا أنه لا يعكس احتياجات الترجمة في جميع أنحاء العالم. وفي هذا العمل، نقوم بإنشاء نموذج ترجمة حقيقي متعدد اللغات للعديد من اللغات يمكنه الترجمة المباشرة بين أي زوج من 100 لغة. نقوم ببناء ومشاركة مجموعة بيانات تدريب مفتوحة المصدر تغطي آلاف اتجاهات اللغة بالبيانات الخاضعة للإشراف، والتي تم إنشاؤها من خلال التنقيب على نطاق واسع. بعد ذلك، نستكشف كيفية زيادة سعة النموذج بفعالية من خلال الجمع بين النطاق الكثيف والمعلمات النادرة الخاصة باللغة لإنشاء نماذج عالية الجودة. ويركز عملنا على النماذج غير المركزة على اللغة الإنجليزية، مما يحقق مكاسب تزيد عن 10 نقاط BLEU عند الترجمة المباشرة بين الاتجاهات غير الإنجليزية، مع أداء تنافسي مقارنة بأفضل الأنظمة الفردية في WMT. ونقوم بمشاركة برامجنا مفتوحة المصدر حتى يتمكن الآخرون من استنساخ البيانات والتقييم والنموذج النهائي M2M-100.

تمت المساهمة بهذا النموذج من قبل [valhalla](https://huggingface.co/valhalla).

## نصائح الاستخدام وأمثلة

M2M100 هو نموذج متعدد اللغات للترميز فك الترميز (الترتيب التسلسلي) المقصود في المقام الأول لمهام الترجمة. نظرًا لأن النموذج متعدد اللغات، فإنه يتوقع تسلسلًا بتنسيق معين: يتم استخدام رمز معرف اللغة الخاص كبادئة في كل من النص المصدر والنص المستهدف. تنسيق النص المصدر هو `[lang_code] X [eos]`، حيث `lang_code` هو معرف لغة المصدر للنص المصدر ومعرف لغة الهدف للنص المستهدف، و`X` هو نص المصدر أو الهدف.

يعتمد [`M2M100Tokenizer`] على `sentencepiece`، لذلك تأكد من تثبيته قبل تشغيل الأمثلة. لتثبيت `sentencepiece`، قم بتشغيل `pip install sentencepiece`.

**التدريب الخاضع للإشراف**

```python
from transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer

model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="en", tgt_lang="fr")

src_text = "Life is like a box of chocolates."
tgt_text = "La vie est comme une boîte de chocolat."

model_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors="pt")

loss = model(**model_inputs).loss  # forward pass
```

**التوليد**

يستخدم M2M100 `eos_token_id` كـ `decoder_start_token_id` للتوليد، مع فرض معرف لغة الهدف كأول رمز منشأ. لإجبار معرف لغة الهدف كأول رمز منشأ، قم بتمرير المعلمة *forced_bos_token_id* إلى طريقة *generate*. يوضح المثال التالي كيفية الترجمة بين الهندية والفرنسية والصينية والإنجليزية باستخدام نقطة تفتيش *facebook/m2m100_418M*.

```python
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> hi_text = "जीवन एक चॉकलेट बॉक्स की तरह है।"
>>> chinese_text = "生活就像一盒巧克力。"

>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")

>>> # translate Hindi to French
>>> tokenizer.src_lang = "hi"
>>> encoded_hi = tokenizer(hi_text, return_tensors="pt")
>>> generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id("fr"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"La vie est comme une boîte de chocolat."

>>> # translate Chinese to English
>>> tokenizer.src_lang = "zh"
>>> encoded_zh = tokenizer(chinese_text, return_tensors="pt")
>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"Life is like a box of chocolate."
```

## الموارد

- [دليل مهام الترجمة](../tasks/translation)
- [دليل مهام الملخص](../tasks/summarization)

## M2M100Config

[[autodoc]] M2M100Config

## M2M100Tokenizer

[[autodoc]] M2M100Tokenizer

- build_inputs_with_special_tokens
- get_special_tokens_mask
- create_token_type_ids_from_sequences
- save_vocabulary

## M2M100Model

[[autodoc]] M2M100Model

- forward

## M2M100ForConditionalGeneration

[[autodoc]] M2M100ForConditionalGeneration

- forward

## استخدام Flash Attention 2

Flash Attention 2 هو إصدار أسرع وأكثر تحسينًا لحساب درجات الاهتمام والتي تعتمد على نوى `cuda`.

### التثبيت

أولاً، تحقق مما إذا كان عتادك متوافقًا مع Flash Attention 2. يمكن العثور على أحدث قائمة للأجهزة المتوافقة في [الوثائق الرسمية](https://github.com/Dao-AILab/flash-attention#installation-and-features).

بعد ذلك، قم بتثبيت أحدث إصدار من Flash Attention 2:

```bash
pip install -U flash-attn --no-build-isolation
```

### الاستخدام

لتحميل نموذج يستخدم Flash Attention 2، يمكننا تمرير الحجة `attn_implementation="flash_attention_2"` إلى طريقة [`.from_pretrained`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained). يمكنك استخدام دقة إما `torch.float16` أو `torch.bfloat16`.

```python
>>> import torch
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M", torch_dtype=torch.float16, attn_implementation="flash_attention_2").to("cuda").eval()
>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")

>>> # translate Hindi to French
>>> hi_text = "जीवन एक चॉकलेट बॉक्स की तरह है।"
>>> tokenizer.src_lang = "hi"
>>> encoded_hi = tokenizer(hi_text, return_tensors="pt").to("cuda")
>>> generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id("fr"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"La vie est comme une boîte de chocolat."
```

### تسريع الأداء المتوقع

فيما يلي رسم بياني للتسريع المتوقع الذي يقارن وقت الاستدلال النقي بين التنفيذ الأصلي وFlash Attention 2.

<div style="text-align: center">
<img src="https://huggingface.co/datasets/visheratin/documentation-images/resolve/main/nllb-speedup.webp">
</div>