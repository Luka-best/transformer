# MegatronGPT2

## نظرة عامة

اقترح نموذج MegatronGPT2 في ورقة بحثية بعنوان "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism" بواسطة Mohammad Shoeybi وآخرون.

يوضح الملخص المستخرج من الورقة البحثية ما يلي:

أظهرت الأعمال الحديثة في نمذجة اللغة أن تدريب نماذج المحول الكبيرة تطور حالة الفن في تطبيقات معالجة اللغة الطبيعية. ومع ذلك، يمكن أن تكون النماذج الكبيرة جدًا صعبة التدريب بسبب قيود الذاكرة. في هذا العمل، نقدم تقنياتنا لتدريب نماذج المحول الكبيرة للغاية وننفذ نهجًا موازيًا بسيطًا وفعالًا داخل الطبقة لتمكين تدريب نماذج المحول مع مليارات من المعلمات. لا يتطلب نهجنا مترجمًا أو تغييرات مكتبية جديدة، وهو متعامد ومكمل لموازاة نموذج الأنابيب، ويمكن تنفيذه بالكامل بإدراج بعض عمليات الاتصال في PyTorch الأصلي. نوضح هذا النهج عن طريق تحويل النماذج المستندة إلى المحول حتى 8.3 مليار معلمة باستخدام 512 وحدة معالجة الرسومات (GPU). نحافظ على 15.1 PetaFLOPs عبر التطبيق بالكامل بكفاءة تحجيم تبلغ 76% عند مقارنتها بخط أساس GPU واحد قوي يحافظ على 39 TeraFLOPs، وهو 30% من ذروة FLOPs. ولإثبات أن نماذج اللغة الكبيرة يمكن أن تواصل تطوير حالة الفن، نقوم بتدريب نموذج لغة محول 8.3 مليار معلمة مشابه لـ GPT-2 ونموذج 3.9 مليار معلمة مشابه لـ BERT. نُظهر أن الاهتمام الدقيق بوضع طبقة التطبيع في النماذج الشبيهة بـ BERT أمر بالغ الأهمية لتحقيق أداء متزايد مع نمو حجم النموذج. باستخدام نموذج GPT-2، نحقق نتائج حالة أفضل على مجموعات بيانات WikiText103 (10.8 مقارنة بدرجة تعقيد 15.8) و LAMBADA (66.5% مقارنة بدقة 63.2%) . يحقق نموذج BERT الخاص بنا نتائج حالة أفضل في مجموعة بيانات RACE (90.9% مقارنة بدقة 89.4%).

ساهم بهذا النموذج [jdemouth] (https://huggingface.co/jdemouth). يمكن العثور على الكود الأصلي [هنا] (https://github.com/NVIDIA/Megatron-LM). يحتوي هذا المستودع على تنفيذ متعدد وحدات معالجة الرسومات (GPU) ومتعدد العقد لنماذج Megatron Language. على وجه الخصوص، يحتوي على نهج موازاة نموذج مختلط باستخدام تقنيات "tensor parallel" و "pipeline parallel".

## نصائح الاستخدام

قمنا بتوفير نقاط التحقق المسبقة التدريب لـ [GPT2-345M] (https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m) للاستخدام في تقييم المهام أو الضبط الدقيق للمهام اللاحقة.

للوصول إلى نقاط التحقق هذه، قم أولاً بالتسجيل في NVIDIA GPU Cloud (NGC) وإعداد CLI لسجل NGC. يمكن العثور على مزيد من الوثائق حول تنزيل النماذج في [وثائق NGC] (https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).

أو يمكنك تنزيل نقاط التحقق مباشرة باستخدام ما يلي:

```bash
wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O
megatron_gpt2_345m_v0_0.zip
```

بمجرد حصولك على نقطة التحقق من NVIDIA GPU Cloud (NGC)، يجب عليك تحويلها إلى تنسيق يمكن تحميله بسهولة بواسطة تنفيذ Hugging Face Transformers GPT2.

يسمح الأمر التالي بإجراء التحويل. نفترض أن المجلد "models/megatron_gpt2" يحتوي على "megatron_gpt2_345m_v0_0.zip" وأن الأمر يتم تشغيله من هذا المجلد:

```bash
python3 $PATH_TO_TRANSFORMERS/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py megatron_gpt2_345m_v0_0.zip
```

<Tip>

تتشابه بنية MegatronGPT2 مع بنية OpenAI GPT-2. راجع [وثائق GPT-2] (gpt2) للحصول على معلومات حول فئات التكوين ومعلماتها.

</Tip>