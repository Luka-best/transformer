# XLSR-Wav2Vec2

## نظرة عامة

تم اقتراح نموذج XLSR-Wav2Vec2 في ورقة "Unsupervised Cross-Lingual Representation Learning For Speech Recognition" بواسطة Alexis Conneau و Alexei Baevski و Ronan Collobert و Abdelrahman Mohamed و Michael Auli.

ملخص الورقة هو كما يلي:

*تقدم هذه الورقة XLSR الذي يتعلم تمثيلات كلامية متعددة اللغات عن طريق التدريب المسبق لنموذج واحد من الموجات الصوتية الخام للكلام بلغات متعددة. نعتمد على wav2vec 2.0 الذي يتم تدريبه عن طريق حل مهمة تمييزية على تمثيلات الكلام الكامنة المقنعة ويتعلم في نفس الوقت تمييزًا كميًا للكامنات المشتركة عبر اللغات. يتم ضبط النموذج الناتج على بيانات موسومة، وتظهر التجارب أن التدريب المتقاطع للغات يفوق بشكل كبير التدريب أحادي اللغة. وعلى معيار CommonVoice، يظهر XLSR انخفاضًا نسبيًا في معدل خطأ المقاطع الصوتية يبلغ 72% مقارنة بأفضل النتائج المعروفة. وعلى مجموعة بيانات BABEL، يحسن نهجنا معدل خطأ الكلمات بنسبة 16% مقارنة بنظام مماثل. يمكّن نهجنا نموذجًا واحدًا متعدد اللغات للتعرف على الكلام قادرًا على منافسة النماذج الفردية القوية. ويبين التحليل أن تمثيلات الكلام الكامنة والمنفصلة مشتركة عبر اللغات مع زيادة المشاركة للغات ذات الصلة. ونأمل تحفيز البحث في فهم الكلام منخفض الموارد من خلال إطلاق XLSR-53، وهو نموذج كبير تم تدريبه المسبق على 53 لغة.*

يمكن العثور على الكود الأصلي [هنا](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec).

ملاحظة: أصدرت Meta (FAIR) إصدارًا جديدًا من [Wav2Vec2-BERT 2.0](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2-bert) - تم التدريب المسبق عليه على 4.5 مليون ساعة من الصوت. نوصي خاصة باستخدامه لمهام الضبط الدقيق، على سبيل المثال كما هو موضح في [هذا الدليل](https://huggingface.co/blog/fine-tune-w2v2-bert).

## نصائح الاستخدام

- XLSR-Wav2Vec2 هو نموذج كلامي يقبل مصفوفة أرقام عشرية مطابقة للموجة الصوتية الخام لإشارة الكلام.

- تم تدريب نموذج XLSR-Wav2Vec2 باستخدام التصنيف الزمني للاتصال (CTC)، لذلك يجب فك تشفير إخراج النموذج باستخدام [`Wav2Vec2CTCTokenizer`].

<Tip>

تستند بنية XLSR-Wav2Vec2 إلى نموذج Wav2Vec2، لذلك يمكن الرجوع إلى [صفحة وثائق Wav2Vec2](wav2vec2).

</Tip>