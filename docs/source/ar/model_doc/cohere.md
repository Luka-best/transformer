# Cohere

## ูุธุฑุฉ ุนุงูุฉ
ุงูุชุฑุญ ูุฑูู Cohere ูููุฐุฌ Cohere Command-R ูู ุงูููุดูุฑ ุนูู ุงููุฏููุฉ [Command-R: Retrieval Augmented Generation at Production Scale](https://txt.cohere.com/command-r/) ุจูุงุณุทุฉ ูุฑูู Cohere.

ุงูููุชุทู ูู ุงููุฑูุฉ ูู ูุง ููู:

> "Command-R ูู ูููุฐุฌ ุชูููุฏู ูุงุจู ููุชุทููุฑ ูุณุชูุฏู RAG ูุงุณุชุฎุฏุงู ุงูุฃุฏูุงุช ูุชูููู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุนูู ูุทุงู ุงูุฅูุชุงุฌ ูููุคุณุณุงุช. ูุงููููุ ููุฏู Command-Rุ ููู ูููุฐุฌ LLM ุฌุฏูุฏ ูุณุชูุฏู ุฃุนุจุงุก ุงูุนูู ุงูุฅูุชุงุฌูุฉ ูุงุณุนุฉ ุงููุทุงู. ููุณุชูุฏู Command-R ูุฆุฉ "ูุงุจูุฉ ููุชุทููุฑ" ุงููุงุดุฆุฉ ูู ุงูููุงุฐุฌ ุงูุชู ุชูุงุฒู ุจูู ุงูููุงุกุฉ ุงูุนุงููุฉ ูุงูุฏูุฉ ุงููููุฉุ ููุง ููููู ุงูุดุฑูุงุช ูู ุชุฌุงูุฒ ููููู ุงูุฅุซุจุงุชุ ูุงูุงูุชูุงู ุฅูู ุงูุฅูุชุงุฌ."

> "Command-R ูู ูููุฐุฌ ุชูููุฏู ููุญูุณูู ูููุงู ุงูุณูุงู ุงูุทูููุฉ ูุซู ุงุณุชุฑุฌุงุน ุงูุชูููุฏ ุงููุนุฒุฒ (RAG) ูุงุณุชุฎุฏุงู ูุงุฌูุงุช ุจุฑูุฌุฉ ุงูุชุทุจููุงุช ูุงูุฃุฏูุงุช ุงูุฎุงุฑุฌูุฉ. ููุฏ ุตููู ููุนูู ุจุงูุชูุณูู ูุน ููุงุฐุฌ Embed ูRerank ุงูุฑุงุฆุฏุฉ ูู ุงูุตูุงุนุฉ ูุชูููุฑ ุฃูุถู ุชูุงูู ูุชุทุจููุงุช RAG ูุงูุชููุฒ ูู ุญุงูุงุช ุงุณุชุฎุฏุงู ุงููุคุณุณุงุช. ูุจุงุนุชุจุงุฑู ูููุฐุฌูุง ููุตูููุง ููุดุฑูุงุช ูุชูููุฐู ุนูู ูุทุงู ูุงุณุนุ ูุชูุงุฎุฑ Command-R ุจูุง ููู:

- ุฏูุฉ ูููุฉ ูู RAG ูุงุณุชุฎุฏุงู ุงูุฃุฏูุงุช
- ุงูุฎูุงุถ ุฒูู ุงููุตููุ ูุณุฑุนุฉ ุนุงููุฉ ูู ูุนุงูุฌุฉ ุงูุจูุงูุงุช
- ุณูุงู ุฃุทูู ูุจูุบ 128 ูููู ุจุงูุช ูุชูููุฉ ุฃูู
- ูุฏุฑุงุช ูููุฉ ุนุจุฑ 10 ูุบุงุช ุฑุฆูุณูุฉ
- ุฃูุฒุงู ุงููููุฐุฌ ูุชุงุญุฉ ุนูู HuggingFace ููุจุญุซ ูุงูุชูููู

ุชููุฏ ููุงุท ุชูุชูุด ุงููููุฐุฌ [ููุง](https://huggingface.co/CohereForAI/c4ai-command-r-v01).

ุณุงูู ุจูุฐุง ุงููููุฐุฌ [Saurabh Dash](https://huggingface.co/saurabhdash) ู [Ahmet รstรผn](https://huggingface.co/ahmetustun). ูุชุณุชูุฏ ุดูุฑุฉ ุงูุชูููุฐ ูู Hugging Face ุฅูู GPT-NeoX [ููุง](https://github.com/EleutherAI/gpt-neox).

## ูุตุงุฆุญ ุงูุงุณุชุฎุฏุงู

<Tip warning={true}>

ุชุณุชุฎุฏู ููุงุท ุงูุชูุชูุด ุงููุฑููุนุฉ ุนูู ุงููุญุงูุฑ `torch_dtype = 'float16'`ุ ูุงูุชู ุณูุชู ุงุณุชุฎุฏุงููุง ุจูุงุณุทุฉ ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช `AutoModel` ูุชุญููู ููุงุท ุงูุชูุชูุด ูู `torch.float32` ุฅูู `torch.float16`.

ุฅู ููุน ุจูุงูุงุช ุงูุฃูุฒุงู ุนุจุฑ ุงูุฅูุชุฑูุช ุบูุฑ ุฐู ุตูุฉ ุฅูู ุญุฏ ูุจูุฑ ูุง ูู ุชูู ุชุณุชุฎุฏู `torch_dtype="auto"` ุนูุฏ ุชููุฆุฉ ูููุฐุฌ ุจุงุณุชุฎุฏุงู `model = AutoModelForCausalLM.from_pretrained("path"ุ torch_dtype = "auto")`. ูุงูุณุจุจ ูู ุฃู ุงููููุฐุฌ ุณูุชู ุชูุฒููู ุฃููุงู (ุจุงุณุชุฎุฏุงู ููุน ุจูุงูุงุช ููุงุท ุงูุชูุชูุด ุนุจุฑ ุงูุฅูุชุฑูุช)ุ ุซู ุณูุชู ุชุญูููู ุฅูู ููุน ุจูุงูุงุช ุงูุงูุชุฑุงุถู ูู `torch` (ูุตุจุญ `torch.float32`)ุ ูุฃุฎูุฑุงูุ ุฅุฐุง ูุงู ููุงู `torch_dtype` ููุฏู ูู ุงูุชููููุ ูุณูุชู ุงุณุชุฎุฏุงูู.

ูุง ูููุตุญ ุจุงูุชุฏุฑูุจ ุนูู ุงููููุฐุฌ ูู `float16` ููู ุงููุนุฑูู ุฃูู ููุชุฌ ุนูู `nan`ุ ูุฐููุ ูุฌุจ ุชุฏุฑูุจ ุงููููุฐุฌ ูู `bfloat16`.

</Tip>

ูููู ุชุญููู ุงููููุฐุฌ ูุงููุญูู ุงููุบูู ุจุงุณุชุฎุฏุงู ูุง ููู:

```python
# pip install transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "CohereForAI/c4ai-command-r-v01"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# ุชูุณูู ุงูุฑุณุงูุฉ ุจุงุณุชุฎุฏุงู ูุงูุจ ุงูุฏุฑุฏุดุฉ command-r
messages = [{"role": "user", "content": "Hello, how are you?"}]
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
## \<BOS_TOKEN\>\<\|START_OF_TURN_TOKEN\|\>\<\|USER_TOKEN\|\>Hello, how are you?\<\|END_OF_TURN_TOKEN\|\>\<\|START_OF_TURN_TOKEN\|\>\<\|CHATBOT_TOKEN\|\>

gen_tokens = model.generate(
    input_ids,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
)

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```

- ุนูุฏ ุงุณุชุฎุฏุงู Flash Attention 2 ุนุจุฑ `attn_implementation="flash_attention_2"`ุ ูุง ุชูุฑุฑ `torch_dtype` ุฅูู ุทุฑููุฉ ุงููุตู `from_pretrained` ูุงุณุชุฎุฏู ุงูุชุฏุฑูุจ ุนูู ุงูุฏูุฉ ุงููุฎุชูุทุฉ ุงูุชููุงุฆูุฉ. ุนูุฏ ุงุณุชุฎุฏุงู `Trainer`ุ ููู ุจุจุณุงุทุฉ ุชุญุฏูุฏ ุฅูุง `fp16` ุฃู `bf16` ุฅูู `True`. ูุฅูุงุ ุชุฃูุฏ ูู ุงุณุชุฎุฏุงูู ูู `torch.autocast`. ููุฐุง ูุทููุจ ูุฃู Flash Attention ูุฏุนู ููุท ููุนู ุงูุจูุงูุงุช `fp16` ู`bf16`.

## ุงูููุงุฑุฏ

ูููุง ููู ูุงุฆูุฉ ุจููุงุฑุฏ Hugging Face ุงูุฑุณููุฉ ูููุงุฑุฏ ุงููุฌุชูุน (ูุดุงุฑ ุฅูููุง ุจุงูุนุงูู ๐) ููุณุงุนุฏุชู ูู ุงูุจุฏุก ุจุงุณุชุฎุฏุงู Command-R. ุฅุฐุง ููุช ููุชููุง ุจุชูุฏูู ููุฑุฏ ูุฅุฏุฑุงุฌู ููุงุ ูุงูุฑุฌุงุก ูุชุญ ุทูุจ ุณุญุจ ูุณูููู ุจูุฑุงุฌุนุชู! ูููุถู ุฃู ููุธูุฑ ุงูููุฑุฏ ุดูุฆูุง ุฌุฏูุฏูุง ุจุฏูุงู ูู ุชูุฑุงุฑ ููุฑุฏ ููุฌูุฏ.

<PipelineTag pipeline="text-generation"/>

ุชุญููู ูููุฐุฌ FP16

```python
# pip install transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "CohereForAI/c4ai-command-r-v01"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# ุชูุณูู ุงูุฑุณุงูุฉ ุจุงุณุชุฎุฏุงู ูุงูุจ ุงูุฏุฑุฏุดุฉ command-r
messages = [{"role": "user", "content": "Hello, how are you?"}]
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
## \<BOS_TOKEN\>\<\|START_OF_TURN_TOKEN\|\>\<\|USER_TOKEN\|\>Hello, how are you?\<\|END_OF_TURN_TOKEN\|\>\<\|START_OF_TURN_TOKEN\|\>\<\|CHATBOT_TOKEN\|\>

gen_tokens = model.generate(
    input_ids,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
)

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```

ุชุญููู ูููุฐุฌ bitsnbytes 4bit ุงูููู

```python
# pip install transformers bitsandbytes accelerate
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_4bit=True)

model_id = "CohereForAI/c4ai-command-r-v01"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)

gen_tokens = model.generate(
    input_ids,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
)

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```

## CohereConfig

[[autodoc]] CohereConfig

## CohereTokenizerFast

[[autodoc]] CohereTokenizerFast

- build_inputs_with_special_tokens
- get_special_tokens_mask
- create_token_type_ids_from_sequences
- update_post_processor
- save_vocabulary

## CohereModel

[[autodoc]] CohereModel

- forward

## CohereForCausalLM

[[autodoc]] CohereForCausalLM

- forward