# T5

<div class="flex flex-wrap space-x-1">
<a href="https://huggingface.co/models?filter=t5">
<img alt="Models" src="https://img.shields.io/badge/All_model_pages-t5-blueviolet">
</a>
<a href="https://huggingface.co/spaces/docs-demos/t5-base">
<img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue">
</a>
<a href="https://huggingface.co/papers/1910.10683">
<img alt="Paper page" src="https://img.shields.io/badge/Paper%20page-1910.10683-green">
</a>
</div>

## نظرة عامة
تم تقديم نموذج T5 في بحث [استكشاف حدود تعلم النقل مع محول النص الموحد إلى النص](https://arxiv.org/pdf/1910.10683.pdf) بواسطة [كولن رافيل](https://huggingface.co/craffel)، ونوام شازير، و[آدم روبرتس](https://huggingface.co/adarob)، وكاثرين لي، وشاران نارانج،
مايكل ماتينا، ويانكي تشو، ووي لي، و[بيتر جيه ليو](https://huggingface.co/peterjliu).

ملخص البحث هو التالي:

> "أصبح تعلم النقل، حيث يتم تدريب النموذج مسبقًا أولاً على مهمة غنية بالبيانات قبل ضبط دقته على مهمة أسفل البث، تقنية قوية في معالجة اللغات الطبيعية (NLP). وقد أدت فعالية تعلم النقل إلى ظهور مجموعة متنوعة من الأساليب والمنهجيات والممارسات. في هذه الورقة، نستكشف مجال تقنيات تعلم النقل لمعالجة اللغات الطبيعية من خلال تقديم إطار عمل موحد يحول كل مشكلة لغوية إلى تنسيق نصي. تقارن دراستنا المنهجية أهداف التدريب المسبق والبنيات والمجموعات غير المعلمة ونهج النقل وعوامل أخرى في عشرات مهام فهم اللغة. من خلال الجمع بين الأفكار المستفادة من استكشافنا مع النطاق و"مجموعتنا الضخمة النظيفة المستخرجة من الويب"، نحقق نتائج متقدمة في العديد من المعايير المرجعية التي تغطي الملخص، والأسئلة والأجوبة، وتصنيف النصوص، والمزيد. ولتسهيل العمل المستقبلي في تعلم النقل لمعالجة اللغات الطبيعية، نقوم بإطلاق مجموعتنا وطرازنا المُدرب مسبقًا وشفرة البرنامج."

يمكن العثور على جميع نقاط التفتيش على [المركز](https://huggingface.co/models?search=t5).

تمت المساهمة بهذا النموذج من قبل [thomwolf](https://huggingface.co/thomwolf). يمكن العثور على الشفرة الأصلية [هنا](https://github.com/google-research/text-to-text-transfer-transformer).

## نصائح الاستخدام

- T5 هو نموذج ترميز فك ترميز مُدرب مسبقًا على مزيج من المهام متعددة المهام غير الخاضعة للإشراف والخاضعة للإشراف، ويتم تحويل كل مهمة منها إلى تنسيق نصي. يعمل T5 بشكل جيد في مجموعة متنوعة من المهام خارج الصندوق عن طريق إضافة بادئة مختلفة إلى الإدخال المقابل لكل مهمة، على سبيل المثال، للترجمة: *ترجمة من الإنجليزية إلى الألمانية: ...*،
للتلخيص: *تلخيص: ...*.

- يتضمن التدريب المسبق كلًا من التدريب الخاضع للإشراف والتدريب الذاتي الخاضع للإشراف. يتم إجراء التدريب الخاضع للإشراف على المهام الأساسية بواسطة معايير GLUE وSuperGLUE (عن طريق تحويلها إلى مهام نصية كما هو موضح أعلاه).

- يستخدم التدريب الذاتي الخاضع للإشراف الرموز المميزة المعطلة، عن طريق إزالة 15% من الرموز بشكل عشوائي واستبدالها برموز مميزة فردية (إذا تم وضع علامة على عدة رموز متتالية للإزالة، يتم استبدال المجموعة بأكملها برمز مميز واحد). إدخال الترميز هو الجملة المعطلة، وإدخال فك الترميز هو الجملة الأصلية والهدف هو الرموز المميزة التي تم إسقاطها محددة برموزها المميزة.

- يستخدم T5 تضمينات قياسية نسبية. يمكن إجراء ترميز الإدخال الفارغ على اليسار وعلى اليمين.

- راجع أقسام [التدريب](#training)، و[الاستنتاج](#inference)، و[الموارد](#resources) أدناه للحصول على جميع التفاصيل المتعلقة بالاستخدام.

يأتي T5 بأحجام مختلفة:

- [google-t5/t5-small](https://huggingface.co/google-t5/t5-small)
- [google-t5/t5-base](https://huggingface.co/google-t5/t5-base)
- [google-t5/t5-large](https://huggingface.co/google-t5/t5-large)
- [google-t5/t5-3b](https://huggingface.co/google-t5/t5-3b)
- [google-t5/t5-11b](https://huggingface.co/google-t5/t5-11b).

بناءً على نموذج T5 الأصلي، أصدرت Google بعض الأعمال اللاحقة:

- **T5v1.1**: T5v1.1 هو إصدار محسن من T5 مع بعض التعديلات المعمارية، وهو مُدرب مسبقًا على C4 فقط دون
مزج المهام الخاضعة للإشراف. راجع وثائق T5v1.1 الموجودة [هنا](t5v1.1).

- **mT5**: mT5 هو نموذج T5 متعدد اللغات. وهو مُدرب مسبقًا على مجموعة بيانات mC4، والتي تشمل 101 لغة. راجع
وثائق mT5 الموجودة [هنا](mt5).

- **byT5**: byT5 هو نموذج T5 مُدرب مسبقًا على تسلسلات البايت بدلاً من تسلسلات الرموز الفرعية SentencePiece. راجع
وثائق byT5 الموجودة [هنا](byt5).

- **UL2**: UL2 هو نموذج مشابه لـ T5 مُدرب مسبقًا على أهداف إزالة الضوضاء المختلفة

- **Flan-T5**: Flan هي طريقة تدريب مسبق تعتمد على المطالبات. Flan-T5 هي نماذج T5 مدربة على مجموعة Flan من
مجموعات البيانات التي تشمل: `taskmaster2`، `djaym7/wiki_dialog`، `deepmind/code_contests`، `lambada`، `gsm8k`، `aqua_rat`، `esnli`، `quasc`، و`qed`.

- **FLan-UL2**: نموذج UL2 مضبوط الدقة باستخدام ضبط دقة المطالبة "Flan" ومجموعة البيانات.

- **UMT5**: UmT5 هو نموذج T5 متعدد اللغات مُدرب على مجموعة mC4 متعددة اللغات المحسنة والمحدثة، 29 تريليون حرف عبر 107 لغة، باستخدام طريقة أخذ العينات الجديدة، UniMax. راجع
وثائق mT5 الموجودة [هنا](umt5).
## التدريب

T5 هو نموذج ترميز وفك ترميز يحول جميع مشكلات معالجة اللغات الطبيعية إلى تنسيق نص إلى نص. يتم تدريبه باستخدام طريقة "teacher forcing". وهذا يعني أنه بالنسبة للتدريب، نحتاج دائمًا إلى تسلسل إدخال وتسلسل هدف مطابق. يتم تغذية تسلسل الإدخال إلى النموذج باستخدام "input_ids". يتم تحويل تسلسل الهدف إلى اليمين، أي يتم إضافة رمز تسلسل البداية في البداية ويتم تغذيته إلى فك الترميز باستخدام "decoder_input_ids". وفي أسلوب "teacher forcing"، يتم بعد ذلك إضافة رمز "EOS" إلى تسلسل الهدف، والذي يتوافق مع "labels". ويتم استخدام رمز "PAD" هنا كرموز تسلسل البداية. يمكن تدريب T5 / ضبطه الدقيق بأسلوب إشرافي وغير إشرافي على حد سواء.

يمكنك استخدام [T5ForConditionalGeneration] (أو المتغير Tensorflow/Flax)، والذي يتضمن رأس نمذجة اللغة أعلى فك الترميز.

- التدريب غير الإشرافي لإزالة التشويش

في هذا الإعداد، يتم قناع أجزاء من تسلسل الإدخال بواسطة ما يسمى رموز الاستطلاع (أو رموز القناع الفريدة) ويتم تشكيل تسلسل الإخراج كتوصيل لرموز الاستطلاع نفسها ورموز القناع "الحقيقية". يمثل كل رمز استطلاع رمز قناع فريد لهذه الجملة ويجب أن يبدأ بـ "<extra_id_0>"، "<extra_id_1>"، ... حتى "<extra_id_99>". كافتراضي، تتوفر 100 رموز استطلاع في [T5Tokenizer].

على سبيل المثال، يجب معالجة الجملة "The cute dog walks in the park" مع الأقنعة الموضوعة على "cute dog" و "the" على النحو التالي:

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids
>>> labels = tokenizer("<extra_id_0> cute dog <extra_id_1> the <extra_id_2>", return_tensors="pt").input_ids

>>> # تقوم دالة التقديم تلقائيًا بإنشاء decoder_input_ids الصحيحة
>>> loss = model(input_ids=input_ids, labels=labels).loss
>>> loss.item()
3.7837
```

إذا كنت مهتمًا بالتدريب المسبق لـ T5 على مجموعة بيانات جديدة، تحقق من [run_t5_mlm_flax.py](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling) script في دليل الأمثلة.

- التدريب الإشرافي

في هذا الإعداد، يكون تسلسل الإدخال والإخراج عبارة عن رسم خرائط قياسي للإدخال والإخراج تسلسل إلى تسلسل. لنفترض أننا نريد ضبط النموذج الدقيق للترجمة على سبيل المثال، ولدينا مثال تدريب: تسلسل الإدخال "The house is wonderful." وتسلسل الإخراج "Das Haus ist wunderbar."، فيجب إعدادهما للنموذج على النحو التالي:

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
>>> labels = tokenizer("Das Haus ist wunderbar.", return_tensors="pt").input_ids

>>> # تقوم دالة التقديم تلقائيًا بإنشاء decoder_input_ids الصحيحة
>>> loss = model(input_ids=input_ids, labels=labels).loss
>>> loss.item()
0.2542
```

كما ترون، يحتاج النموذج إلى إدخالين فقط لحساب الخسارة: `input_ids` (التي هي `input_ids` لتسلسل الإدخال المشفر) و`labels` (التي هي `input_ids` لتسلسل الهدف المشفر). سيقوم النموذج تلقائيًا بإنشاء `decoder_input_ids` بناءً على `labels`، عن طريق تحويلها موضع واحد إلى اليمين وإضافة `config.decoder_start_token_id`، والذي يساوي 0 لـ T5 (أي معرف رمز الحشو). لاحظ أيضًا بادئة المهمة: نقوم بإضافة بادئة إلى تسلسل الإدخال بـ 'translate English to German: ' قبل تشفيره. سيساعد هذا في تحسين الأداء، حيث تم استخدام بادئة المهمة هذه أثناء التدريب المسبق لـ T5.

ومع ذلك، فإن المثال أعلاه يوضح مثال تدريب واحد فقط. في الممارسة العملية، يتم تدريب نماذج التعلم العميق في دفعات. وهذا يعني أنه يجب علينا إضافة أمثلة / اقتطاعها إلى نفس الطول. بالنسبة لنماذج الترميز وفك الترميز، يتم عادةً تحديد `max_source_length` و`max_target_length`، والتي تحدد الطول الأقصى لتسلسلات الإدخال والإخراج على التوالي (وإلا يتم اقتطاعها). يجب ضبط هذه القيم بعناية اعتمادًا على المهمة.

بالإضافة إلى ذلك، يجب التأكد من أن معرفات رموز الحشو لـ `labels` لا تؤخذ في الاعتبار بواسطة دالة الخسارة. في PyTorch وTensorflow، يمكن القيام بذلك عن طريق استبدالها بـ -100، وهو `ignore_index` لـ `CrossEntropyLoss`. في Flax، يمكنك استخدام `decoder_attention_mask` لتجاهل الرموز المضافة من الخسارة (راجع [مخطوط ملخص Flax](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization) للحصول على التفاصيل). كما نقوم بتمرير `attention_mask` كإدخال إضافي إلى النموذج، والذي يتأكد من تجاهل رموز الحشو للإدخالات. يوضح مثال الكود أدناه كل هذا.

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration
>>> import torch

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> # هاتان المعلمتان الخاصتان بالمهام
>>> max_source_length = 512
>>> max_target_length = 128

>>> # لنفترض أن لدينا مثالين تدريب التاليين:
>>> input_sequence_1 = "Welcome to NYC"
>>> output_sequence_1 = "Bienvenue à NYC"

>>> input_sequence_2 = "HuggingFace is a company"
>>> output_sequence_2 = "HuggingFace est une entreprise"

>>> # تشفير الإدخالات
>>> task_prefix = "translate English to French: "
>>> input_sequences = [input_sequence_1, input_sequence_2]

>>> encoding = tokenizer(
...     [task_prefix + sequence for sequence in input_sequences],
...     padding="longest"،
...     max_length=max_source_length،
...     truncation=True،
...     return_tensors="pt"،
... )

>>> input_ids, attention_mask = encoding.input_ids, encoding.attention_mask

>>> # تشفير الأهداف
>>> target_encoding = tokenizer(
...     [output_sequence_1, output_sequence_2],
...     padding="longest"،
...     max_length=max_target_length،
...     truncation=True،
...     return_tensors="pt"،
... )
>>> labels = target_encoding.input_ids

>>> # استبدل معرفات رموز الحشو في العلامات بـ -100 حتى يتم تجاهلها بواسطة الخسارة
>>> labels[labels == tokenizer.pad_token_id] = -100

>>> # تمرير إلى الأمام
>>> loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss
>>> loss.item()
0.188
```

نصائح تدريب إضافية:

- تحتاج نماذج T5 إلى معدل تعلم أعلى قليلاً من الافتراضي المحدد في `Trainer` عند استخدام محسن AdamW. عادةً ما تعمل القيم 1e-4 و3e-4 بشكل جيد لمعظم المشكلات (التصنيف، والتلخيص، والترجمة، والإجابة على الأسئلة، وتوليد الأسئلة). لاحظ أن T5 تم تدريبه مسبقًا باستخدام محسن AdaFactor.

وفقًا [لهذا المنشور في المنتدى](https://discuss.huggingface.co/t/t5-finetuning-tips/684)، فإن بادئات المهام مهمة عند (1) إجراء التدريب متعدد المهام (2) مهمتك مشابهة أو مرتبطة بمهمة إشرافية تم استخدامها في مزيج التدريب المسبق لـ T5 (راجع التذييل D من [الورقة](https://arxiv.org/pdf/1910.10683.pdf) للحصول على بادئات المهام المستخدمة).

إذا كنت تتدرب على TPU، فمن المستحسن إضافة أمثلة مجموعة البيانات بأكملها إلى نفس الطول أو استخدام "pad_to_multiple_of" للحصول على عدد صغير من أحجام الدلاء المحددة مسبقًا لتناسب جميع الأمثلة. لا يُنصح بإضافة الدفعات ديناميكيًا إلى أطول مثال على TPU لأنه يؤدي إلى إعادة تجميع لكل شكل دفعة يتم مواجهته أثناء التدريب، مما يبطئ التدريب بشكل كبير. (إضافة فقط إلى أطول مثال في دفعة) يؤدي إلى تدريب بطيء للغاية على TPU.

## الاستنتاج

في وقت الاستنتاج، يُنصح باستخدام [`~generation.GenerationMixin.generate`]. تهتم هذه الطريقة بتشفير الإدخال وتغذية الحالات المخفية المشفرة عبر طبقات الاهتمام المتقاطع إلى فك الترميز وتوليد إخراج فك الترميز تلقائيًا. تحقق من [هذه التدوينة](https://huggingface.co/blog/how-to-generate) لمعرفة جميع التفاصيل حول توليد النص باستخدام Transformers.

هناك أيضًا [هذه التدوينة](https://huggingface.co/blog/encoder-decoder#encoder-decoder) التي تشرح كيفية عمل التوليد بشكل عام في نماذج الترميز وفك الترميز.

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
>>> outputs = model.generate(input_ids)
>>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))
Das Haus ist wunderbar.
```

لاحظ أن T5 يستخدم `pad_token_id` كـ `decoder_start_token_id`، لذا عند إجراء التوليد بدون استخدام [`~generation.GenerationMixin.generate`]، تأكد من بدئه بـ `pad_token_id`.

يوضح المثال أعلاه مثالًا واحدًا فقط. يمكنك أيضًا إجراء الاستدلال على دفعات، كما يلي:

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> task_prefix = "translate English to German: "
>>> # استخدام جمل ذات أطوال مختلفة لاختبار الدفعات
>>> sentences = ["The house is wonderful.", "I like to work in NYC."]

>>> inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors="pt", padding=True)

>>> output_sequences = model.generate(
...     input_ids=inputs["input_ids"],
...     attention_mask=inputs["attention_mask"],
...     do_sample=False,  # تعطيل العينات لاختبار ما إذا كان الدفعات تؤثر على الإخراج
... )

>>> print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))
['Das Haus ist wunderbar.', 'Ich arbeite gerne in NYC.']
```

نظرًا لأن T5 تم تدريبه على هدف إزالة التشويش من النطاق، فيمكن استخدامه للتنبؤ برموز الاستطلاع (الرموز المقنعة) أثناء الاستدلال. ثم يتم وضع الرموز المتوقعة بين رموز الاستطلاع.

```python
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
>>> model = T5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

>>> input_ids = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="pt").input_ids

>>> sequence_ids = model.generate(input_ids)
>>> sequences = tokenizer.batch_decode(sequence_ids)
>>> sequences
['<pad> <extra_id_0> park offers <extra_id_1> the <extra_id_2> park.</s>']
```

## الأداء

إذا كنت تريد أداء تدريب واستدلال أسرع، فقم بتثبيت [NVIDIA APEX](https://github.com/NVIDIA/apex#quick-start) لمعالجات NVIDIA GPU، أو [ROCm APEX](https://github.com/ROCmSoftwarePlatform/apex) لمعالجات AMD GPU، ثم سيستخدم النموذج تلقائيًا `apex.normalization.FusedRMSNorm` بدلاً من `T5LayerNorm`. يستخدم السابق نواة مدمجة محسنة وهي أسرع عدة مرات من الأخير.
## الموارد

قائمة بموارد Hugging Face الرسمية وموارد المجتمع (يشار إليها بالرمز 🌎) لمساعدتك في البدء مع T5. إذا كنت مهتمًا بتقديم مورد لإدراجه هنا، فيرجى فتح طلب سحب Pull Request وسنقوم بمراجعته! ويفضل أن يُظهر المورد شيئًا جديدًا بدلاً من تكرار مورد موجود.

<PipelineTag pipeline="text-classification"/>

- دفتر ملاحظات حول كيفية [ضبط T5 الدقيق للتصنيف والاختيار المتعدد](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb).

- دفتر ملاحظات حول كيفية [ضبط T5 الدقيق لاستخراج المشاعر من النص](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb). 🌎

<PipelineTag pipeline="token-classification"/>

- دفتر ملاحظات حول كيفية [ضبط T5 الدقيق للتعرف على الكيانات المسماة](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing). 🌎

<PipelineTag pipeline="text-generation"/>

- دفتر ملاحظات حول [ضبط CodeT5 الدقيق لتوليد التوثيقات من كود Ruby](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb).

<PipelineTag pipeline="summarization"/>

- دفتر ملاحظات لضبط [T5-base-dutch الدقيق لأداء الملخص باللغة الهولندية على وحدة معالجة Tensor Processing Unit (TPU)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tuning_Dutch_T5_base_on_CNN_Daily_Mail_for_summarization_(on_TPU_using_HuggingFace_Accelerate).ipynb).

- دفتر ملاحظات حول كيفية [ضبط T5 الدقيق للتلخيص في PyTorch وتتبع التجارب باستخدام WandB](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb#scrollTo=OKRpFvYhBauC). 🌎

- منشور مدونة حول [التدريب الموزع: تدريب BART/T5 للتلخيص باستخدام 🤗 Transformers وAmazon SageMaker](https://huggingface.co/blog/sagemaker-distributed-training-seq2seq).

- [`T5ForConditionalGeneration`] مدعوم بواسطة [سكريبت المثال](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) و[دفتر الملاحظات](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb).

- [`TFT5ForConditionalGeneration`] مدعوم بواسطة [سكريبت المثال](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization) و[دفتر الملاحظات](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).

- [`FlaxT5ForConditionalGeneration`] مدعوم بواسطة [سكريبت المثال](https://github.com/huggingface/transformers/tree/main/examples/flax/summarization).

- فصل [التلخيص](https://huggingface.co/course/chapter7/5?fw=pt#summarization) من دورة 🤗 Hugging Face.

- [دليل مهمة التلخيص](../tasks/summarization)

<PipelineTag pipeline="fill-mask"/>

- [`FlaxT5ForConditionalGeneration`] مدعوم بواسطة [سكريبت المثال](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#t5-like-span-masked-language-modeling) لتدريب T5 بهدف نموذج اللغة المقنعة. يوضح السكربت أيضًا كيفية تدريب محلل T5. [`FlaxT5ForConditionalGeneration`] مدعوم أيضًا بواسطة هذا [دفتر الملاحظات](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).

<PipelineTag pipeline="translation"/>

- [`T5ForConditionalGeneration`] مدعوم بواسطة [سكريبت المثال](https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation) و[دفتر الملاحظات](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb).

- [`TFT5ForConditionalGeneration`] مدعوم بواسطة [سكريبت المثال](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/translation) و[دفتر الملاحظات](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).

- [دليل مهمة الترجمة](../tasks/translation)

<PipelineTag pipeline="question-answering"/>

- دفتر ملاحظات حول كيفية [ضبط T5 الدقيق للإجابة على الأسئلة باستخدام TensorFlow 2](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb). 🌎

- دفتر ملاحظات حول كيفية [ضبط T5 الدقيق للإجابة على الأسئلة على وحدة معالجة Tensor Processing Unit (TPU)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil).

🚀 **النشر**

- منشور مدونة حول كيفية نشر [T5 11B للاستنتاج بأقل من 500 دولار](https://www.philschmid.de/deploy-t5-11b).

## T5Config

[[autodoc]] T5Config

## T5Tokenizer

[[autodoc]] T5Tokenizer

- build_inputs_with_special_tokens

- get_special_tokens_mask

- create_token_type_ids_from_sequences

- save_vocabulary

## T5TokenizerFast

[[autodoc]] T5TokenizerFast

<frameworkcontent>

<pt>

## T5Model

[[autodoc]] T5Model

- forward

## T5ForConditionalGeneration

[[autodoc]] T5ForConditionalGeneration

- forward

## T5EncoderModel

[[autodoc]] T5EncoderModel

- forward

## T5ForSequenceClassification

[[autodoc]] T5ForSequenceClassification

- forward

## T5ForTokenClassification

[[autodoc]] T5ForTokenClassification

- forward

## T5ForQuestionAnswering

[[autodoc]] T5ForQuestionAnswering

- forward

</pt>

<tf>

## TFT5Model

[[autodoc]] TFT5Model

- call

## TFT5ForConditionalGeneration

[[autodoc]] TFT5ForConditionalGeneration

- call

## TFT5EncoderModel

[[autodoc]] TFT5EncoderModel

- call

</tf>

<jax>

## FlaxT5Model

[[autodoc]] FlaxT5Model

- __call__

- encode

- decode

## FlaxT5ForConditionalGeneration

[[autodoc]] FlaxT5ForConditionalGeneration

- __call__

- encode

- decode

## FlaxT5EncoderModel

[[autodoc]] FlaxT5EncoderModel

- __call__

</jax>

</frameworkcontent>