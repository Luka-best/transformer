# Reformer

## نظرة عامة

اقتُرح نموذج Reformer في الورقة البحثية [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451.pdf) من قبل نيكيتا كيتايف، ولوكاسز كايزر، وأنزلوم ليفسكايا.

الملخص من الورقة البحثية هو كما يلي:

*تحقق نماذج Transformer الكبيرة بشكل روتيني نتائج متقدمة في عدد من المهام، ولكن يمكن أن يكون تدريب هذه النماذج باهظ التكلفة، خاصة على التسلسلات الطويلة. نقدم تقنيتين لتحسين كفاءة محولات الطاقة. أولاً، نستبدل الاهتمام بمنتج النقاط باهتمام يستخدم التجزئة الحساسة للمحلية، مما يغير تعقيدها من O(L^2) إلى O(Llog(L))، حيث L هو طول التسلسل. علاوة على ذلك، نستخدم طبقات بقايا عكسية بدلاً من البقايا القياسية، والتي تسمح بتخزين التنشيطات مرة واحدة فقط في عملية التدريب بدلاً من N مرة، حيث N هو عدد الطبقات. يؤدي النموذج الناتج، وهو Reformer، إلى أداء مماثل لنماذج المحول أثناء كونه أكثر كفاءة في الذاكرة وأسرع بكثير على التسلسلات الطويلة.*

تمت المساهمة بهذا النموذج من قبل [باتريكفونبلاتين](https://huggingface.co/patrickvonplaten). يمكن العثور على كود المؤلفين [هنا](https://github.com/google/trax/tree/master/trax/models/reformer).

## نصائح الاستخدام

- لا يعمل Reformer مع *torch.nn.DataParallel* بسبب وجود خلل في PyTorch، راجع [القضية #36035](https://github.com/pytorch/pytorch/issues/36035).
- استخدم ترميز الموضع المحوري (انظر أدناه لمزيد من التفاصيل). إنها آلية لتجنب وجود مصفوفة ترميز موضعي ضخمة (عندما يكون طول التسلسل كبيرًا جدًا) عن طريق تحليلها إلى مصفوفات أصغر.
- استبدل الاهتمام التقليدي بالاهتمام LSH (تجزئة محلية الحساسة) (انظر أدناه لمزيد من التفاصيل). إنها تقنية لتجنب حساب المنتج الكامل للاستعلام والمفتاح في طبقات الاهتمام.
- تجنب تخزين النتائج المتوسطة لكل طبقة باستخدام طبقات المحول العكسي للحصول عليها أثناء المرور الخلفي (يتم إرجاعها عن طريق طرح البقايا من إدخال الطبقة التالية) أو إعادة حسابها للنتائج داخل طبقة معينة (أقل كفاءة من تخزينها ولكنها توفر الذاكرة).
- احسب عمليات التغذية الأمامية على دفعات وليس على الدفعة بأكملها.

### ترميزات الموضع المحوري

تم تنفيذ ترميزات الموضع المحوري لأول مرة في مكتبة [trax](https://github.com/google/trax/blob/4d99ad4965bab1deba227539758d59f0df0fef48/trax/layers/research/position_encodings.py#L29) من Google وتم تطويرها بواسطة مؤلفي ورقة النموذج هذه. في النماذج التي تعالج تسلسلات الإدخال الطويلة جدًا، تخزن ترميزات الموضع التقليدية مصفوفة تضمين بحجم \\(d\\) وهو `config.hidden_size` لكل موضع \\(i, \ldots, n_s\\)، مع \\(n_s\\) كونها `config.max_embedding_size`. وهذا يعني أن وجود طول تسلسل قدره \\(n_s = 2^{19} \approx 0.5M\\) و`config.hidden_size` من \\(d = 2^{10} \approx 1000\\) سينتج عنه مصفوفة ترميز موضعي:

$$X_{i,j}, \text{ with } i \in \left[1,\ldots, d\right] \text{ and } j \in \left[1,\ldots, n_s\right]$$

التي تحتوي بمفردها على أكثر من 500 مليون معلمة لتخزينها. تقوم الترميزات الموضعية المحورية بتحليل \\(X_{i,j}\\) إلى مصفوفتين:

$$X^{1}_{i,j}, \text{ with } i \in \left[1,\ldots, d^1\right] \text{ and } j \in \left[1,\ldots, n_s^1\right]$$

و

$$X^{2}_{i,j}, \text{ with } i \in \left[1,\ldots, d^2\right] \text{ and } j \in \left[1,\ldots, n_s^2\right]$$

مع:

$$d = d^1 + d^2 \text{ and } n_s = n_s^1 \times n_s^2 .$$

لذلك، ينطبق ما يلي:

$$X_{i,j} = \begin{cases}
X^{1}_{i, k}, & \text{if }\ i < d^1 \text{ with } k = j \mod n_s^1 \\
X^{2}_{i - d^1, l}, & \text{if } i \ge d^1 \text{ with } l = \lfloor\frac{j}{n_s^1}\rfloor
\end{cases}$$

بديهيا، هذا يعني أن متجه ترميز الموضع \\(x_j \in \mathbb{R}^{d}\\) هو الآن تركيبة من متجهات الترميز المُعامل: \\(x^1_{k, l} + x^2_{l, k}\\)، حيث يتم تحليل بُعد `config.max_embedding_size` \\(j\\) إلى \\(k \text{ and } l\\). يضمن هذا التصميم أن كل متجه ترميز موضعي \\(x_j\\) يكون فريدًا.

باستخدام المثال أعلاه مرة أخرى، يمكن لترميز الموضع المحوري مع \\(d^1 = 2^9, d^2 = 2^9, n_s^1 = 2^9, n_s^2 = 2^{10}\\) أن يقلل بشكل كبير عدد المعلمات من 500 000 000 إلى \\(2^{18} + 2^{19} \approx 780 000\\) معلمة، مما يعني انخفاض استخدام الذاكرة بنسبة 85%.

في الممارسة العملية، يتم تعيين معلمة `config.axial_pos_embds_dim` إلى زوج \\((d^1, d^2)\\) يجب أن يكون مجموعها مساويًا لـ `config.hidden_size` و`config.axial_pos_shape` يتم تعيينه إلى زوج \\((n_s^1, n_s^2)\\) يجب أن يكون ناتجهما مساويًا لـ `config.max_embedding_size`، والذي يجب أن يكون أثناء التدريب مساويًا لـ *طول التسلسل* لـ `input_ids`.

### الاهتمام الذاتي LSH

في الاهتمام الذاتي LSH (تجزئة محلية الحساسة)، يتم ربط أوزان إسقاط المفتاح والاستعلام. لذلك، يتم أيضًا ربط متجهات تضمين الاستعلام والمفتاح. يستخدم الاهتمام الذاتي LSH آلية التجزئة الحساسة للمحلية المقترحة في [Practical and Optimal LSH for Angular Distance](https://arxiv.org/abs/1509.02897) لتعيين كل متجهات تضمين الاستعلام والمفتاح المرتبطة إلى واحدة من `config.num_buckets` الدلاء الممكنة. الفكرة هي أن متجهات تضمين الاستعلام والمفتاح الأكثر "تشابهًا" (من حيث *تشابه جيب التمام*) من المحتمل أن يتم تعيينها إلى نفس الدلو.

يمكن تحسين دقة آلية LSH عن طريق زيادة `config.num_hashes` أو مباشرة وسيط `num_hashes` للدالة الأمامية بحيث يقترب إخراج الاهتمام الذاتي LSH بشكل أفضل من إخراج الاهتمام "العادي" الكامل. يتم بعد ذلك فرز الدلاء وتقسيمها إلى قطع متجهات الاستعلام والمفتاح لكل منها طول `config.lsh_chunk_length`. لكل قطعة، تحضر متجهات الاستعلام إلى متجهات المفاتيح الخاصة بها (المرتبطة بنفسها) وإلى متجهات المفاتيح الخاصة بـ `config.lsh_num_chunks_before` القطع المجاورة السابقة و`config.lsh_num_chunks_after` القطع المجاورة التالية.

لمزيد من المعلومات، راجع [الورقة الأصلية](https://arxiv.org/abs/2001.04451) أو هذه [التدوينة الرائعة](https://www.pragmatic.ml/reformer-deep-dive/).

لاحظ أن `config.num_buckets` يمكن أيضًا تحليلها إلى قائمة \\((n_{\text{buckets}}^1, n_{\text{buckets}}^2)\\). بهذه الطريقة، بدلاً من تعيين متجهات الاستعلام والمفتاح إلى واحدة من \\((1,\ldots, n_{\text{buckets}})\\)، يتم تعيينها إلى واحدة من \\((1-1,\ldots, n_{\text{buckets}}^1-1, \ldots, 1-n_{\text{buckets}}^2, \ldots, n_{\text{buckets}}^1-n_{\text{buckets}}^2)\\). هذا أمر بالغ الأهمية للتسلسلات الطويلة جدًا لتوفير الذاكرة.

عند تدريب نموذج من الصفر، يُنصح بترك `config.num_buckets=None`، بحيث يتم حساب قيمة جيدة لـ `num_buckets` أثناء التنقل بناءً على طول التسلسل. سيتم بعد ذلك حفظ هذه القيمة تلقائيًا في التكوين ويجب إعادة استخدامها للاستدلال.

باستخدام الاهتمام الذاتي LSH، يمكن تقليل تعقيد الذاكرة والوقت لعملية ضرب المصفوفة الاستعلامية الرئيسية من \\(\mathcal{O}(n_s \times n_s)\\) إلى \\(\mathcal{O}(n_s \times \log(n_s))\\)، والذي يمثل عادةً عنق الزجاجة في الذاكرة والوقت في نموذج المحول، مع \\(n_s\\) كونها طول التسلسل.

### الاهتمام الذاتي المحلي

الاهتمام الذاتي المحلي هو في الأساس طبقة "عادية" للاهتمام الذاتي مع إسقاطات المفتاح والاستعلام والقيمة، ولكن يتم تقسيمها إلى قطع بحيث في كل قطعة من طول `config.local_chunk_length` تحضر متجهات الاستعلام فقط إلى متجهات المفاتيح الموجودة في قطعتها وإلى متجهات المفاتيح الخاصة بـ `config.local_num_chunks_before` القطع المجاورة السابقة و`config.local_num_chunks_after` القطع المجاورة التالية.

باستخدام الاهتمام الذاتي المحلي، يمكن تقليل تعقيد الذاكرة والوقت لعملية ضرب المصفوفة الاستعلامية الرئيسية من \\(\mathcal{O}(n_s \times n_s)\\) إلى \\(\mathcal{O}(n_s \times \log(n_s))\\)، والذي يمثل عادةً عنق الزجاجة في الذاكرة والوقت في نموذج المحول، مع \\(n_s\\) كونها طول التسلسل.

### التدريب

أثناء التدريب، يجب التأكد من أن طول التسلسل مضبوط على قيمة يمكن تقسيمها بواسطة المضاعف المشترك الأصغر لـ `config.lsh_chunk_length` و`config.local_chunk_length` وأن معلمات ترميزات الموضع المحوري مضبوطة بشكل صحيح كما هو موضح أعلاه. Reformer فعال للغاية في الذاكرة بحيث يمكن تدريب النموذج بسهولة على تسلسلات طويلة مثل 64000 رمز.

بالنسبة للتدريب، يجب استخدام [`ReformerModelWithLMHead`] كما يلي:

```python
input_ids = tokenizer.encode("This is a sentence from the training data", return_tensors="pt")
loss = model(input_ids, labels=input_ids)[0]
```

## الموارد

- [دليل مهام تصنيف النصوص](../tasks/sequence_classification)
- [دليل مهام الإجابة على الأسئلة](../tasks/question_answering)
- [دليل مهام نمذجة اللغة السببية](../tasks/language_modeling)
- [دليل مهام نمذجة اللغة المُقنعة](../tasks/masked_language_modeling)

## ReformerConfig

[[autodoc]] ReformerConfig

## ReformerTokenizer

[[autodoc]] ReformerTokenizer

- save_vocabulary

## ReformerTokenizerFast

[[autodoc]] ReformerTokenizerFast

## ReformerModel

[[autodoc]] ReformerModel

- forward

## ReformerModelWithLMHead

[[autodoc]] ReformerModelWithLMHead

- forward

## ReformerForMaskedLM

[[autodoc]] ReformerForMaskedLM

- forward

## ReformerForSequenceClassification

[[autodoc]] ReformerForSequenceClassification

- forward

## ReformerForQuestionAnswering

[[autodoc]] ReformerForQuestionAnswering

- forward