# XLM-V

## نظرة عامة

XLM-V هو نموذج لغة متعدد اللغات مع مفردات مكونة من مليون رمز تم تدريبه على 2.5 تيرابايت من البيانات من Common Crawl (نفس XLM-R).
تم تقديمه في الورقة البحثية [XLM-V: التغلب على عنق الزجاجة في المفردات في نماذج اللغة المقنعة متعددة اللغات](https://arxiv.org/abs/2301.10472)
من قبل ديفيس ليANG، وهيلا جونين، ويونينج ماو، وروي هو، ونامان جويال، ومرجان غازفينينجاد، ولوك زيتيلموير، وماديان خابسا.

من الملخص الخاص بورقة XLM-V:

> *تمد نماذج اللغة متعددة اللغات الكبيرة عادةً على مفردات واحدة مشتركة عبر 100+ لغة.
ومع زيادة عدد معلمات هذه النماذج وعمقها، ظل حجم المفردات دون تغيير إلى حد كبير.
تؤدي عنق زجاجة المفردات هذه إلى الحد من القدرات التمثيلية للنماذج متعددة اللغات مثل XLM-R.
في هذه الورقة، نقدم نهجًا جديدًا للقياس إلى مفردات متعددة اللغات كبيرة جدًا عن طريق
إلغاء التأكيد على مشاركة الرموز بين اللغات ذات التداخل المعجمي القليل وتخصيص سعة المفردات
لتحقيق التغطية الكافية لكل لغة على حدة. وعادة ما تكون عمليات التمييز باستخدام مفرداتنا أكثر دلالية وقصرًا مقارنة بـ XLM-R. من خلال الاستفادة من هذه المفردات المحسنة، نقوم بتدريب XLM-V،
نموذج لغة متعدد اللغات مع مفردات مكونة من مليون رمز. يفوق XLM-V أداء XLM-R في كل مهمة قمنا
تم اختباره بدءًا من الاستدلال اللغوي الطبيعي (XNLI)، والأسئلة والأجوبة (MLQA، XQuAD، TyDiQA)، و
تعريف الكيان المسمى (WikiAnn) إلى المهام منخفضة الموارد (Americas NLI، MasakhaNER).*

تمت المساهمة بهذا النموذج من قبل [stefan-it](https://huggingface.co/stefan-it)، بما في ذلك التجارب التفصيلية مع XLM-V على المهام النهائية.
يمكن العثور على مستودع التجارب [هنا](https://github.com/stefan-it/xlm-v-experiments).

## نصائح الاستخدام

- XLM-V متوافق مع بنية نموذج XLM-RoBERTa، حيث كان من الضروري تحويل أوزان النموذج فقط من مكتبة [`fairseq`](https://github.com/facebookresearch/fairseq)
المكتبة.
- يتم استخدام تنفيذ `XLMTokenizer` لتحميل المفردات وإجراء التمييز.

يتوفر نموذج XLM-V (بحجم base) تحت محدد [`facebook/xlm-v-base`](https://huggingface.co/facebook/xlm-v-base).

<Tip>

تتشابه بنية XLM-V مع XLM-RoBERTa، راجع [وثائق XLM-RoBERTa](xlm-roberta) للحصول على مرجع API وأمثلة.

</Tip>