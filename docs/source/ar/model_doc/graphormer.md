# Graphormer

> **ملاحظة:** 
> هذا النموذج في وضع الصيانة فقط، ولن نقبل أي طلبات سحب (Pull Requests) جديدة لتغيير شفرته. إذا واجهتك أي مشكلات أثناء تشغيل هذا النموذج، يرجى إعادة تثبيت الإصدار الأخير الذي يدعم هذا النموذج: v4.40.2. يمكنك القيام بذلك عن طريق تشغيل الأمر التالي: `pip install -U transformers==4.40.2`.

## نظرة عامة

اقتُرح نموذج Graphormer في الورقة البحثية [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) من قبل Chengxuan Ying وآخرين. وهو نموذج محول رسومي (Graph Transformer) معدل للسماح بإجراء الحسابات على الرسوم البيانية بدلاً من تسلسلات النص من خلال توليد التضمينات (embeddings) والسمات ذات الاهتمام أثناء المعالجة المسبقة والجمع، ثم استخدام انتباه معدل.

هذا هو الملخص من الورقة:

> أصبحت بنية المحول (Transformer) اختيارًا مهيمنًا في العديد من المجالات، مثل معالجة اللغة الطبيعية والرؤية الحاسوبية. ومع ذلك، لم يحقق أداءً تنافسيًا في لوحات القيادة الشائعة للتنبؤ على مستوى الرسم البياني مقارنة بالمتغيرات الشائعة لشبكات الرسوم البيانية العصبية (GNN). لذلك، يظل الأمر لغزًا كيف يمكن أن تؤدي المحولات أداءً جيدًا لتعلم تمثيل الرسم البياني. في هذه الورقة، نقوم بحل هذا اللغز من خلال تقديم Graphormer، والذي تم بناؤه بناءً على بنية المحول القياسية، ويمكنه تحقيق نتائج ممتازة في مجموعة واسعة من مهام تعلم تمثيل الرسم البياني، خاصة في تحدي OGB Large-Scale Challenge الأخير. تتمثل رؤيتنا الأساسية لاستخدام المحول في الرسم البياني في ضرورة ترميز المعلومات الهيكلية للرسم البياني بشكل فعال في النموذج. ولهذه الغاية، نقترح عدة طرق ترميز هيكلية بسيطة وفعالة لمساعدة Graphormer على نمذجة البيانات المبنية على الرسوم البيانية بشكل أفضل. بالإضافة إلى ذلك، نحدد الخصائص التعبيرية لـ Graphormer ونظهر أنه مع طرقنا لترميز المعلومات الهيكلية للرسوم البيانية، يمكن تغطية العديد من المتغيرات الشائعة لشبكات GNN كحالات خاصة لـ Graphormer.

تمت المساهمة بهذا النموذج من قبل [clefourrier](https://huggingface.co/clefourrier). يمكن العثور على الكود الأصلي [هنا](https://github.com/microsoft/Graphormer).

## نصائح الاستخدام

لن يعمل هذا النموذج بشكل جيد على الرسوم البيانية الكبيرة (أكثر من 100 عقدة/حافة)، حيث سيؤدي ذلك إلى انفجار الذاكرة. يمكنك تقليل حجم الدفعة، أو زيادة ذاكرة الوصول العشوائي (RAM)، أو تقليل معلمة `UNREACHABLE_NODE_DISTANCE` في ملف `algos_graphormer.pyx`، ولكن سيكون من الصعب تجاوز 700 عقدة/حافة.

لا يستخدم هذا النموذج رمزًا مميزًا (tokenizer)، ولكنه يستخدم بدلاً من ذلك مجمعًا (collator) خاصًا أثناء التدريب.

## GraphormerConfig

[[autodoc]] GraphormerConfig

## GraphormerModel

[[autodoc]] GraphormerModel

- forward

## GraphormerForGraphClassification

[[autodoc]] GraphormerForGraphClassification

- forward