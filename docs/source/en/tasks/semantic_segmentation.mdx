<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Semantic segmentation

<Youtube id="dKE8SIt9C-w"/>

Semantic segmentation assigns a label or class to each individual pixel of an image to a specific class. There are several types of segmentation tasks, and in the case of semantic segmentation, it doesn't differentiate between unique instances of the same object. Both objects are given the same label. Common real-world applications of semantic segmentation include training self-driving cars to identify pedestrians and important traffic information, identifying cells and abnormalities in medical imagery, and monitoring environmental changes from satellite imagery.

This guide will show you how to finetune [SegFormer](https://huggingface.co/nvidia/mit-b0) on the [SceneParse150](https://huggingface.co/datasets/scene_parse_150) dataset.

<Tip>

See the image segmentation [task page](https://huggingface.co/tasks/image-segmentation) for more information about its associated models, datasets, and metrics.

</Tip>

Before you begin, make sure you have all the necessary libraries installed:

```bash
pip install -q datasets transformers evaluate
```

## Load SceneParse150 dataset

Load the first 50 examples of the SceneParse150 dataset from the ðŸ¤— Datasets library so you can quickly train and test a model:

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

Split this dataset into a train and test set:

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

Then take a look at an example:

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

The `image` field contains a PIL image and the `annotation` field contains a PIL image of the segmentation map. There is also a `scene_category` field which describes the scene of the image like airport terminal, street or kitchen.

You'll also want to create a dictionary that maps a label id to a label class, which will be useful when you set up the model later. Download the mappings from the Hub and create the `id2label` and `label2id` dictionaries:

```py
>>> import json
>>> from huggingface_hub import cached_download, hf_hub_url

>>> repo_id = "datasets/huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename)), "r"))
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
```

## Preprocess

Load a SegFormer feature extractor to process the image into an array of tensors. Some datasets, like this one, uses the zero-index as the background class. However, the background class isn't included in the 150 classes, so you'll need to set `reduce_labels=True` to subtract one from all the labels. The zero-index is replaced by `255` so it is ignored by SegFormer's loss function. 

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("nvidia/mit-b0", reduce_labels=True)
```

It is common to apply some data augmentations to an image dataset to make a model more robust against overfitting. Here, you'll use the [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) function from [torchvision](https://pytorch.org/vision/stable/index.html) to randomly change the color properties of an image.

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

Now create a preprocessing function that'll apply the `jitter` transform and return the `pixel_values` and `labels` of an image. `pixel_values` is the expected model input and it is generated by the feature extractor. You should only `jitter` the images in the training set. For the `labels` and the test set, the feature extractor will only crop and normalize the `images` and `labels`.

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = feature_extractor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = feature_extractor(images, labels)
...     return inputs
```

To apply the `jitter` over the entire dataset, use ðŸ¤— Datasets [`~datasets.Dataset.set_transform`] function. The transform is applied on the fly which is faster and consumes less disk space.

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

## Train

<frameworkcontent>
<pt>
Load SegFormer with [`AutoModelForSemanticSegmentation`], specify the number of labels, and pass the model the mapping between label ids and label classes:

```py
>>> from transformers import AutoModelForSemanticSegmentation

>>> pretrained_model_name = "nvidia/mit-b0"
>>> model = AutoModelForSemanticSegmentation.from_pretrained(
...     pretrained_model_name, num_labels=num_labels, id2label=id2label, label2id=label2id
... )
```

<Tip>

If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#finetune-with-trainer)!

</Tip>

Define your training hyperparameters in [`TrainingArguments`]. It is important you don't remove unused columns because this will drop the `image` column. Without the `image` column, you can't create `pixel_values`. Set `remove_unused_columns=False` to prevent this behavior!

```py
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(
...     output_dir="./results",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     evaluation_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
... )
```

To evaluate model performance during training, you'll need to create a function to compute and report metrics. For semantic segmentation, you'll typically compute the [mean intersection over union](https://huggingface.co/spaces/evaluate-metric/mean_iou) (IoU) metric. The mean IoU metric measures the overlapping area between the predicted and ground truth segmentation maps. 

Load the mean IoU from the ðŸ¤— Evaluate library:

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

Then create a function to [`~evaluate.EvaluationModule.compute`] the metrics. Your predictions need to be converted to logits before you can call [`~evaluate.EvaluationModule.compute`]:

```py
>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if type(value) is np.ndarray:
...                 metrics[key] = value.tolist()
...         return metrics
```

Now pass your model, training arguments, datasets, and metrics function to the [`Trainer`]:

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )
```

Lastly, call [`~Trainer.train`] to finetune your model:

```py
>>> trainer.train()
```
</pt>
</frameworkcontent>

## Inference

Great, now that you've finetuned a model, you can use it for inference!

Load an image for inference:

```py
>>> ds = load_dataset("scene_parse_150", split="train[:50]")
>>> image = ds[0]["image"]
>>> image
```

Process the image with the feature extractor and place the `pixel_values` on a GPU:

```py
# Use GPU if available, otherwise use a CPU
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
>>> encoding = feature_extractor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

Pass your input to the model and return the `logits`:

```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```

Rescale the logits to the original image size:

```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode='bilinear',
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```

To visualize the results, load the dataset color palette that maps each class to their RGB values. Then you can combine and plot your image and the predicted segmentation map:

```py
>>> def ade_palette():
...     """ADE20K palette that maps each class to RGB values."""
...     return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],
...             [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],
...             [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],
...             [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],
...             [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],
...             [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],
...             [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],
...             [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],
...             [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],
...             [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],
...             [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],
...             [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],
...             [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],
...             [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],
...             [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],
...             [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],
...             [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],
...             [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],
...             [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],
...             [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],
...             [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],
...             [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],
...             [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],
...             [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],
...             [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],
...             [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],
...             [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],
...             [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],
...             [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],
...             [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],
...             [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],
...             [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],
...             [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],
...             [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],
...             [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],
...             [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],
...             [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],
...             [102, 255, 0], [92, 0, 255]]

>>> import matplotlib.pyplot as plt

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
# Convert to BGR
>>> color_seg = color_seg[..., ::-1]

# Plot image and predicted mask
>>> img = np.array(image) * 0.5 + color_seg * 0.5
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```