<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Glossary

This glossary defines general machine learning and ðŸ¤— Hugging Face terms to help you better understand the documentation.

## A

### attention mask
 
A tensor filled with 0's and 1's that is identical in shape to the input id tensor, and determines which tokens should be attended to (indicated by 1's) and which tokens should be ignored (indicated by 0's) by the model's attention layers. When you pass text to a [`PreTrainedTokenizer`], it'll automatically create and return the attention mask depending on the specific tokenizer used.

Used when batching sequences of different lengths together to create a rectangular tensor. Padded values are represented by 0's, meaning they shouldn't be attended to because they don't contain any meaningful information. For example:

```py
>>> from transformers import AutoTokenizer

>>> text = [
...   "But what about second breakfast?",
...   "Don't think he knows about second breakfast, Pip.",
...   "What about elevensies?",
... ]
>>> encoded_inputs = tokenizer(text, padding=True)
>>> print(encoded_inputs)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 
'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

## C

### causal language modeling

A pretraining objective where a model is trained to predict the next token given the tokens it has already attended to at a certain timestep. A mask is used to prevent the model from attending to future tokens. Models pretrained on causal language modeling - also known as *autoregressive models* - are great for generative tasks; [GPT-2](model_doc/gpt2) is an example of a famous causal language model.

### channel

### connectionist temporal classification (CTC)

An algorithm which allows a model to learn without knowing exactly how the input and output are aligned; CTC simply approximates the output by maximizing the probability. CTC is commonly used in speech recognition tasks because speech doesn't always cleanly align with the transcript due to speaker's different speech rates.

### convolution

## D

### deep learning

A specific subset of machine learning algorithms that have multiple layers to learn increasingly complex representations of data (text, images, and video).

### decoder input ids

A tensor specific to encoder-decoder models (such as [BART](model_doc/bart) and [T5](model_doc/t5)) containing the input ids to be passed to the decoder. These inputs are used for sequence-to-sequence tasks such as translation or summarization. Most encoder-decoder models typically create the `decoder_input_ids` on their own from the `labels`, and it's the preferred way for training the model.

```py
>>> from transformers import T5Tokenizer, T5ForConditionalGeneration

>>> tokenizer = T5Tokenizer.from_pretrained("t5-small")
>>> model = TFT5ForConditionalGeneration.from_pretrained("t5-small")

>>> inputs = tokenizer("The <extra_id_0> walks in <extra_id_1> park", return_tensors="tf").input_ids
>>> labels = tokenizer("<extra_id_0> cute dog <extra_id_1> the <extra_id_2>", return_tensors="tf").input_ids
# the forward function automatically creates the decoder_input_ids from the labels
>>> outputs = model(inputs, labels=labels)
```

## F

### feed-forward chunking

A method developed by the authors of [Reformer](https://arxiv.org/abs/2001.04451) that reduces memory use at the cost of increased computation time. In each residual attention block, the self-attention layer is usually followed by 2 feed-forward layers. The intermediate embedding size of these feed-forward layers is often quite large, and the memory required to calculate it consumes a large amount of memory. By calculating the output embeddings of the 2 feed-forward layers in parallel and then concatenating them afterward, you end up using less memory but the results are the same.

In ðŸ¤— Transformers, you can use the [`~transformers.apply_chunking_to_forward`] function to apply feed-forward chunking. The `chunk_size` parameter defines the number of output embeddings to be computed in parallel.

## I

### image patches

Vision based Transformers models split an image into patches which are linearly embedded, and then passed as a sequence to the model. You can find the `patch_size` - or resolution - of a vision model in it's configuration.

### input ids

A tensor of the numerical representations of tokens that are passed to the model as the input. Tokens are generated from a tokenizer which splits a sequence of text into words or subwords depending on the [tokenizer type](tokenizer_summary).  

```python
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
>>> sequence = "A Titan RTX has 24GB of VRAM"
>>> tokenizer(sequence).input_ids
[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]
```

## K

### kernel

## L

### labels

An optional tensor passed to the model to compute the loss itself. The labels should be the expected prediction of
the model; the model uses the standard loss to compute the loss between its predictions and the expected value (the
label). Base models in ðŸ¤— Transformers don't accept labels because they only output features.

Models with different modeling heads have different labels formats:

- For sequence classification models ([`BertForSequenceClassification`]), the model expects a tensor of dimension
  `(batch_size)` with each value of the batch corresponding to the expected label of the entire sequence.
- For token classification models ([`BertForTokenClassification`]), the model expects a tensor of dimension
  `(batch_size, seq_length)` with each value corresponding to the expected label of each individual token.
- For masked language modeling ([`BertForMaskedLM`]), the model expects a tensor of dimension `(batch_size,
  seq_length)` with each value corresponding to the expected label of each individual token. The labels are the token
  ids for the masked token, and the values to be ignored (usually set to `-100`).
- For sequence-to-sequence tasks ([`BartForConditionalGeneration`] or [`MBartForConditionalGeneration`]), the model
  expects a tensor of dimension `(batch_size, tgt_seq_length)` with each value corresponding to the target sequences
  associated with each input sequence. During training, BART and T5 creates the appropriate `decoder_input_ids` and
  decoder attention masks (you don't need to provide these). However, this does not apply to models using the
  Encoder-Decoder framework. See the documentation of each model for more information on each specific model's labels.

## M

### masked language modeling

A pretraining objective where a model is trained to predict a randomly masked token in a sequence. Unlike [causal language modeling](#causal-language-modeling), the model is allowed to attend to tokens bidirectionally. Models pretrained on masked language modeling - also known as *autoencoding models* - are great for comprehension tasks; [BERT](model_doc/bert) is an example of a famous masked language model.

### multimodal

An input combining text with other types of inputs like image and audio.

## N

### natural language generation (NLG)

A subset of NLP focused on generating text such as translation and abstractive text summarization.

### natural language processing (NLP)

An overarching field encompassing NLG and NLU, and can be used to refer to any text related tasks.

### natural language understanding (NLU)

A subset of NLP focused on reading comprehension such as sentiment analysis and question answering.

## P

### pixel values

A tensor of the numerical representations of an image that is passed to a model. The pixel values have a shape of [`batch_size`, `num_channels`, `height`, `width`], and are generated from a feature extractor.

### pooling

### position ids

An optional tensor passed to the model to indicate the position of each token in a sequence because a Transformer model is unaware. The position ids are automatically created as absolute positional embeddings, which are selected between `[0, config.max_position_embeddings - 1]`.

  Other types fo positional embeddings include sinusoidal position embeddings and relative position embeddings.

### pretrained model

A model that has been pretrained on some data, for instance all of Wikipedia. Pretraining tasks involve a self-supervised objective such as reading the text and trying to predict the next word (see [causal language modeling](#causal-language-modeling)) or masking some words and trying to predict them (see [MLM](#masked-language-modeling)).

## R

### recurrent neural network (RNN)

A type deep learning model that uses a for-loop to iterate over a layer.

## S

### sampling rate

A measurement in hertz of the number of samples (the audio signal) taken per second. The sampling rate is a result of discretizing a continuous signal.

### self-attention

An attention mechanism where each element of the input finds out which other elements of the input they should attend to, helping the model gain a beter "understanding" of the context and word.

### sequence-to-sequence (seq2seq)

An encoder-decoder model (such as [BART](model_doc/bart) or [T5](model_doc/t5)) accepts an input sequence and outputs another sequence. These types of models are great for summarization and translation.

### stride

## T

### token

A part of a sentence, usually a word but it can also be a subword (non-common words are often split in subwords) or a punctuation symbol. Tokens are generated by tokenizers according to specific rules, and the token is converted to [input ids](#input-ids).

### token type ids

A tensor for identifying which tokens belong to which input sequence when there is more than one sequence. This helps a model understand where a sequence ends and another begins. The token type ids are automatically generated by the tokenizer if the model requires them. 

For example, in [`BERT`], `0` corresponds to `sequence_a` and `1` corresponds to `sequence_b`.

```py
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
>>> sequence_a = "HuggingFace is based in NYC"
>>> sequence_b = "Where is HuggingFace based?"

>>> tokenizer(sequence_a, sequence_b).token_type_ids
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

### Transformer

A self-attention based deep learning model architecture introduced in the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper. We built a super cool [library](https://huggingface.co/docs/transformers/index) on it. ðŸ¤—