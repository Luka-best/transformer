<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# What ðŸ¤— Transformers can do

The [Transformer](https://arxiv.org/abs/1706.03762) is a uniquely flexible and powerful architecture. While it is typically associated with natural language processing (NLP) tasks, other fields like computer vision and speech processing are increasingly adopting Transformer architectures to solve tasks in their area. If you look at some of the underlying technology behind popular consumer products like smartphones, apps, and self-driving cars, odds are that you'll find some variant of a Transformer behind it. That is because the Transformer is a general architecture that can be applied to many tasks!

This page provides an overview of some of the different audio, computer vision, and natural language processing tasks that can be solved by Transformer models. We'll also show you how easy it is to solve these tasks with ðŸ¤— Transformers in just three lines of code!

## Audio

Classical speech and audio processing techniques typically involved Gaussian mixture models, Hidden Markov models, and recurrent neural networks (RNNs). But these have been eclipsed by more modern approaches like [sequence-to-sequence models](./glossary#sequencetosequence-seq2seq) and [connectionist temporal classification (CTC)](./glossary#connectionist-temporal-classification-ctc), which dramatically simplify training end-to-end audio and speech processing systems and improve performance.

The [wav2vec2](./model_doc/wav2vec2) model was one of the first successful Transformer-based speech processing models. It adapted some of the ideas that made pretrained language models like BERT so successful. For instance, wav2vec2 pretrained on raw unlabeled speech and used a Transformer encoder to contextualize the speech representations. Its pretraining objective was to predict masked parts of the audio, just like the [masked language modeling](./glossary#masked-language-modeling) objective in BERT!

Speech and audio tasks today continue to rely on Transformer architectures, with greater emphasis being placed on models with multilingual capabilities. This makes it possible to build more robust speech processing systems that can be used across many languages and dialects, especially low-resource languages.

### Audio classification

Audio classification is a type of task that labels audio data from a predefined set of classes. It is a broad category with many specific applications, some of which include:

* acoustic scene classification: label audio with a scene label ("office", "beach", "stadium")
* acoustic event detection: label audio with a sound event label ("car horn", "whale calling", "glass breaking")
* tagging: label audio containing multiple sounds (birdsongs, speaker identification in a meeting)
* music classification: label music with a genre label ("metal", "hip-hop", "country")

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification")
>>> classifier("path/to/audio/file.mp3")
```

### Automatic speech recognition

Automatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due in part to speech being such a natural form of human communication. Today, ASR systems are embedded in "smart" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather. 

But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition")
>>> transcriber("path/to/audio/file.mp3")
```

## Computer vision

One of the first and earliest successful computer vision applications was recognizing zip code numbers using a [convolutional neural network (CNN)](glossary#convolution). For many years, CNNs have been the go-to for computer vision tasks, and modern CNNs like [ConvNeXT](./model_doc/convnext) continue to see strong usage with competitive performance.

However, with the introduction of the [Vision Transformers (ViT)](./model_doc/vit), many computer vision tasks have now shifted towards Transformer architectures. Drawing on the success of Transformers in NLP, ViT splits an image into patches and passes these to the encoder, similar to how text is tokenized. Instead of training a CNN for each type of computer vision task, a large and scalable pretrained ViT can be used for many different vision tasks. Today, it is common for other vision Transformer models to use ViT as a [backbone](.glossary#backcone).

While speech and NLP tasks today mainly use Transformer models, in computer vision, convolutions are still a powerful tool for building up an understanding of an image and have not been completely replaced. There are computer vision models that combine convolutions with Transformers, like [CvT](./model_doc/cvt), or use a pretrained convolutional backbone, such as [DETR](./model_doc/detr).

### Image classification

Image classification labels an entire image from a predefined set of classes. Like most classification tasks, there are many practical use cases for image classification, some of which include:

* healthcare: label medical images to detect disease or monitor patient health
* environment: label satellite images to monitor deforestation, inform wildland management or detect wildfires
* agriculture: label images of crops to monitor plant health or satellite images for land use monitoring 
* ecology: label images of animal or plant species to monitor wildlife populations or track endangered species

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> classifier("path/to/image/file.jpg")
```

### Object detection

Unlike image classification, object detection identifies multiple objects within an image, as well as the objects' positions in an image (defined by the bounding box). Some example applications of object detection include:

* self-driving vehicles: detect common traffic objects such as other vehicles, pedestrians, and traffic lights
* remote sensing: disaster monitoring, urban planning, and weather forecasting
* defect detection: detect cracks or structural damage in buildings, and manufacturing defects

```py
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> detector("path/to/image/file.jpg")
```

### Image segmentation

Image segmentation is a pixel-level task that assigns every pixel in an image to a class. It differs from object detection, which uses bounding boxes to label and predict objects in an image, because segmentation is more granular. Segmentation can detect objects at a pixel-level. There are several types of image segmentation:

* instance segmentation: in addition to labeling the class of an object, it also labels each distinct instance of an object ("dog-1", "dog-2")
* panoptic segmentation: a combination of semantic and instance segmentation; it labels each pixel with a semantic class **and** each distinct instance of an object

Segmentation tasks are useful for helping self-driving vehicles create a pixel-level map of the world around them so they can navigate safely around pedestrians and other vehicles. It is also useful for medical imaging where the task's finer granularity can help identify anomalous cells or organ features. Image segmentation can also be used in ecommerce to virtually try on clothes or create augmented reality experiences by overlaying objects in the real-world through your camera.

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(task="image-segmentation")
>>> segmenter("path/to/image/file.jpg")
```

### Depth estimation

Depth estimation predicts the distance of each pixel in an image from the camera. This type of computer vision task is especially important for scene understanding and reconstruction. For example, in self-driving cars, vehicles need to understand how far objects like pedestrians, traffic signs, and other vehicles are to avoid obstacles and collision. Depth information is also useful for constructing 3D representations from 2D images, and can be used to create high-quality 3D representations of biological structures or buildings.

There are two approaches to depth estimation:

* stereo: depths are estimated by comparing two images of the same image from slightly different angles
* monocular: depths are estimated from a single image

```py
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation")
>>> depth_estimator("path/to/image/file.jpg")
```

## Natural language processing

The Transformer was introduced in the, [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762), paper for tasks such as language modeling, translation, and summarization. At the time, NLP tasks were typically solved with long short-term memory (LSTM) networks and recurrent neural networks (RNNs). But the Transformer introduced a new architecture based entirely on self-attention mechanisms. These attention layers allow the Transformer to focus on specific words when learning the representation of each word. As a result, the Transformer didn't only outperform previous models, but it was also more efficient and faster to train! Since then, Transformer models have driven most of the recent progress in NLP.

### Text classification

Like classification tasks in any modality, text classification labels a sequence of text (it can be sentence-level, a paragraph or a document) from a predefined set of classes. There are many useful applications for text classification, some of which include:

* sentiment analysis: label text according to some polarity like `positive` or `negative` which can inform and support decision-making in fields like politics, finance, and marketing
* content classification: label text according to some topic to help organize and filter information in news and social media feeds (`weather`, `sports`, `finance`, etc.)

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="sentiment-analysis")
>>> classifier("Hugging Face is the best thing since sliced bread!")
```

### Token classification

In any NLP task, text is preprocessed by separating the sequence of text into individual words or subwords. These are known as [tokens](/glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:

* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in the biomedical domain where it can be used to label genes, proteins, and drug names.
* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb).

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="ner")
>>> classifier("Hugging Face is a French company based in New York City.")
```

### Question answering

Question answering is another token-level task that returns an answer to a question, sometimes with context (open-domain) and other times without any context (closed-domain). There are two common types of question answering:

* extractive: given a question and some context, the answer is a span of text from the context the model must extract
* abstractive: given a question and some context, the answer is generated from the context

This type of task is seen whenever we ask a virtual assistant to tell us whether a restaurant is open. It can also provide customer or technical support, and help search engines retrieve the relevant information you're asking for.

```py
>>> from transformers import pipeline

>>> question_answerer = pipeline(task="question-answering")
>>> question_answerer(
...     question="What is the name of the repository?",
...     context="The name of the repository is huggingface/transformers",
... )
```

### Summarization

Summarization creates a shorter version of a text from a longer one while trying to preserve most of the meaning of the original document. Summarization is a type of sequence-to-sequence task, it just outputs a shorter sequence of text than the input. A lot of long-form documents that can be summarized to help readers quickly understand the main points. Legislative bills, legal and financial documents, patents, and scientific papers are just a few examples of documents that could be summarized to save readers time and serve as a reading aid.

Like question answering, there are two types of summarization:

* extractive: identify and extract the most important sentences from the original text
* abstractive: generate the target summary (which may include new words not in the input document) from the original text

```py
>>> from transformers import pipeline

>>> summarizer = pipeline(task="summarization")
>>> summarizer(
...     "Hugging Face is a French company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge which is visible from the window."
... )
```

### Translation

Translation converts a sequence of text in one language to another. It is important for helping people from different backgrounds communicate with each other, help translate content to reach wider audiences, and it can even be used as a learning tool to help people learn a new language. Along with summarization, translation is a sequence-to-sequence task, meaning the model receives an input sequence and returns a target output sequence. 

In the early days, translation models were mostly monolingual but recently, there is increasing interest in multilingual models that can translate between many pairs of languages.

```py
>>> from transformers import pipeline

>>> text = "translate English to French: Hugging Face is a community-based open-source platform for machine learning."
>>> translator = pipeline(task="translation")
>>> translator(text)
```

### Language modeling

Language modeling is a task that predicts a word in a sequence of text. It has become a very popular NLP task because a pretrained language model can be finetuned for many other downstream tasks. Recently, there has been a lot of interest in large language models (LLMs) which demonstrate zero- or few-shot learning. This means the model can solve tasks it wasn't explicitly trained to do! Language models can be used to generate fluent and convincing text, though you need to be careful since the text may not always be accurate.

There are two types of language modeling:

* causal: the model's objective is to predict the next token in a sequence, and future tokens are masked
* masked: the model's objective is to predict a masked token in a sequence with full access to the tokens in the sequence

```py
>>> from transformers import pipeline
# causal language modeling

>>> prompt = "Hugging Face is a"
>>> text_generator = pipeline(task="text-generation")
>>> text_generator(prompt)
# masked language modeling

>>> text = "Hugging Face is a <mask> company based in New York City."
>>> fill_mask = pipeline(task="fill-mask")
>>> fill_mask(text, top_k=3)
```

Hopefully, this has given you some more background information about all these tasks and the practical importance of each one. The Transformer really changed the paradigm for how we approach all these different tasks and it's exciting to see how Transformer models are being used to solve problems new and old. In the next [section](tasks_explained), you'll learn **how** ðŸ¤— Transformers work to solve these tasks.