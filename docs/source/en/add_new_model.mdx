<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
-->

# Add a model to ü§ó Transformers

The ü§ó Transformers library is often able to offer new models thanks to community contributors. But this can be a challenging project and requires an in-depth knowledge of the ü§ó Transformers library and the model to implement. At Hugging Face, we're trying to empower more of the community to actively add models and we've put together this guide to walk you through the process of adding a PyTorch model (make sure you have [PyTorch installed](https://pytorch.org/get-started/locally/)).

<Tip>

If you're interested in implementing a TensorFlow model, take a look at the [How to convert a ü§ó Transformers model to TensorFlow](add_tensorflow_model) guide!

</Tip>

Along the way, you'll:

- get insights into open-source best practices
- understand the design principles behind one of the most popular deep learning libraries
- learn how to efficiently test large models
- learn how to integrate Python utilities like `black`, `isort`, and `make fix-copies` to ensure clean and readable code

A Hugging Face team member will be available to help you along the way so you'll never be alone. ü§ó ‚ù§Ô∏è

To get started, open a [New model addition](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&template=new-model-addition.yml) issue for the model you want to see in ü§ó Transformers. If you're not especially picky about contributing a specific model, you can filter by the [New model label](https://github.com/huggingface/transformers/labels/New%20model) to see if there are any unclaimed model requests and work on it.

Once you've opened a new model request, the first step is to get familiar with ü§ó Transformers if you aren't already!

## General overview of ü§ó Transformers

ü§ó Transformers is a very opinionated library with specific design choices. From our experience, we've
found that these fundamental design choices and philosophies are crucial to efficiently scale ü§ó
Transformers while keeping maintenance costs reasonable.

Some good resources for better understanding these design choices are the library [Philosophy](philosophy) and the [Repeat Yourself](https://huggingface.co/blog/transformers-design-philosophy) blog post. The main choices we try to apply to all models are:

- Composition is generally favored over abstraction.
- Duplicating code is not always bad if it strongly improves the readability or accessibility of a model.
- Model files are as self-contained as possible so that when you read the code of a specific model, ideally you only
  have to look at its respective `modeling_my-awesome-model.py` file.

In our opinion, the library's code is not just a means to provide a product. We don't want to just provide the ability to use BERT for
inference, but we also want to be able to improve the underlying code. Our design choices allow anyone reading your code to tweak it and better understand it, not just the person using the model.

### Model overview

Every model shares two base classes: [`PreTrainedModel`] and [`PretrainedConfig`]. A *configuration* contains a model's specific attributes, and the model is created from the configuration. 

For this guide, imagine you're adding a new model, `BrandNewBert`, which means you'll need to create a new configuration and pretrained model class: `BrandNewBertPreTrainedModel` and `BrandNewBertConfig`.

* `BrandNewBertPreTrainedModel` is the model-specific [`PreTrainedModel`] class and it inherits from [`PreTrainedModel`]. The [`PreTrainedModel`] class gives the new model-specific class several important functionalities like [`~PreTrainedModel.from_pretrained`] and
[`~PreTrainedModel.save_pretrained`]. These methods are used for serialization and deserialization, making it simple to save and initialize model weights. The model is always serialized to a `pytorch_model.bin` file.
  
  From the model-specific `BrandNewBertPreTrainedModel` class, you can create models with different heads that inherit from it. For example, you can create a base `BrandNewBertModel` model without a head or a `BrandNewBertForMaskedLM` model with a language modeling head. Other important functionalities of your model, like the `forward()` pass, should be defined in a new `modeling_brand_new_bert.py` script. This keeps the code for the `forward()` function contained in the `modeling_brand_new_bert.py` script so users don't have to go digging through other files to find it.
  
  Notice how there are never more than two levels of abstraction for any model in the library. `BrandNewBertModel` inherits from `BrandNewBertPreTrainedModel` which in turn inherits from [`PreTrainedModel`]. Even the model with a specific head, `BrandNewBertForMaskedLM`, inherits from [`PreTrainedModel`] and only *calls* `BrandNewBertModel` as a component in its forward pass. This design pattern is very important because it helps keep the level of abstraction low.

* `BrandNewBertConfig` is required for every new model and it is always stored as an attribute in [`PreTrainedModel`]. You can access the configuration by:

  ```py
  model = BrandNewBertModel.from_pretrained("brand_new_bert")
  model.config
  ```

  `BrandNewBertConfig` also inherits the basic serialization and deserialization methods, [`~PretrainedConfig.from_pretrained`] and [`~PretrainedConfig.save_pretrained`], from [`PretrainedConfig`]. The configuration is always serialized to a `config.json` file.

<Tip>

Calling [`PreTrainedModel.save_pretrained`] automatically calls [`PretrainedConfig.save_pretrained`] so you don't have to remember to save the configuration in a separate call. This way, both the model and configuration are saved!

</Tip>

### Code style

When coding your new model, keep in mind that ü§ó Transformers is an opinionated library and we have a few quirks of our
own regarding how code should be written. üòä

1. The `forward()` pass of your model should be fully written in the modeling file while being fully independent of other
   models in the library. If you want to reuse a block from another model, copy the code and paste it with a
   `# Copied from` comment on top (see the [RoBERTa self-attention](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160) class
   for a good example).
2. The code should be fully understandable even by a non-native English speaker. This means you should pick
   descriptive variable names and avoid abbreviations. As an example, `activation` is preferred to `act`.
   One-letter variable names are strongly discouraged unless it's an index in a for loop.
3. Generally we prefer longer explicit code to short magical one.
4. Avoid subclassing `nn.Sequential` in PyTorch but subclass `nn.Module` and write the `forward()` pass so that anyone
   using your code can quickly debug it by adding print statements or breaking points.
5. Your function signature should be type-annotated. For the rest, good variable names are way more readable and
   understandable than type annotations.

## Step-by-step recipe

Everyone has different preferences for how to port a model so it can be very helpful for you to take a look at how other contributors have ported models to Hugging Face:

* [Porting GPT2 Model](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28) by [Thomas](https://huggingface.co/thomwolf)
* [Porting WMT19 MT Model](https://huggingface.co/blog/porting-fsmt) by [Stas](https://huggingface.co/stas)

From experience, the most important things to keep in mind when adding a model are:

- Don't reinvent the wheel! Most of the code you will add for the new model already exist
  somewhere in ü§ó Transformers. Take some time to find similar existing models and tokenizers you can copy
  from ([grep](https://www.gnu.org/software/grep/) and [rg](https://github.com/BurntSushi/ripgrep) are your
  friends here). It is that your model's tokenizer is based on one model implementation, and
  your model's modeling code is based on another. For example, FSMT's modeling code is based on BART but FSMT's tokenizer code
  is based on XLM.
- Adding a model is more of an engineering challenge than a scientific challenge. You should spend more time on creating an
  efficient debugging environment than trying to understand all the theoretical aspects of the model in the paper.
- Ask for help when you're stuck! Models are the core components of ü§ó Transformers so we are more
  than happy to help you at every step to add your model. Don't hesitate to ask if you notice you are not making
  progress.

Use the following checklist to make sure you've followed all the steps to successfully add a model:

‚òê (Optional) Understood the model's theoretical aspects<br>
‚òê Prepared ü§ó Transformers dev environment<br>
‚òê Set up debugging environment of the original repository<br>
‚òê Created script that successfully runs the `forward()` pass using the original repository and checkpoint<br>
‚òê Successfully added the model skeleton to ü§ó Transformers<br>
‚òê Successfully converted original checkpoint to ü§ó Transformers checkpoint<br>
‚òê Successfully ran `forward()` pass in ü§ó Transformers that gives identical output to original checkpoint<br>
‚òê Finished model tests in ü§ó Transformers<br>
‚òê Successfully added tokenizer in ü§ó Transformers<br>
‚òê Run end-to-end integration tests<br>
‚òê Finished docs<br>
‚òê Uploaded model weights to the Hub<br>
‚òê Submitted the pull request<br>
‚òê (Optional) Added a demo notebook

Now let's cook up the `BrandNewBert` model!

### Theoretical aspects of BrandNewBert

To begin, we usually recommend you start by getting a good theoretical understanding of `BrandNewBert` by reading the paper. However,
if you prefer to understand these conceptual details *on-the-job*, then it is totally fine to dive directly
into `BrandNewBert`'s codebase. This option might suit you better if you have trouble understanding the `BrandNewBert` paper or if you just enjoy programming more than reading scientific papers. The goal is not to get a deep conceptual understanding of the paper, but to extract the necessary information required to implement the model in ü§ó Transformers. It is totally fine if there are sections of the paper you don't understand!

The main details you should keep in mind when reading the paper are:

- What type of model is `BrandNewBert`? Is it a BERT-like encoder-only model or is it a GPT2-like decoder-only model? BART-like
  encoder-decoder model? Look at the [model_summary](model_summary) if you're not familiar with the differences between the model types.
- What are the applications of `BrandNewBert`? Text classification? Text generation? Seq2Seq tasks such as summarization?
- What makes this model different from BERT, GPT-2 or BART?
- Which of the already existing [ü§ó Transformers models](https://huggingface.co/transformers/#contents) is most
  similar to `BrandNewBert`?
- What type of tokenizer is used? A sentencepiece tokenizer? Word piece tokenizer? Is it the same tokenizer as BERT's or BART's?

After you feel like you have gotten a good overview of the model architecture, you might want to write the Hugging Face
team any questions you have. This can include questions regarding the model's architecture, attention layer, and so on.
We are more than happy to help you!

### Prepare your environment

1. Fork the ü§ó Transformers [repository](https://github.com/huggingface/transformers) by clicking on the **Fork** button on the
   repository's page. This creates a copy of the code under your GitHub user account.

2. Clone your `transformers` fork to your local disk, and add the base repository as a remote:

```bash
git clone https://github.com/[your Github handle]/transformers.git
cd transformers
git remote add upstream https://github.com/huggingface/transformers.git
```

3. Set up a development environment by running the following command:

```bash
python -m venv .env
source .env/bin/activate
pip install -e ".[dev]"
```

and return to the parent directory

```bash
cd ..
```

4. To port `BrandNewBert`, you will also need access to its original repository:

```bash
git clone https://github.com/org_that_created_brand_new_bert_org/brand_new_bert.git
cd brand_new_bert
pip install -e .
```

Now you have set up a development environment to port `BrandNewBert` to ü§ó Transformers!

### Run a pretrained checkpoint with the original repository

Start by working on the original `BrandNewBert` repository. Oftentimes the original implementation is very
"researchy", meaning documentation may be lacking and the code can be difficult to understand. But this should
be exactly your motivation to reimplement `BrandNewBert`. At Hugging Face, one of our main goals is to *help people
stand on the shoulders of giants* which translates very well in practice to taking a working model and rewriting it to make
it as **accessible, user-friendly, and beautiful** as possible. This is the number one motivation to reimplement
models into ü§ó Transformers - trying to make complex and new deep learning technology accessible to **everybody**.

You should start by diving into the original repository, and try to run the official pretrained model. This can often be **the most difficult** step, but from our experience, it is very important to spend some time getting familiar with the original codebase. You need to figure out:

- Where are the pretrained weights?
- How do I load the pretrained weights into the corresponding model?
- How do I run the tokenizer independently from the model?
- Trace one `forward()` pass so you know which classes and functions are required for it. Usually,
  you only have to reimplement those functions.
- Be able to locate the important components of the model: the model class, model subclasses (EncoderModel, DecoderModel), and the self-attention layer (are there multiple different attention layers like self-attention and cross-attention?).
- How can I debug the model in the original environment of the repo? Do I have to add *print* statements, can I
  work with an interactive debugger like *ipdb*, or should I use an efficient IDE to debug the model like PyCharm?

#### Debugging

It is very important you can **efficiently** debug code in the original repository before you start the porting
process! Also, remember that you are working with an open-source library, so do not hesitate to open an issue or even a
pull request in the original repository. The maintainers of this repository will most likely be very happy someone is
looking into their code!

At this point, it is really up to you which debugging environment and strategy you prefer to use to debug the original
model. We strongly advise against setting up a costly GPU environment. Instead, work on a CPU when you start diving
into the original repository and also when you start writing the ü§ó Transformers implementation of the model. Only at
the very end, when the model has already been successfully ported to ü§ó Transformers, should you verify that the model
also works as expected on a GPU.

In general, there are two possible debugging environments for running the original model:

-  [Jupyter notebooks](https://jupyter.org/) or [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)
-  Local Python scripts

The advantage of using Jupyter notebooks is that they allow cell-by-cell execution which can be helpful for: better splitting
logical components from one another, and faster debugging cycles because intermediate results can be stored. Also,
notebooks are often easier to share with other contributors, which is very helpful if you want to ask the Hugging
Face team for help. If you are familiar with Jupyter notebooks, we strongly recommend you work with them.

The obvious disadvantage of Jupyter notebooks is that if you are not used to working with them, you will have to spend
some time adjusting to the new programming environment and you might not be able to use your familiar debugging tools
anymore like `ipdb`.

For each codebase, a good first step is to always load a **small** pretrained checkpoint and to be able to reproduce a
single `forward()` pass with a dummy vector of integers as the input ids. The pseudocode could look like:

```python
model = BrandNewBertModel.load_pretrained_checkpoint("/path/to/checkpoint/")
input_ids = [0, 4, 5, 2, 3, 7, 9]  # vector of input ids
original_output = model.predict(input_ids)
```

Now for your debugging strategy, there are a few you can choose from:

- Decompose the original model into many small testable components and run a `forward()` pass on each of the components to verify they work.
- Decompose the original model into the original *tokenizer* and the original *model*, run a `forward()` pass on each of 
  these, and use intermediate print statements or breakpoints to verify they work.

You are free to choose whichever strategy you like and often, one or the other is more advantageous depending on the original codebase.

If the original codebase allows you to decompose the model into smaller sub-components (for example, the original
codebase can be easily run in eager mode) it is usually worth the effort to do so. There are some significant advantages
in taking the more difficult road in the beginning:

- At a later stage when you're comparing the original model to the Hugging Face implementation, you can automatically
  verify that each individual component matches it's corresponding component in the ü§ó Transformers implementation
  instead of relying on visual comparison via print statements.
- It can give you some rope by decomposing the larger challenge of porting an entire model into smaller individual
  components which can help you structure your work better.
- Separating the model into logical and meaningful components will help you get a better overview of the model's design
  and a better understanding of the model.
- At a later stage, these component-by-component tests help you ensure no regression occurs as you continue to change
  your code.

For a nice example of how to decompose the model into smaller components, take a look at Lysandre's [integration checks
for ELECTRA](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed).

However, if the original codebase is very complex or only allows intermediate components to be run in a compiled mode,
it might be too time-consuming or even impossible to separate the model into smaller testable sub-components (for
example, [T5's MeshTensorFlow](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow) library. For codebases
such as these, you'll often rely on verifying print statements.

No matter which debugging strategy you choose, the recommended procedure is often the same: you should start to debug the
starting layers first and the ending layers last.

We recommend you retrieve the output, either by print statements or sub-component functions, of the following
layers in the following order:

1. Retrieve the input ids passed to the model
2. Retrieve the word embeddings
3. Retrieve the input of the first Transformer layer
4. Retrieve the output of the first Transformer layer
5. Retrieve the output of the following n - 1 Transformer layers
6. Retrieve the output of the whole BrandNewBert Model

Input ids should consist of an array of integers: `input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]`.

The outputs of these layers often consist of multidimensional float arrays and can look like this:

```python
[
    [
        [-0.1465, -0.6501, 0.1993, ..., 0.1451, 0.3430, 0.6024],
        [-0.4417, -0.5920, 0.3450, ..., -0.3062, 0.6182, 0.7132],
        [-0.5009, -0.7122, 0.4548, ..., -0.3662, 0.6091, 0.7648],
        ...,
        [-0.5613, -0.6332, 0.4324, ..., -0.3792, 0.7372, 0.9288],
        [-0.5416, -0.6345, 0.4180, ..., -0.3564, 0.6992, 0.9191],
        [-0.5334, -0.6403, 0.4271, ..., -0.3339, 0.6533, 0.8694],
    ]
],
```

We expect every model added to ü§ó Transformers pass a couple of integration tests. This means the original model and
the reimplemented version in ü§ó Transformers should give the exact same output up to a precision of 0.001! Since it is
common an identical model written in different library framework can give a slightly different output, we accept an
error tolerance of 1e-3 (0.001). It is not enough if the model gives *nearly* the same output, they have to be the
*almost identical*! 

As such, you'll almost certainly compare the intermediate outputs of the ü§ó Transformers model multiple times against
the intermediate outputs of the original implementation of `BrandNewBert`. In situations like this, an **efficient**
debugging environment of the original repository is absolutely critical. Here is some advice is to make your debugging
environment as efficient as possible:

- Find the best way of debugging intermediate results. Is the original repository written in PyTorch? Then you should
  probably take the time to write a longer script that decomposes the original model into smaller sub-components to
  retrieve intermediate values. Is the original repository written in Tensorflow 1? Then you might have to rely on
  TensorFlow print operations like [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) to output
  intermediate values. Is the original repository written in JAX? Then make sure the model is **not jitted** when
  running the `forward()` pass (for example, take a look at this [issue](https://github.com/google/jax/issues/196)).
- Use the smallest pretrained checkpoint you can find. The smaller the checkpoint, the faster your debug cycle becomes.
  It is not efficient if your pretrained model is so big that your `forward()` pass takes more than 10 seconds. If only
  very large checkpoints are available, it might make more sense to create a dummy model in the new environment with
  randomly initialized weights and save those weights for comparison with the ü§ó Transformers version of your model.
- Make sure you are using the easiest way of calling a `forward()` pass in the original repository. Ideally, you want
  to find the function in the original repository that **only** calls a single `forward()` pass. You don't want it to
  also call `predict`, `evaluate`, `forward` or `__call__`. You also don't want to debug a function that calls
  `forward()` multiple times such as when you're generating text.
- Try to separate the tokenization from the model's `forward()` pass. If the original repository shows examples where
  you have to input a string, then try to find out where in the `forward()` call the string input is changed to input
  ids. Start from this point. This might mean you have to write a small script or change the original code so that you
  can directly input the ids instead of an input string.
- Make sure the model in your debugging setup is **not** in training mode! This can often cause the model to yield
  random outputs due to multiple dropout layers in the model. Make sure the `forward()` pass in your debugging
  environment is **deterministic** so that the dropout layers are not used. Or use [`~transformers.set_seed`] if the
  old and new implementations are in the same framework.

Once you're comfortable with a debugging strategy and confident you can efficiently debug the original model, you're ready to start writing some new code in ü§ó Transformers!

### Port BrandNewBert to ü§ó Transformers

Start by going to the clone of your ü§ó Transformers' fork:

```bash
cd transformers
```

For special cases where you are adding a model whose architecture exactly matches the model architecture of an
existing model, you can just reuse the entire model architecture of the existing model. You just need to add a [conversion script](add_new_model#write-a-conversion-script) to convert the checkpoint of `BrandNewBert` from the original repository to a checkpoint compatible with your ü§ó Transformers implementation of `BrandNewBert`.

Otherwise, let's start generating the new model. There are two commands you can run:

- `transformers-cli add-new-model-like` adds a new model like an existing one
- `transformers-cli add-new-model` adds a new model from our template (will look like BERT or Bart depending on the type of model you select)

Either way, you will be prompted to input some basic information about your model. The second command requires you to install `cookiecutter`, a project templating package. For more information about these commands, take a look at the [Adding a new model README](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model).

### Open a Pull Request

Before you start to adapt the automatically generated code, now is the time to open a **Work in progress (WIP)** pull
request ([WIP] Add `BrandNewBert`) on the main ü§ó Transformers repository so that you and the Hugging Face team can work
side-by-side on the model integration.

1. Create a branch with a descriptive name from your main branch:

```bash
git checkout -b add_brand_new_bert
```

2. Commit the automatically generated code:

```bash
git add .
git commit
```

3. Fetch and rebase to current main:

```bash
git fetch upstream
git rebase upstream/main
```

4. Push the changes to your account using:

```bash
git push -u origin a-descriptive-name-for-my-changes
```

5. Once you are satisfied, go to the webpage of your fork on GitHub and click on **Pull Request**. Make sure to add the
   GitHub handle of some Hugging Face team members as reviewers so we can keep up with your progress.

6. Change the PR into a draft by clicking on **Create Draft Pull Request** on the right of the GitHub pull request webpage.

Whenever you have made some progress, don't forget to commit your work and push it to your account so
that it shows in the pull request. You should also make sure to update your work with the current `main` branch from
time to time by doing:

```bash
git fetch upstream
git merge upstream/main
```

In general, any questions you might have regarding the model or your implementation should be asked, discussed or
solved in your PR. This way, the Hugging Face team will always be notified when you are committing new code or if you
have a question. 

It is often very helpful to point the Hugging Face team to your added code so we can quickly understand your problem or
question. To do so, you can go to the **Files changed** tab where you can see all of your changes. Go to the line of
code where you want to ask a question, and click on the + symbol to add a comment. Whenever a question or problem has
been solved, you can click on the **Resolve** button of the comment.

Our team will use the same workflow to open comments when reviewing your code. We recommend asking most questions on
GitHub on your PR. For very general questions that are not useful to the public, feel free to ping the Hugging Face
team by Slack or email.

### Adapt the generated model code for BrandNewBert

Start by focusing only on the model. All the relevant code should be found in the generated files
`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` and
`src/transformers/models/brand_new_bert/configuration_brand_new_bert.py`.

Ok, now you're really ready to start coding! üòä

The generated code in `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` will either have the same architecture as BERT if
it's an encoder-only model or BART if it's an encoder-decoder model. At this point, you should remind yourself what
you've learned in the beginning about the theoretical aspects of the model: How is the model different from BERT or
BART?. Implementing those changes can often mean changing the *self-attention* layer, the order of the normalization
layer, and so on. Again, it is often useful to look at the architecture of existing models with similiar architectures in ü§ó Transformers to
get a better feeling of how your model should be implemented.

**Note** that at this point it's not super important that your code is completely correct or clean. Rather, it is
better to add a rough first draft of the copy-pasted version of the original code into
`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` with all the necessary code. From our experience, it is much more efficient to quickly add a first version of the required code and then iteratively 
improve and correct the code with the conversion script as described in the next section. The only thing that
has to work at this point is that you can instantiate the ü§ó Transformers implementation of `BrandNewBert`. The
following command should work:

```python
from transformers import BrandNewBertModel, BrandNewBertConfig

model = BrandNewBertModel(BrandNewBertConfig())
```

The above command will create a model according to the default parameters defined in `BrandNewBertConfig()` with
random weights, ensuring the `init()` methods of all components works.

### Write a conversion script

Next, you should write a conversion script to convert the checkpoint you used to debug `BrandNewBert` in
the original repository to a checkpoint compatible with your ü§ó Transformers implementation of
`BrandNewBert`. It is not advised to write the conversion script from scratch, but rather, look through
existing conversion scripts in ü§ó Transformers for one that has been used to convert a similar model written in
the same framework as `BrandNewBert`. Usually, it is enough to copy an existing conversion script and
slightly adapt it for your use case. Don't hesitate to ask the Hugging Face team to point you to a similar already conversion script for your model if you can't find one!

- If you are porting a model from TensorFlow to PyTorch, a good starting point might be BERT's conversion script [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)
- If you are porting a model from PyTorch to PyTorch, a good starting point might be BART's conversion script [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)

The following section quickly explains how PyTorch models store layer weights and define layer names. In PyTorch, the
name of a layer is defined by the name of the class attribute you give the layer. To see how it works, let's define a dummy model in
PyTorch called `SimpleModel`:

```python
from torch import nn


class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(10, 10)
        self.intermediate = nn.Linear(10, 10)
        self.layer_norm = nn.LayerNorm(10)
```

Now we can create an instance of this model definition which fills all the weights, `dense`, `intermediate`, and
`layer_norm` with random weights. We can print the model:

```python
model = SimpleModel()
print(model)
```

and the architecture should look like:

```
SimpleModel(
  (dense): Linear(in_features=10, out_features=10, bias=True)
  (intermediate): Linear(in_features=10, out_features=10, bias=True)
  (layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
)
```

As you can see, the layer names are defined by the name of the class attribute in PyTorch. You can print out the weight
values of a specific layer to see that the weights are randomly initialized:

```python
print(model.dense.weight.data)
tensor(
    [
        [-0.0818, 0.2207, -0.0749, -0.0030, 0.0045, -0.1569, -0.1598, 0.0212, -0.2077, 0.2157],
        [0.1044, 0.0201, 0.0990, 0.2482, 0.3116, 0.2509, 0.2866, -0.2190, 0.2166, -0.0212],
        [-0.2000, 0.1107, -0.1999, -0.3119, 0.1559, 0.0993, 0.1776, -0.1950, -0.1023, -0.0447],
        [-0.0888, -0.1092, 0.2281, 0.0336, 0.1817, -0.0115, 0.2096, 0.1415, -0.1876, -0.2467],
        [0.2208, -0.2352, -0.1426, -0.2636, -0.2889, -0.2061, -0.2849, -0.0465, 0.2577, 0.0402],
        [0.1502, 0.2465, 0.2566, 0.0693, 0.2352, -0.0530, 0.1859, -0.0604, 0.2132, 0.1680],
        [0.1733, -0.2407, -0.1721, 0.1484, 0.0358, -0.0633, -0.0721, -0.0090, 0.2707, -0.2509],
        [-0.1173, 0.1561, 0.2945, 0.0595, -0.1996, 0.2988, -0.0802, 0.0407, 0.1829, -0.1568],
        [-0.1164, -0.2228, -0.0403, 0.0428, 0.1339, 0.0047, 0.1967, 0.2923, 0.0333, -0.0536],
        [-0.1492, -0.1616, 0.1057, 0.1950, -0.2807, -0.2710, -0.1586, 0.0739, 0.2220, 0.2358],
    ]
)
```

In the conversion script, you should fill those randomly initialized weights with the exact weights of the
corresponding layer in the checkpoint:

```python
# retrieve matching layer weights, e.g. by
# recursive algorithm
layer_name = "dense"
pretrained_weight = array_of_dense_layer

model_pointer = getattr(model, "dense")

model_pointer.weight.data = torch.from_numpy(pretrained_weight)
```

While filling in the weights, you must verify that each randomly initialized weight of your PyTorch model and its corresponding
pretrained checkpoint weight exactly match in both **shape and name**. To do so, it is **necessary** to add `assert`
statements for the shape:

```python
assert (
    model_pointer.weight.shape == pretrained_weight.shape
), f"Pointer shape of random weight {model_pointer.shape} and array shape of checkpoint weight {pretrained_weight.shape} mismatched"
```

and print out the names of both weights to make sure they match:

```python
logger.info(f"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}")
```

Finally, you should check that **all** required weights are initialized and print out all checkpoint weights that
were not used for initialization to make sure the model is correctly converted.

It is completely normal if the conversion fails with either a wrong shape statement or wrong name assignment. This is
most likely because you used incorrect parameters in `BrandNewBertConfig()`, have a wrong architecture in the ü§ó
Transformers implementation, you have a bug in the `init()` functions of one of the components of the ü§ó Transformers
implementation or you need to transpose one of the checkpoint weights.

This step should be iterated with the previous steps until all the weights of the checkpoint are correctly loaded in
the ü§ó Transformers model. Once you've correctly loaded the checkpoint into the ü§ó Transformers implementation, save
the model under a folder of your choice - `/path/to/converted/checkpoint/folder` - which contain a `pytorch_model.bin`
file and a `config.json` file:

```python
model.save_pretrained("/path/to/converted/checkpoint/folder")
```

### Implement the forward pass*

The next step is to make sure the `forward()` pass is correctly implemented. In [Run a pretrained checkpoint with the original repository](#run-a-pretrained-checkpoint-with-the-original-repository), you already created a script that runs a `forward()`
pass of the model using the original repository. Now you should write an analogous script using the ü§ó Transformers
implementation instead of the original one. It should look like:

```python
model = BrandNewBertModel.from_pretrained("/path/to/converted/checkpoint/folder")
input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]
output = model(input_ids).last_hidden_states
```

It is very likely the ü§ó Transformers implementation and the original model implementation don't give the exact
same output the very first time or the `forward()` pass throws an error. Don't be disappointed, it's expected! First,
you should make sure the `forward()` pass doesn't throw any errors. It often happens that the wrong dimensions are
used leading to a *dimensionality mismatch* error or the wrong data type object is used (`torch.long`
instead of `torch.float32`). Don't hesitate to ask the Hugging Face team for help if you're stuck on certain errors.

The final step to make sure the ü§ó Transformers implementation works correctly is to ensure the outputs are
equivalent to a precision of `1e-3`. Ensure the output shapes are identical. For example, `outputs.shape` should yield the same value for your ü§ó Transformers implementation and the original
implementation. Next, ensure the output values are identical as well. This is one of the most difficult
parts of adding a new model. Some common mistakes for why the outputs are not identical are:

- Some layers were not added, like an *activation* layer or a residual connection was forgotten.
- The word embedding matrix was not tied.
- The wrong positional embeddings are used because the original implementation uses an offset.
- Dropout is applied during the `forward()` pass. To fix this, make sure `model.training=False` and that no dropout
  layer is falsely activated during the `forward()` pass. In other words, pass `self.training`` to [PyTorch's functional dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout).

The best way to fix the problem is usually to compare the `forward()` pass of the original implementation and the ü§ó
Transformers implementation side-by-side and check if there are any differences. Ideally, you should debug and print out
intermediate outputs of both implementations of the `forward()` pass to find the exact position in the network where the ü§ó
Transformers implementation shows a different output than the original implementation. 

First, make sure the hard-coded `input_ids` in both scripts are identical. Next, verify the outputs of the first transformation of
the `input_ids` (usually the word embeddings) are identical. Then work your way up to the very last layer of the
network. At some point, you will notice a difference between the two implementations which should point you to the bug
in the ü§ó Transformers implementation. From our experience, a simple and efficient way to do this is to add many print statements
in both the original implementation and ü§ó Transformers implementation at the same positions in the network
respectively, and then successively remove print statements that show the same values for the intermediate outputs.

When you're confident that both implementations yield the same output, verify the outputs with:

```py
torch.allclose(original_output, output, atol=1e-3)
```

Congratulations, you're done with the most difficult part! The remaining work should be a cakewalk. üòä

### Add all necessary model tests

At this point, you have successfully added a new model! However, it's possible the model does not
fully comply with the required design yet. To make sure the implementation is fully compatible with ü§ó Transformers, all
common tests should pass. When you ran the `transformers-cli add new model` command, it should have automatically added a test file for your model. You can probably find it under `tests/models/brand_new_bert/test_modeling_brand_new_bert.py`. Run this test file to verify all common
tests pass:

```bash
pytest tests/models/brand_new_bert/test_modeling_brand_new_bert.py
```

Once all common tests pass, it is important to ensure that all the nice work you have done is well tested so

- The community can easily understand your work by looking at specific tests of `BrandNewBert`.
- Future changes to your model will not break any important feature of the model.

First, add integration tests which are essentially the same as the debugging scripts
you used earlier to implement the model in ü§ó Transformers. A template of the integration tests has already been added by
`cookiecutter`. It is called `BrandNewBertModelIntegrationTests` and you just need to fill this out. To ensure the integration
tests are passing, run:

```bash
RUN_SLOW=1 pytest -sv tests/models/brand_new_bert/test_modeling_brand_new_bert.py::BrandNewBertModelIntegrationTests
```

<Tip>

In case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`

</Tip>

Second, all features that are unique to `BrandNewBert` should be tested in a separate test under
`BrandNewBertModelTester`/`BrandNewBertModelTest`. This step is often forgotten, but it is extremely useful in two
ways:

- It helps to transfer the knowledge you acquired during the model addition to the community by showing how the
  special features of `BrandNewBert` should work.
- Future contributors can quickly test changes to the model by running those special tests.

### Implement the tokenizer

Next, we should add the tokenizer of `BrandNewBert`. Usually, the tokenizer is equivalent or very similar to an existing tokenizer in ü§ó Transformers.

It is very important to find and extract the original tokenizer file and load it into the ü§ó
Transformers' implementation of the tokenizer.

To ensure the tokenizer works correctly, we recommend creating a script in the original repository first that inputs a string and returns the `input_ids`. It could look similar to this (in pseudocode):

```python
input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."
model = BrandNewBertModel.load_pretrained_checkpoint("/path/to/checkpoint/")
input_ids = model.tokenize(input_str)
```

You might have to take a deeper look into the original repository to find the correct tokenizer function or you
might even have to adapt your clone of the original repository to only output the `input_ids`. Having written
a functional tokenization script that uses the original repository, create an analogous script for ü§ó Transformers. It should look similar to this:

```python
from transformers import BrandNewBertTokenizer

input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."

tokenizer = BrandNewBertTokenizer.from_pretrained("/path/to/tokenizer/folder/")

input_ids = tokenizer(input_str).input_ids
```

When the `input_ids` of the original repository ü§ó Transformers yield the same values, add a tokenizer test file. Just like the modeling test files of `BrandNewBert`, the tokenization test files should contain a couple of hard-coded integration tests.

### Run end-to-end integration tests

Having added the tokenizer, you should also add a couple of end-to-end integration tests (for both the model and the
tokenizer) to `tests/models/brand_new_bert/test_modeling_brand_new_bert.py` in ü§ó Transformers. This test should show
the ü§ó Transformers implementation works as expected on a meaningful example depending on the task your model is
designed for. For example, a meaningful text-to-text example can include a source-to-target-translation pair, an
article-to-summary pair, a question-to-answer pair, etc. If none of the ported checkpoints have been finetuned on a
downstream task, it is enough to simply rely on the model tests. 

In the final step to ensure the model is fully functional, we recommend running all tests on a GPU. It can happen that
you forgot to add some `.to(self.device)` statements to internal tensors of the model, in which case a test would
highlight the error. If you don't have access to a GPU, the Hugging Face team can take care of running those tests for
you.

### Add docstrings

Now that all the necessary functionalities for `BrandNewBert` has been added, you're almost done! The only thing left to add is
a nice docstring and a doc page. The `cookiecutter` tool should have added a template file called
`docs/source/en/model_doc/brand_new_bert.mdx` that you should fill out. Users will usually look at
this page first before using your model so your documentation must be understandable and concise. It is also very useful for
the community if you add some *Tips* to show how the model should be used. Don't hesitate to ping the Hugging Face team
if you need help with the docstrings.

Next, make sure that the docstring added to `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` is
correct and includes all necessary inputs and outputs. For more details, take a look at our [detailed guide](https://github.com/huggingface/transformers/tree/main/docs#writing-documentation---specification) about writing documentation and docstrings. It is always to good keep in mind that documentation is just as important as the code in ü§ó Transformers since the documentation is usually the first impression the community gets about your model.

### Code refactor

Great, now you have added all the necessary code and documentation for `BrandNewBert`! At this point, you should correct any potential
incorrect code style by running:

```bash
make style
```

and verify that your coding style passes the quality check:

```bash
make quality
```

There are a couple of other very strict design tests in ü§ó Transformers that might still be failing, and it'll show up in
the tests of your pull request. This is often because some information is missing in the docstring or incorrect
naming. We'll help you if you're stuck here.

Lastly, it is always a good idea to refactor your code after having ensured the code works correctly. With all
tests passing, now is a good time to go over the added code and do some refactoring.

You have now finished the coding part, congratulations, you are awesome! üòé

### Upload the model to the Hub

To make your model available to the community, convert and upload all your checkpoints to the Hub and add a [model card](https://huggingface.co/docs/hub/model-cards) for each uploaded checkpoint. You can get learn more about how to share a model to the Hub in our [sharing tutorial](model_sharing).

Work alongside the Hugging Face team here to decide on an appropriate name for each
checkpoint and to get the required access to upload the model under the author's organization of
`BrandNewBert`. The [`~PreTrainedMode.push_to_hub`] method (available to all ü§ó Transformers models), is a quick and efficient way to push your checkpoint to the Hub:

```python
brand_new_bert.push_to_hub("brand_new_bert")
# Uncomment the following line to push to an organization.
# brand_new_bert.push_to_hub("<organization>/brand_new_bert")
```

It is worth spending some time creating helpful model cards for each checkpoint. The model cards should highlight the
specific characteristics of this particular checkpoint, such as the dataset it was
pretrained or finetuned on or what downstream tasks can the model be used for. You should also include a code example for how to
correctly use the model.

### Add a notebook

It is very helpful to add a Colab notebook that showcases in detail how `BrandNewBert` can be used for inference and or
finetuned on a downstream task. This is not mandatory to merge your PR, but it is very useful for the community!

### Submit your finished PR

You're done programming now and can move to the last step, which is getting your PR merged into the `main` ü§ó Transformers repository. It is worth taking some time to give your finished
PR a nice description and add comments to your code if you want to point out certain design choices to your
reviewer.

### Share your work 

Congratulations on adding a new model to ü§ó Transformers! ü•≥

This is a major open-source contribution and you should be very proud of your work and achievement. We hope you will continue to contribute to ü§ó Transformers in the future if this process didn't scare you off! But now is the time to tell everyone about your new model:

* Create a [Gradio](https://gradio.app/) demo for users to try out your model or include a notebook that showcases the model in detail. This'll help the community learn more about your awesome work.
* Introduce your model to the world on social media like Twitter and LinkedIn. We'll be more than happy to retweet your posts and help you share your work with the community.
