<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# How ðŸ¤— Transformers solve tasks

In [What ðŸ¤— Transformers can do](task_summary), you learned about natural language processing (NLP), speech and audio, and computer vision tasks and some important applications of them.

This page will take a closer look at how models solve these tasks, and explain what's happening under the hood for each of them. There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models the general idea is the same. Owing to it's flexible architecture, most model's are a variant of an encoder, decoder or encoder-decoder structure. The exception is modern convolutional neural networks (CNNs) which are still used today for computer vision tasks, but we'll also explain how these work.

To keep things relatively brief, for each task, we'll provide a high-level overview of how each of the following models work. The models we'll look at are among the most popular ones for solving a task:

- [wav2vec2](model_doc/wav2vec2) for audio classification and automatic speech recognition (ASR)
- [Vision Transformer (ViT)](model_doc/vit) and[ConvNeXT](model_doc/convnext) for image classification
- [DPR](model_doc/dpr) for image segmentation and object detection
- [GLPN](model_doc/glpn) for depth estimation
- [BERT](model_doc/bert) for NLP tasks like text classification, token classification and question answering that use an encoder
- [GPT2](model_doc/gpt2) for NLP tasks like text generation that use a decoder
- [BART](model_doc/bart) for NLP tasks like summarization and translation that use an encoder-decoder

To use a pretrained model for a downstream task, you only need to add a task-specific head and finetune it on your data.

<Tip>

Before you go further, you should have a basic understanding of the original Transformer architecture and know how encoders, decoders, and attention work. If you're just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information about how Transformers work! 

</Tip>

## Speech and audio

wav2vec2 is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition. This is powerful because it demonstrates you can learn meaningful representations from speech alone and you don't need large amounts of labeled training data. wav2vec2 has four main components:

1. A **feature encoder** (a multilayer convolutional neural network) takes the raw waveform and converts it into a sequence of feature vectors that are each 20ms long. The total receptive field of the feature encoder is 25ms of audio. The feature encoder also normalizes the waveform to zero mean and unit variance.

2. The feature vectors from the encoder are passed to a **quantization module**. The purpose of the quantizer is to learn discrete speech units. Audio waveforms are continuous by nature which means they can't be broken up into separate units the way a sequence of text can be split into words. So what are speech units? Each speech unit is a speech representation sampled from a group of *codebooks*, and each codebook contains a set vocabulary. wav2vec2 has two codebooks with 320 codewords in each codebook. This gives wav2vec2 a total of 102,400 speech units.

    In practice, the feature vectors are multiplied by a quantization matrix with a score for each potential codeword in each codebook. Then the Gumbel softmax chooses the codeword with the highest probability. Finally, the codewords are concatenated to form the final speech unit.

3. About half of the feature vectors from the encoder are randomly masked, and the masked positions are replaced by the same trained feature vector. These are now fed to a **context network**. The context network is a Transformer encoder, but unlike the original Transformer, wav2vec2 adds a relative positional embedding with another convolutional layer.

4. The objective of the context network is to learn the speech representation of the masked positions by solving a **contrastive task**. This basically means the model has to predict the true quantized speech representation from a set of false ones. The contrastive loss encourages the model to maximize the similarity between the context vector and the quantized speech unit (this is the target label). 

Now that wav2vec2 is pretrained, you can finetune it for downstream tasks like automatic speech recognition and audio classification!

### Audio classification

To use the pretrained model for audio classification, you add a sequence classification head on top of the base wav2vec2 model. The sequence classification head is just a linear layer that accepts the hidden states output by wav2vec2 and does a linear transformation to convert them into logits. The cross-entropy loss is then calculated between the logits and target to find the most likely label.

### Automatic speech recognition

To use the pretrained model for automatic speech recognition, you add a language modeling head on top of the base wav2vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is just a linear layer that accepts the hidden states output by wav2vec2 and does a linear transformation to convert them into logits which represent a token class (the number of tokens comes from the vocabulary of the task). The CTC loss is then calculated between the logits and target to find the most likely token.

## Computer vision

There are two ways to computer vision tasks:

1. Use a modern CNN, like ConvNeXT, which continue to rely on convolutional layers but with new network designs.
2. Use a Transformer architecture to split an image into patches and process the patches as a sequence.

<Tip>

A third way is a hybrid Transformer with convolutions (for example, [Convolutional Vision Transformer](model_doc/cvt), or [LeViT](model_doc/levit)) but we won't discuss those here because they just combine the two approaches we examine here.

</Tip>

ViT follows closely to the original Transformer, and it opened the door to vision models beyond convolutional neural networks. This is important because the Transformer architecture is more efficient than CNNs which means it can be pretrained and scaled up to get even better performance on vision tasks. However, this doesn't mean CNNs are obsolete for vision tasks because convolutions are a powerful idea and still used in models today with competitive performance to Transformers. 

ViT and ConvNeXT are commonly used for classification, but for other vision tasks like object detection, segmentation and depth estimation, we'll take a look at DETR and GLPN respectively, which are better suited for those tasks.

### Image classification

ViT and ConvNeXT are both used for image classification; the main difference is that ViT uses a Transformer architecture while ConvNeXT uses convolutions.

#### Transformer

ViT replaces convolutions entirely with a pure Transformer architecture. This is important because the Transformer architecture is more efficient than CNNs which means it can be pretrained and scaled up to get even better performance. If you're familiar with the original Transformer, then you're already most of the way towards understanding ViT.

The main change ViT introduced was in how images are processed:

1. An image is split into square patches. So if you had a 224x224 pixel image, you can split it into 256 14x14 image patches. Just like how text is tokenized into words, an image is "tokenized" into a seqeunce patches. Each patch is flattened by concatenating the pixel values of the height and width of a patch into a single vector. For a 3D image, the height, width, and the number of *channels* are flattened.

    The pixel values are linearly transformed into *patch embeddings* by a 2D convolution, which creates the proper input dimensions. Image patches that are similar are mapped to similar patch embeddings.

2. A *learnable embedding* - a special `[CLS]` token - is added to beginning of the patch embeddings just like in the BERT model. The resulting output with the `[CLS]` token once it gets passed through the encoder is used as the input to the attached classification head; other outputs are ignored. This helps the model learn how to encode a representation of the image.

3. The last thing to add to the patch embedding with the special `[CLS]` token is the *position embedding* because the model doesn't know how the image patches are ordered. The position embeddings are also learnable and have the same size as the patch embeddings. Finally, all of the embeddings are passed to the Transformer encoder.

4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer perceptron head (the classification head which is just a linear layer). Like other classification heads, the MLP head converts the output into logits and calculates the cross-entropy loss to find the most likely label.

ViT's pretraining objective is actually just a classification task so you can use it for image classification! The output can also be used for other tasks beyond classification such as segmentation or regression by passing the output to a decoder.

#### ConvNet

ConvNeXT is a CNN architecture that uses a new and modern network design to improve performance of CNNs. CNNs work particularly well for visual processing due to inherent properties like equivariance and preserving spatial information with sliding windows. ConvNeXT keeps these properties that made CNNs so popular in computer vision, and improves on the architectural design.

To understand ConvNeXT, you'll need an intuitive understanding of convolutions. From a very high-level, a [convolution](glossary#convolution) looks at a small window of the image pixels and computes some features from it such as a certain texture or the curvature of a line. You can feed this output to another convolutional layer, and with each successive layer, the network learns more complex and abstract things like hotdogs or rockets. Between convolutional layers, it is common to add a pooling layer to reduce dimensionality and make the model more robust to variations of a feature's position. 

From here, ConvNeXT makes several changes:

1. High-level architecture design: changes the number of blocks in each stage and "patchifying" an image with a 4x4 stride 4 convolutional layer. The larger stride and correspodning kernel size, and non-overlapping sliding window makes this patchifying strategy similar to how ViT splits an image into patches.

2. Adopt ideas from [ResNeXT](https://arxiv.org/abs/1611.05431): replaces the typical 3x3 convolutional layer in the bottleneck block with *depthwise convolution*, which is just applying a convolution to each input channel separately and then stacking them back together at the end. This widens the network width which improves performance.

3. Inverted bottleneck: a *bottleneck* layer shrinks the number of channels and then restores it because it is faster to do a 1x1 convolution and you can increase the depth. An inverted bottleneck does the opposite by expanding the number of channels first and then shrinking it which is more memory efficient.

4. Big kernel: ViT has a global receptive field which means it can see more of an image at once thanks to its attention mechanism. ConvNeXT attempts to replicate this effect by increasing the kernel size to 7x7.

5. Low-level layer design: many of the changes at this level adopt design choices in Transformers. The ReLU is replaced by GELU (which is used in BERT and ViT), fewer activation layers (the Transformer only uses one activation function in the MLP block), fewer normalization layers and replace BatchNorm with LayerNorm (LayerNorm is used in Transformers), and separate downsampling layers (do spatial downsampling in between stages like [Swin](model_doc/swin)).

All together, these improvements allow ConvNeXT to perform competitively with Transformer models like Swin!

### Object detection

DETR, *DEtection TRansformer*, is an end-to-end object detection model that combines a CNN with a Transformer encoder-decoder. Previous detection models involved several components, each with a specific function like filtering over image regions and cropping features in a location. DETR significantly simplifies this by just passing the image features through a Transformer encoder-decoder.

1. A pretrained CNN *backbone* takes an image, represented by its pixel values (`channels, height, width`), and creates a low resolution activation map of the image features. A 1x1 convolution is applied to the feature map to reduce dimensionality, creating a new feature map that gives a high-level representation of the image. Since the Transformer is a sequential model, the feature map is flattened into a sequence of feature vectors. Like the original Transformer, you need to add positional embeddings to the feature vectors.

2. The feature vectors are passed to the encoder which learns the image representations using its attention layers. These encoder hidden states are passed to the decoder which combines them with *object queries*, to utilize the image features from the encoder.  Object queries are learned embeddings that focus on a different region of an image, and they're updated as they progress through each attention layer. The decoder hidden states are passed to a feed-forward network that predicts the bounding box coordinates and class label for each object query, or alternatively `no object` if there isn't one.

    An interesting difference from the original Transformer is that DETR decodes each object query in parallel to output *N* final predictions. Unlike a typical autoregressive model that predicts one element at a time, object detection is a set prediction task (bounding box, class label) that makes *N* predictions in a single pass.

3. DETR uses *bipartite matching loss* during training to compare a fixed number of predictions with a fixed set of ground truth labels. If there are fewer ground truth labels in the set of *N* labels, then they're padded with a `no object` class. This type of loss function encourages DETR to find a one-to-one assignment between the predictions and ground truth labels. If either the bounding boxes or class label aren't correct, a loss is incurred. Likewise, if DETR predicts an object that doesn't exist, it is also penalized. This encourages DETR to find other objects in an image instead of just focusing on one maybe really prominent object.

### Image segmentation

DETR can also be easily extended for panoptic segmentation by adding a mask head on top of the decoder. Just like how DETR is trained for object detection, for segmentation, DETR is trained to predict the bounding boxes around the *stuff* and *things* in an image. The mask head takes the output from the decoder for each bounding box detected in an image and creates an attention map for each object. The attention maps are passed to a CNN which outputs the mask logits at each pixel. An argmax is used at each logit to predict a class for each pixel, resulting in a final panoptic prediction.

### Depth estimation

GLPN, *Global-Local Path Network*, is another type of Transformer for depth estimation that combines a [SegFormer](model_doc/segformer) encoder with a lightweight decoder. The SegFormer encoder is a hierarchical encoder - also known as a Mix Transformer (MiT) - which simply means it can learn low-level features as well as more abstract high-level things, just like a CNN! This is in contrast to ViT which only creates one feature map from the image.

1. Like ViT, an image is split into a sequence of patches except these image patches are smaller which is better for dense prediction tasks like segmentation or depth estimation. The image patches are transformed into patch embeddings (see the [image classification](#image-classification) section for more details about how patch embeddings are created) which are fed to the encoder.

2. The encoder takes the patch embedding and image features are learned through the attention and a Mix-FFN layer. The purpose of the latter is to provide positional information. At the end of the encoder block, a *patch merging* layer, which is important for creating the hierarchical representations. The features of each group of neighboring patches are concatenated, and a linear layer is applied on the concatenated features which reduces the number of patches to a resolution of 1/4. This becomes the input to the next encoder block where this whole process is repeated until you have image features with resolutions of 1/8, 1/16, and 1/32.

3. A lightweight decoder takes the last feature map (1/32 scale) from the encoder and upsamples it to 1/16 scale. From here, the feature is passed into a *Selective Feature Fusion (SFF)* module which selects and combines local and global features from an attention map for each feature, and then upsamples it to 1/8th. This process is repeated until the decoded features are the same size as the original image. The output is passed through two convolution layers and then a sigmoid function to predict the depth of each pixel.

## Natural language processing

The Transformer was originally designed for machine translation, and since then it has practically become the default architecture for solving all NLP tasks. Some tasks lend themselves to the Transformer's encoder structure while others are better suited for the decoder. Still other tasks make use of both the Transformer's encoder-decoder structure.

### Text classification

[BERT](model_doc/bert) is an encoder-only model and is the first model to effectively implement bidirectionality which allows BERT to learn richer representations of the text by attending to words before and after.

1. BERT uses [WordPiece](tokenizer_summary#wordpiece) tokenization to generate a token embedding of the text. To tell the difference between a single sentence and a pair of sentences, a special `[SEP]` token is added to differentiate them. A special `[CLS]` token is added to the beginning of every sequence of text. The final output with the `[CLS]` token is used as the input to the classification head for classification tasks. BERT also adds a segment embedding to denote whether a token belongs to the first or second sentence in a pair of sentences.

2. BERT is pretrained with two objectives: masked language modeling and next sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked and the model needs to predict these. This solves the issue of deep bidirectionality where the model could cheat and see all the words and "predict" the next word. The final hidden states of the predicted mask tokens are passed to a feed-forward network with a softmax over the vocabulary to predict the masked word.

    The second pretraining object is next sentence prediction. The model must predict whether a sentence B is the next sentence after sentence A. Half of the time sentence B is the next sentence and the other half of the time, sentence B is a random sentence. The prediction, whether it is the next sentence or not, is passed to a feed-forward network with a softmax over the two classes (`IsNext` and `NotNext`).

3. The input embeddings are passed through multiple encoder layers to output some final hidden states. To use the pretrained model for classification, you add a sequence classification head on top of the base BERT model. The sequence classification head is just a linear layer that accepts the final hidden states and does a linear transformation to convert them into logits. The cross-entropy loss is then calculated between the logits and taraget to find the most likely label.

### Token classification

To use BERT for token classification tasks like named entity recognition (NER), you add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and does a linear transformation to convert them into logits. The cross-entropy loss is then calculated between the logits and each token to find the most likely label.

### Question answering

To use BERT for question answering, you add a span classification head on top of the base BERT model. This is a linear layer that accepts the final hidden states and does a linear transformation to compute the `span` start and end logits corresponding to the answer. The cross-entropy loss is then calculated between the logits and the label position to find the most likely span of text corresponding to the answer.

### Text generation

[GPT-2](model_doc/gpt2) is a decoder-only model pretrained on a large amount of text, and is able to generate convincing text given a prompt and can also complete other NLP tasks like question answering despite not being explicitly trained for them.

1. GPT-2 uses [byte pair encoding (BPE)](tokenizer_summary#bytepair-encoding-bpe) to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses a *masked self-attention* layer which means GPT-2 can't attend to future tokens. It is only allowed to attend to tokens on the left. This is different from BERT's [`mask`] token, because in masked self-attention, an attention mask is used to set the score to 0 for future tokens.

2. The output from the decoder is passed to a language modeling head which does a linear transformation to convert the hidden states into logits. The label is simply the next token in the sequence, so to create those, the logits are shifted to the right by one. The cross-entropy loss is then calculated between the shifted logits and the labels to output the next most likely token.

GPT-2's pretraining objective is based entirely on [causal language modeling](glossary#causal-language-modeling), that is, simply predicting the next word in a sequence. This makes GPT-2 especially good at tasks that involve generating text. To use GPT-2 for text generation, call the [`generate`] method to start generating text!

<Tip>

For more information about text generation, check out the [text generation strategies](generation_strategies) guide!

</Tip>

### Summarization

[BART](model_doc/bart) is an encoder-decoder Transformer that combines the best of both worlds: the bidirectional encoder of BERT and the decoder of GPT-2.

1. The encoder architecture is very similar to BERT and accepts a token and positional embedding of the text. BART is pretrained by corrupting the input and then reconstructing it with the decoder. Unlike other encoders with specific corruption strategies, BART can apply any type of corruption. Specificially, the *text infilling* strategy works particuarly well. In text infilling, a number of text spans are replaced with a **single** [`mask`] token. This is important because not only does the model have to predict the masked tokens, it also teaches the model to predict the number of missing tokens. The input embeddings and masked spans are passed through the encoder to output some final hidden states, but unlike BERT, BART doesn't add a final feed-forward network at the end to predict a word.

2. The encoder's output are passed to the decoder which must predict the masked tokens from the encoder's output and any uncorrupted tokens. To help it restore the original text, the decoder has *cross-attention* layers that allow each decoder layer to attend to the final hidden state of the encoder. The output from the decoder is passed to a language modeling head which does a linear transformation to convert the hidden states into logits. The cross-entropy loss is then calculated between the logits and the label, which is just the token shifted to the right.

To use BART for summarization, call the [`generate`] method to start summarizing!

<Tip>

For more information about text generation, check out the [text generation strategies](generation_strategies) guide!

</Tip>

### Translation

BART can also be used for translation by making a small addition to the architecture. A separate randomly initialized encoder is added to map a source language into an input that BART can decode into the target language. The embeddings from this new encoder is passed to the pretrained encoder instead of the original word embeddings. The source encoder is trained by updating the source encoder, positional embeddings and input embeddings with the cross-entropy loss from the model output. In this first step, the model parameters are frozen and in the second step, all the model parameters are trained together.

BART has since been followed up by mBART, a multilingual version of BART primarily intended for translation and pretrained on many different languages.

To use BART or mBART for translation, call the [`generate`] method to start translating!

<Tip>

For more information about text generation, check out the [text generation strategies](generation_strategies) guide!

</Tip>