<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# How ðŸ¤— Transformers solve tasks

In [What ðŸ¤— Transformers can do](task_summary), you learned how natural language processing (NLP), speech and audio, and computer vision problems used to be solved before the Transformer was introduced and some important applications of these problems. 

This page will dig deeper into how Transformer models solve these tasks, and explain what's happening under the hood for each of them. Each Transformer model is a bit different - some may implement certain tricks or techniques - but the general idea is the same. So to keep things relatively brief, for each task, we'll discuss how a specific Transformer model works. Then, to use the pretrained model for a specific downstream task, you only need to add a task-specific head on top and finetune it on your data.

The Transformer models we'll look at are among the most common ones in their field, and they'll provide you with a good understanding of how they solve a task:

- [wav2vec2](model_doc/wav2vec2) for audio classification and automatic speech recognition
- [Vision Transformer (ViT)](model_doc/vit) for image classification
- [DPR](model_doc/dpr) for image segmentation and object detection
- [GLPN](model_doc/glpn) for depth estimation
- [BERT](model_doc/bert) for NLP tasks like text classification that use an encoder, [GPTJ](model_doc/gptj) for NLP tasks like text generation that use a decoder, and [T5](model_doc/t5) for NLP tasks like summarization that use both an encoder and a decoder

<Tip>

Before you go further, you should have a basic understanding of the original Transformer architecture and know how encoders, decoders, and attention work. If you're just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information about how Transformers work.

</Tip>

## Speech and audio

wav2vec2 is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition. This is powerful because it demonstrates you can learn meaningful representations from speech alone and you don't need large amounts of labeled training data. wav2vec2 has four main components:

1. A **feature encoder** (a multilayer convolutional neural network) takes the raw waveform and converts it into a sequence of feature vectors that are each 20ms long. The total receptive field of the feature encoder is 25ms of audio. The feature encoder also normalizes the waveform to zero mean and unit variance.

2. The feature vectors from the encoder are passed to a **quantization module**. The purpose of the quantizer is to learn discrete speech units. Audio waveforms are continuous by nature which means they can't be broken up into separate units the way a sequence of text can be split into words. So what are speech units? Each speech unit is a speech representation sampled from a group of *codebooks*, and each codebook contains a set vocabulary. wav2vec2 has two codebooks with 320 codewords in each codebook. This gives wav2vec2 a total of 102,400 speech units.

    In practice, the feature vectors are multiplied by a quantization matrix with a score for each potential codeword in each codebook. Then the Gumbel softmax chooses the codeword with the highest probability. Finally, the codewords are concatenated to form the final speech unit.

3. About half of the feature vectors from the encoder are randomly masked, and the masked positions are replaced by the same trained feature vector. These are now fed to a **context network**. The context network is a Transformer encoder, but unlike the original Transformer, wav2vec2 adds a relative positional embedding with another convolutional layer.

4. The objective of the context network is to learn the speech representation of the masked positions by solving a **contrastive task**. This basically means the model has to predict the true quantized speech representation from a set of false ones. The contrastive loss encourages the model to maximize the similarity between the context vector and the quantized speech unit (this is the target label). 

Now that wav2vec2 is pretrained, you can finetune it for downstream tasks like automatic speech recognition and audio classification!

### Audio classification

To use the pretrained model for audio classification, you add a sequence classification head on top of the base wav2vec2 model. The sequence classification head is just a linear layer that accepts the hidden states output by wav2vec2 and does a linear transformation to convert them into logits. The cross-entropy loss is then calculated between the logits and target to find the most likely label.

You can see this in [`Wav2Vec2ForSequenceClassification.forward`].

### Automatic speech recognition

To use the pretrained model for automatic speech recognition, you add a language modeling head on top of the base wav2vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is just a linear layer that accepts the hidden states output by wav2vec2 and does a linear transformation to convert them into logits which represent a token class (the number of tokens comes from the vocabulary of the task). The CTC loss is then calculated between the logits and target to find the most likely token.

You can see this in [`Wav2Vec2ForCTC.forward`].

## Computer vision

ViT follows closely to the original Transformer, and it opened the door to vision models beyond convolutional neural networks. This is important because the Transformer architecture is more efficient than CNNs which means it can be pretrained and scaled up to get even better performance on vision tasks. 

ViT is commonly used for classification, but for other vision tasks like object detection, segmentation and depth estimation, we'll take a look at DETR and GLPN respectively, which are better suited for those tasks.

### Image classification

ViT replaces convolutions entirely with a pure Transformer architecture. This is important because the Transformer architecture is more efficient than CNNs which means it can be pretrained and scaled up to get even better performance. If you're familiar with the original Transformer, then you're already most of the way towards understanding ViT.

The main change ViT introduced was in how images are processed:

1. An image is split into square patches. So if you had a 224x224 pixel image, you can split it into 256 14x14 image patches. Just like how text is tokenized into words, an image is "tokenized" into a seqeunce patches. Each patch is flattened by concatenating the pixel values of the height and width of a patch into a single vector. For a 3D image, the height, width, and the number of *channels* are flattened.

    The pixel values are linearly transformed into *patch embeddings* by a 2D convolution, which creates the proper input dimensions. Image patches that are similar are mapped to similar patch embeddings.

2. A *learnable embedding* - a special `[CLS]` token - is added to beginning of the patch embeddings just like in the BERT model. The resulting output with the `[CLS]` token once it gets passed through the encoder is used as the input to the attached classification head; other outputs are ignored. This helps the model learn how to encode a representation of the image.

3. The last thing to add to the patch embedding with the special `[CLS]` token is the *position embedding* because the model doesn't know how the image patches are ordered. The position embeddings are also learnable and have the same size as the patch embeddings. Finally, all of the embeddings are passed to the Transformer encoder.

4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer perceptron head (the classification head which is just a linear layer). Like other classification heads, the MLP head converts the output into logits and calculates the cross-entropy loss to find the most likely label.

ViT's pretraining objective is actually just a classification task so you can use it for image classification! The output can also be used for other tasks beyond classification such as segmentation or regression by passing the output to a decoder.

### Object detection

DETR, *DEtection TRansformer*, is an end-to-end object detection model that combines a CNN with a Transformer encoder-decoder. Preview detection models involved several components, each with a specific function like filtering over image regions and cropping features in a location. DETR significantly simplifies this by just passing the image features through a Transformer encoder-decoder.

1. A pretrained CNN *backbone* takes an image, represented by its pixel values (`channels, height, width`), and creates a low resolution activation map of the image features. A 1x1 convolution is applied to the feature map to reduce dimensionality, creating a new feature map that gives a high-level representation of the image. Since the Transformer is a sequential model, the feature map is flattened into a sequence of feature vectors. Like the original Transformer, you need to add positional embeddings to the feature vectors.

2. The feature vectors are passed to the encoder which learns the image representations using its attention layers. These encoder hidden states are passed to the decoder which combines them with *object queries*, to utilize the image features from the encoder.  Object queries are learned embeddings that focus on a different region of an image, and they're updated as they progress through each attention layer. The decoder hidden states are passed to a feed-forward network that predicts the bounding box coordinates and class label for each object query, or alternatively `no object` if there isn't one.

    An interesting difference from the original Transformer is that DETR decodes each object query in parallel to output *N* final predictions. Unlike a typical autoregressive model that predicts one element at a time, object detection is a set prediction task (bounding box, class label) that makes *N* predictions in a single pass.

3. DETR uses *bipartite matching loss* during training to compare a fixed number of predictions with a fixed set of ground truth labels. If there are fewer ground truth labels in the set of *N* labels, then they're padded with a `no object` class. This type of loss function encourages DETR to find a one-to-one assignment between the predictions and ground truth labels. If either the bounding boxes or class label aren't correct, a loss is incurred. Likewise, if DETR predicts an object that doesn't exist, it is also penalized. This encourages DETR to find other objects in an image instead of just focusing on one maybe really prominent object.

You can see all of this logic implemented in [`DeTRForObjectDetection.forward`].

### Image segmentation

DETR can also be easily extended for panoptic segmentation by adding a mask head on top of the decoder. Just like how DETR is trained for object detection, for segmentation, DETR is trained to predict the bounding boxes around the *stuff* and *things* in an image. The mask head takes the output from the decoder for each bounding box detected in an image and creates an attention map for each object. The attention maps are passed to a CNN which outputs the mask logits at each pixel. An argmax is used at each logit to predict a class for each pixel, resulting in a final panoptic prediction.

You can see all of this logic implemented in [`DeTRForSegmentation.forward`].

### Depth estimation

GLPN, *Global-Local Path Network*, is another type of Transformer for depth estimation that combines a [SegFormer](model_doc/segformer) encoder with a lightweight decoder. The SegFormer encoder is a hierarchical encoder - also known as a Mix Transformer (MiT) - which simply means it can learn low-level features as well as more abstract high-level things, just like a CNN! This is in contrast to ViT which only creates one feature map from the image.

1. Like ViT, an image is split into a sequence of patches except these image patches are smaller which is better for dense prediction tasks like segmentation or depth estimation. The image patches are transformed into patch embeddings (see the [image classification](#image-classification) section for more details about how patch embeddings are created) which are fed to the encoder.

2. The encoder takes the patch embedding and image features are learned through the attention and a Mix-FFN layer. The purpose of the latter is to provide positional information. At the end of the encoder block, a *patch merging* layer, which is important for creating the hierarchical representations. The features of each group of neighboring patches are concatenated, and a linear layer is applied on the concatenated features which reduces the number of patches to a resolution of 1/4. This becomes the input to the next encoder block where this whole process is repeated until you have image features with resolutions of 1/8, 1/16, and 1/32.

3. A lightweight decoder takes the last feature map (1/32 scale) from the encoder and upsamples it to 1/16 scale. From here, the feature is passed into a *Selective Feature Fusion (SFF)* module which selects and combines local and global features from an attention map for each feature, and then upsamples it to 1/8th. This process is repeated until the decoded features are the same size as the original image. The output is passed through two convolution layers and then a sigmoid function to predict the depth of each pixel.

You can see all of this logic implemented in [`GLPNForDepthEstimation.forward`].