<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# PoNet

## Overview

The PoNet model was proposed in [PoNet: Pooling Network for Efficient Token Mixing in Long Sequences](https://openreview.net/pdf?id=9jInD9JjicF) by Chao-Hong Tan, Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, Zhen-Hua Ling.
We propose a novel Pooling Network for token mixing with linear complexity, achieve competitive performance on the Long Range Arena benchmark, and 95.7% of the accuracy of BERT on the GLUE demonstrating its transferability.

The abstract from the paper is the following:

Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also conduct systematic studies on the transfer learning capability of PoNet and observe that PoNet achieves 95.7 percent of the accuracy of BERT on the GLUE benchmark, outperforming FNet by 4.5 percent relative. Comprehensive ablation analysis demonstrates effectiveness of the designed multi-granularity pooling and pooling fusion for token mixing in long sequences and efficacy of the designed pre-training tasks for PoNet to learn transferable contextualized language representations.

Tips:

- PoNet is an encoder architecture that can be used for efficient comprehension of long sentences.
- For applications with long sentences, i.e. longer than 512, it needs to extend position embeddings. Please refer to the following link: https://github.com/lxchtan/PoNet/blob/main/run_long_classification.py#L297.

This model was contributed by [chtan](https://huggingface.co/chtan).
The original code can be found [here](https://github.com/lxchtan/PoNet).


## PoNetConfig

[[autodoc]] PoNetConfig
    - all

## PoNetTokenizer

[[autodoc]] PoNetTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## PoNet specific outputs

[[autodoc]] models.ponet.modeling_ponet.PoNetForPreTrainingOutput

## PoNetModel

[[autodoc]] PoNetModel
    - forward

## PoNetForPreTraining

[[autodoc]] PoNetForPreTraining
    - forward

## PoNetLMHeadModel

[[autodoc]] PoNetLMHeadModel
    - forward

## PoNetForMaskedLM

[[autodoc]] PoNetForMaskedLM
    - forward

## PoNetForNextSentencePrediction

[[autodoc]] PoNetForNextSentencePrediction
    - forward

## PoNetForSequenceClassification

[[autodoc]] PoNetForSequenceClassification
    - forward
