<!--Copyright 2023 The Better Planet Investments, labml.ai and The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# GeoV

## Overview

The GeoV model was designed by Georges Harik and uses [Rotary Positional Embeddings with Relative distances (RoPER)](http://research.labml.ai/RoPER.html) by [Georges Hark](https://twitter.com/ghark) and [Varuna Jayasiri](https://twitter.com/vpj).

[RoPER]((http://research.labml.ai/RoPER.html), in addition to using relative positions in the attention score calculation by RoPE embeddings, adds relative positional information explicitly to value embeddings. Specifically, it incorporates the relative positions of the tokens paid attention to. RoPER gives better performance in algorithmic tasks. Results have shown an improvement over RoPE in a language modeling setting on a 3 billion parameter transformer.

The GeoV tokenizer uses [SentencePiece](https://github.com/google/sentencepiece) [unigram language model](https://arxiv.org/abs/1804.10959) and tokenizes symbols, digits and new line characters separately, in order to achieve better performance on mathematical content and code.

This model was contributed by [gharik](https://huggingface.co/gharik) and [vpj](https://huggingface.co/vpj).

We plan to to release checkpoints around every 20b tokens trained from here until around 300b tokens.
We will also train smaller and larger versions. Our aim is to have broadly available smaller and larger models.

## Generation

The `generate()` method can be used to generate text using GeoV model.

```python
>>> from transformers import GeoVForCausalLM, GeoVTokenizer

>>> model = GeoVForCausalLM.from_pretrained("GoeV/GeoV-9b")
>>> tokenizer = GeoVTokenizer.from_pretrained("GoeV/GeoV-9b")

>>> prompt = "In mathematics, topology is the study of"

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

## GeoVConfig

[[autodoc]] GeoVConfig

## GeoVTokenizer

[[autodoc]] GeoVTokenizer

## GeoVModel

[[autodoc]] GeoVModel
    - forward

## GeoVForCausalLM

[[autodoc]] GeoVForCausalLM
    - forward
