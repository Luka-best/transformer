<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
-->

# Efficient Training on CPU

This guide focuses on training large models efficiently on CPU.

## Mixed precision with IPEX

IPEX is optimized for CPUs with AVX-512 or above, and functionally works for CPUs with only AVX2. So, it is expected to bring performance benefit for Intel CPU generations with AVX-512 or above while CPUs with only AVX2 (e.g., AMD CPUs or older Intel CPUs) might result in a better performance under IPEX, but not guaranteed. IPEX provides performance optimizations for CPU training with both Float32 and BFloat16. The usage of BFloat16 is the main focus of the following sections.

Low precision data type BFloat16 has been natively supported on the 3rd Generation Xeon® Scalable Processors (aka Cooper Lake) with AVX512 instruction set and will be supported on the next generation of Intel® Xeon® Scalable Processors with Intel® Advanced Matrix Extensions (Intel® AMX) instruction set with further boosted performance. The Auto Mixed Precision for CPU backend has been enabled since PyTorch-1.10. At the same time, the support of Auto Mixed Precision with BFloat16 for CPU and BFloat16 optimization of operators has been massively enabled in Intel® Extension for PyTorch, and partially upstreamed to PyTorch master branch. Users can get better performance and user experience with IPEX Auto Mixed Precision.

Check more detailed information for [Auto Mixed Precision](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/amp.html).

### IPEX installation:

IPEX release is following PyTorch, to install via pip:

| PyTorch Version   | IPEX version   |
| :---------------: | :----------:   |
| 1.13              |  1.13.0+cpu    |
| 1.12              |  1.12.300+cpu  |
| 1.11              |  1.11.200+cpu  |
| 1.10              |  1.10.100+cpu  |

```
pip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu
```

Check more approaches for [IPEX installation](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html).

### Usage in Trainer
To enable auto mixed precision with IPEX in Trainer, users should add `use_ipex`, `bf16` and `no_cuda` in training command arguments.

Take an example of the use cases on [Transformers question-answering](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)

- Training with IPEX using BF16 auto mixed precision on CPU:
<pre> python run_qa.py \
--model_name_or_path bert-base-uncased \
--dataset_name squad \
--do_train \
--do_eval \
--per_device_train_batch_size 12 \
--learning_rate 3e-5 \
--num_train_epochs 2 \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/debug_squad/ \
<b>--use_ipex \</b>
<b>--bf16 --no_cuda</b></pre> 

### Performance Examples

[Bert-base-cased](https://huggingface.co/bert-base-cased?text=Paris+is+the+%5BMASK%5D+of+France.), [Distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased?text=The+goal+of+life+is+%5BMASK%5D.) and [Bert-large-uncased](https://huggingface.co/bert-large-uncased?text=Paris+is+the+%5BMASK%5D+of+France.) are selected as examples with different model sizes to demonstrate the BFloat16 training performance achieved by IPEX and speed-up ratio against Stock PyTorch FP32 on the 3rd Generation Xeon® Scalable Processors (Intel® Xeon® Platinum 8380H CPU). All the benchmarkings are using [IPEX CPU launcher](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/launch_script.html) to set system configs to ensure a best-to-best comparison, like numactl (--node_id 0 , i.e., one socket full cores) and jemalloc (--enable_jemalloc).


The examples are refering to the [Text-classification](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) (based on the script [run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py)), and SST-2, STSB and RTE are scoped from the [GLUE](https://gluebenchmark.com/) benchmark of fine-tuning the library models for sequence classification (taking samples/second as the performance metric).

#### Software configuration:
| Software | Version|
|-------|------------------------------|
| PyTorch  | v1.12.1+cpu  |
| IPEX  | v1.12.100+cpu  |
|Transformers| [main](https://github.com/huggingface/transformers/commit/0ea53822f8bdc2c0c41b45bcd04fa6c031e5e700) |

#### Training configuration:
| GLUE Task  | Setups |
|-------|------------------------------|
| SST-2; STS-B; RTE |per_device_train_batch_size=128 ; max_seq_len=128 ; num_train_epochs=3 ; learning_rate=2e-5|

#### Performance speed-up ratio:

<p>
<table>
<tbody>
  <tr>
    <th rowspan="2">Workload</th>
    <th colspan="2">Training samples per second</th>
  </tr>
  <tr>
    <th>Task</th>
    <th>IPEX BFloat16  vs PT Float32</th>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle" rowspan="10">Bert-base-cased</td>    
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle">SST-2</td>
    <td style="text-align: center; vertical-align: middle">1.72x</td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle">STS-B</td>
    <td style="text-align: center; vertical-align: middle">1.69x</td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle">RTE</td>
    <td style="text-align: center; vertical-align: middle">1.68x</td>
  </tr>
</tbody>
</table>
</p>

<p>
<table>
<tbody>
  <tr>
    <th rowspan="2">Workload</th>
    <th colspan="2">Training samples per second</th>
  </tr>
  <tr>
    <th scope="col">Task</th>
    <th scope="col">IPEX BFloat16  vs PT Float32</th>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle" rowspan="10">Distilbert-base-uncased</td>    
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle">SST-2</td>
    <td style="text-align: center; vertical-align: middle">1.69x</td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle">STS-B</td>
    <td style="text-align: center; vertical-align: middle">1.69x</td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle">RTE</td>
    <td style="text-align: center; vertical-align: middle">1.67x</td>
  </tr>
</tbody>
</table>
</p>

<p>
<table>
<tbody>
  <tr>
    <th rowspan="2">Workload</th>
    <th colspan="2">Training samples per second</th>
  </tr>
  <tr>
    <th scope="col">Task</th>
    <th scope="col">IPEX BFloat16  vs PT Float32</th>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle" rowspan="10">Bert-large-uncased</td>    
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle">SST-2</td>
    <td style="text-align: center; vertical-align: middle">1.79x</td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle">STS-B</td>
    <td style="text-align: center; vertical-align: middle">1.78x</td>
  </tr>
</tbody>
</table>
</p>

