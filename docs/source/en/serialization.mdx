<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Export to ONNX

If you need to deploy ðŸ¤— Transformers models in production environments, we recommend
exporting them to a serialized format that can be loaded and executed on specialized
runtimes and hardware. In this guide, we'll show you how to export ðŸ¤— Transformers
models to [ONNX (Open Neural Network eXchange)](http://onnx.ai).

To do that, it is possible to use the [`optimum.exporters.onnx` tool](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model).
The `transformers.onnx` command-line tool also exists, and may be the one you are used to. The `optimum.exporters.onnx` tool is very similar,
but integrates better we the rest of the [ðŸ¤— Optimum library](https://github.com/huggingface/optimum) to perform various optimizations and
inference with the exported ONNX models.


<Tip>

Once exported, a model can be optimized for inference via techniques such as
quantization and graph optimization. If you are interested in optimizing your models to run with
maximum efficiency, check out the [ðŸ¤— Optimum
library](https://github.com/huggingface/optimum).

</Tip>


# Contributing the export to ONNX for a new architecture

Although `optimum.exporters.onnx` already supports a wide variety of models, it is possible that the model you want to export
is not supported yet. You can either a feature request in the [ðŸ¤— Optimum library](https://github.com/huggingface/optimum) repo,
or contribute and add support for the architecture yourself!
You can learn how to add support for a model [here](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute).
