<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Language modeling

Language modeling fits a model to a corpus. There are two forms of language modeling:

- Causal language modeling (CLM): CLM predicts the next token in a sequence of tokens, and the model is only allowed to attend to tokens on the left.
- Masked language modeling (MLM): MLM predicts a masked token in a sequence, and the model is allowed to attend to tokens bidirectionally.

This guide will show you how to fine-tune a pretrained model on the [ELI5](https://huggingface.co/datasets/eli5) dataset for both forms of language modeling.

<Tip>

See the text generation [task page](https://huggingface.co/tasks/text-generation) and fill mask [task page](https://huggingface.co/tasks/fill-mask) for more information about models, datasets and metrics.

</Tip>

## Load ELI5 dataset

Let's use the [r/askscience](https://www.reddit.com/r/askscience/) subset of the ELI5 dataset from the ðŸ¤— Datasets library. Load only the first 5000 rows of this dataset since it is quite large:

```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5", split="train_asks[:5000]")
```

Split this dataset into a train and test set:

```py
eli5 = eli5.train_test_split(test_size=0.2)
```

Then take a look at an example from the dataset:

```py
>>> eli5["train"][0]
{'answers': {'a_id': ['c3d1aib', 'c3d4lya'],
  'score': [6, 3],
  'text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
   "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"]},
 'answers_urls': {'url': []},
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls': {'url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg']},
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls': {'url': []}}
```

Notice the `text` field is actually nested inside the `answers` dictionary! When you preprocess the data, you will need to extract the `text` subfield into it's own column.

## Preprocess

Load the DistilGPT2 tokenizer to process the `text` field:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
```

Extract the `text` subfield from it's nested structure with the [`flatten()`](https://huggingface.co/docs/datasets/process.html#flatten) method:

```py
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
{'answers.a_id': ['c3d1aib', 'c3d4lya'],
 'answers.score': [6, 3],
 'answers.text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
  "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"],
 'answers_urls.url': [],
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls.url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg'],
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls.url': []}
```

Each of the subfields are now their own columns, as indicated by the `answers` prefix. There is one more step you need to include in your preprocessing function. The `answers.text` field should be a string instead of a list. You can create a preprocessing function to convert the list to a string, and add a truncation strategy:

```py
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]], truncation=True)
```

Use ðŸ¤— Datasets [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) function to apply the preprocessing function over the entire dataset. You can speed up the preprocessing by increasing the number of processes, and remove the columns you don't need:

```py
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=[
...         "q_id",
...         "title",
...         "selftext",
...         "document",
...         "subreddit",
...         "answers.a_id",
...         "answers.text",
...         "answers.score",
...         "title_urls.url",
...         "selftext_urls.url",
...         "answers_urls.url",
...     ],
... )
```

Now you need a second preprocessing function to:

- Concatenate all the text.
- Split the result into smaller chunks defined by `block_size`.
- Create a copy of the inputs for the labels because the model shifts the labels to the right.

```py
>>> block_size = 128


>>> def group_texts(examples):
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     total_length = (total_length // block_size) * block_size
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     result["labels"] = result["input_ids"].copy()
...     return result
```

Apply the `group_texts` function over the dataset:

```py
>>> lm_dataset = tokenized_eli5.map(
...     group_texts,
...     batched=True,
...     num_proc=4,
... )
```

## Causal language modeling

CLM is often used to generate text, or map a sequence of text to another which is useful for tasks like summarization and translation. This gives CLM a wide range of abilities as long as you can map one sequence to another sequence. This section of the guide shows you how to fine-tune a causal language model to generate new text.

### Fine-tune with `Trainer`

Load [DistilGPT2](https://huggingface.co/distilgpt2) with [`AutoModelForCausalLM`]:

```py
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
```

At this point, only three steps remain:

1. Define your training hyperparameters in [`TrainingArguments`].
2. Pass the training arguments to [`Trainer`] along with the model and datasets.
3. Call [`~transformers.Trainer.train`] to fine-tune your model.

```py
>>> training_args = TrainingArguments(
...     output_dir="./results",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     weight_decay=0.01,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
... )

>>> trainer.train()
```

### Fine-tune with TensorFlow

To fine-tune a model in TensorFlow is just as easy, with only a few differences.

Start by batching the processed examples together with the [`DefaultDataCollator`] function. Make sure you set `return_tensors="tf"` to return `tf.Tensor` outputs instead of PyTorch tensors!

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")
```

Convert your datasets to the `tf.data.Dataset` format with [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset), and specify inputs and labels in the `columns` argument:

```py
>>> tf_train_set = lm_dataset["train"].to_tf_dataset(
...     columns=["attention_mask", "input_ids", "labels"],
...     dummy_labels=True,
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = lm_dataset["test"].to_tf_dataset(
...     columns=["attention_mask", "input_ids", "labels"],
...     dummy_labels=True,
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

Set up an optimizer function, learning rate schedule, and some training hyperparameters:

```py
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

Load DistilGPT2 with [`TFAutoModelForCausalLM`]:

```py
>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("distilgpt2")
```

Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)
```

Finally, fine-tune the model by calling [`fit`](https://keras.io/api/models/model_training_apis/#fit-method):

```py
>>> model.fit(
...     tf_train_set,
...     validation_data=tf_test_set,
...     epochs=3,
... )
```

## Masked language modeling

MLM is also known as a fill-mask task because it aims to predict a masked token in a sequence. These types of models require a good contextual understanding of the entire sequence, as opposed to only the left context. This section of the guide shows you how to fine-tune a masked language model to predict the masked word.

### Fine-tune with `Trainer`

Load [DistilRoBERTa](https://huggingface.co/distilroberta-base) with [`AutoModelForMaskedlM`]:

```py
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("distilroberta-base")
```

Use [`DataCollatorForLanguageModeling`] to batch the examples together. You didn't explicitly specify this during CLM because you were only using the default parameters. For MLM, you want to randomly mask the tokens each time you iterate over the data.

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

At this point, only three steps remain:

1. Define your training hyperparameters in [`TrainingArguments`].
2. Pass the training arguments to [`Trainer`] along with the model, datasets and data collator.
3. Call [`~transformers.Trainer.train`] to fine-tune your model.

```py
>>> training_args = TrainingArguments(
...     output_dir="./results",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     num_train_epochs=3,
...     weight_decay=0.01,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
...     data_collator=data_collator,
... )

>>> trainer.train()
```

### Fine-tune with TensorFlow

To fine-tune a model in TensorFlow is just as easy, with only a few differences.

Start by batching the processed examples together with the [`DataCollatorForLanguageModeling`] function. Make sure you set `return_tensors="tf"` to return `tf.Tensor` outputs instead of PyTorch tensors!

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

Convert your datasets to the `tf.data.Dataset` format with [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset), and specify inputs and labels in the `columns` argument:

```py
>>> tf_train_set = lm_dataset["train"].to_tf_dataset(
...     columns=["attention_mask", "input_ids", "labels"],
...     dummy_labels=True,
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = lm_dataset["test"].to_tf_dataset(
...     columns=["attention_mask", "input_ids", "labels"],
...     dummy_labels=True,
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

Set up an optimizer function, learning rate schedule, and some training hyperparameters:

```py
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

Load DistilRoBERTa with [`TFAutoModelForMaskedLM`]:

```py
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForCausalLM.from_pretrained("distilroberta-base")
```

Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)
```

Finally, fine-tune the model by calling [`fit`](https://keras.io/api/models/model_training_apis/#fit-method):

```py
>>> model.fit(
...     tf_train_set,
...     validation_data=tf_test_set,
...     epochs=3,
... )
```

<Tip>

For a more in-depth example of how to fine-tune a model for causal language modeling, take a look at the corresponding
[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb)
or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling-tf.ipynb).

</Tip>
