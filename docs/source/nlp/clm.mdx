<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Causal language modeling

Language modeling fits a model to a corpus. There are two forms of language modeling, masked language modeling (MLM) and causal language modeling (CLM). CLM predicts the next token in a sequence of tokens, and the model is only allowed to attend to tokens on the left. This guide will show you how to fine-tune a pretrained model on the [ELI5](https://huggingface.co/datasets/lvwerra/codeparrot-clean) dataset to generate text.

<Tip>

See the text generation [task page](https://huggingface.co/tasks/text-generation) for more information about other forms of text generation, models, datasets and metrics.

</Tip>

## Load ELI5 dataset

Load the ELI5 dataset from the ðŸ¤— Datasets library:

```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5")
```

There are several fields in this dataset, but you will only use the text in the `` field to generate

```py
>>> eli5["train_asks"][0]

```