import numpy as np


np.random.seed(2)
from PIL import Image

from src.transformers.models.vit_mae.modeling_tf_vit_mae import (
    TFViTMAEForPreTraining,
)
from transformers import ViTFeatureExtractor, ViTMAEConfig


# Load a test image from disk and prepare it for inference.
image = Image.open("./tests/fixtures/tests_samples/COCO/000000039769.png")
feature_extractor = ViTFeatureExtractor.from_pretrained("facebook/vit-mae-base")
inputs = feature_extractor(images=image, return_tensors="tf")

# Initialize ViTMAEConfig to calculate the total number of patches without
# `cls` token.
vit_mae_config = ViTMAEConfig()
num_patches = int((vit_mae_config.image_size // vit_mae_config.patch_size) ** 2)

# Generate a noise vector to control randomness in masking. This is needed
# to ensure that the PT and TF models operate with same inputs.
# Since we are performing inference with a single image we will keep the
# batch size as 1.
noise = np.random.uniform(size=(1, num_patches))

# Initialize the model and perform inference.
model = TFViTMAEForPreTraining.from_pretrained(
    "facebook/vit-mae-base", from_pt=True
)

# Perform assertions.
outputs = model(**inputs, noise=noise)
assert outputs.logits.shape == [1, 196, 768]

# print(outputs.logits[0, :3, :3].numpy())

# This not the expected_slice from
# https://github.com/huggingface/transformers/blob/master/tests/vit_mae/test_modeling_vit_mae.py.
# Since we need to ensure the equivalence tests of PT/TF pass, this slice
# was generated by passing the PT model through a `noise` vector generated
# from `np.random.uniform(size=(1, 196))`` with `np.random.seed(2)`.
# This way, we ensure PT and TF models operate with the extact same inputs.
expected_slice_cpu = [
    [-0.0548, -1.7023, -0.9325],
    [0.3721, -0.5670, -0.2233],
    [0.8235, -1.3878, -0.3524],
]

np.testing.assert_allclose(
    outputs.logits[0, :3, :3].numpy(),
    np.array(expected_slice_cpu),
    atol=1e-4,
)
