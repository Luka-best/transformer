name: SSH into our runners

on:
  workflow_dispatch:
    inputs:
      runner_type:
        description: 'Type of runner to test (a10 or t4)'
        required: true 
      docker_image:
        description: 'Name of the Docker image'
        required: true

env:
  IS_GITHUB_CI: "1"
  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}
  HF_HOME: /mnt/cache 
  TRANSFORMERS_IS_CI: yes 
  OMP_NUM_THREADS: 8 
  MKL_NUM_THREADS: 8 
  RUN_SLOW: yes # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access. # This token is created under the bot `hf-transformers-bot`. 
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }} 
  TF_FORCE_GPU_ALLOW_GROWTH: true 
  RUN_PT_TF_CROSS_TESTS: 1

jobs:
  ssh_runner:
    name: SSH
    runs-on: [single-gpu, nvidia-gpu, ${{ github.event.inputs.runner_type }}, ci]
    container:
      image: ${{ github.event.inputs.runner_type }}
      options: --gpus all --privileged --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/

    steps:
      - name: Check out code
        uses: actions/checkout@v4
      
      - name: Install locally transformers & other libs
        run: |
          apt install sudo
          sudo -H pip install --upgrade pip
          sudo -H pip uninstall -y transformers 
          sudo -H pip install -U -e ".[testing]" 
          MAX_JOBS=4 pip install flash-attn --no-build-isolation
          pip install bitsandbytes
      
      - name: NVIDIA-SMI
        run: |
          nvidia-smi
      
      - name: Show installed libraries and their versions
        run: pip freeze
      
      - name: Tailscale # In order to be able to SSH when a test fails
        uses: huggingface/tailscale-action@v1
        with:
          authkey: ${{ secrets.TAILSCALE_SSH_AUTHKEY }}
          slackChannel: ${{ secrets.SLACK_CIFEEDBACK_CHANNEL }}
          slackToken: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}
          waitForSSH: true
